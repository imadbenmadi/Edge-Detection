{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83d26a5c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Pathway | What it does | Implementation |\n",
    "|---------|--------------|----------------|\n",
    "| **X** | Local contrast (center-surround) | `3×3 Conv` - `1×1 Conv` |\n",
    "| **Y** | Large RF contrast | `5×5 Dilated Conv` - `1×1 Conv` |\n",
    "| **W** | Directional | `1×3 Conv` (horizontal) + `3×1 Conv` (vertical) |\n",
    "\n",
    "**W pathway is NOT Gabor filters** - it's separable directional convolutions.\n",
    "\n",
    "**Gabor filters** are specifically oriented edge detectors with sinusoidal patterns at different angles (0°, 45°, 90°, 135°, etc.). They're biologically inspired by V1 cortex cells.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab8640a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce GTX 1070\n",
      "Memory: 8.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import os\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "119b7c09-85de-4e45-84b6-7f5dfd51c20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir(\"/kaggle/input/hed-bsds-v2/processed_HED_v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6665db5e-b036-4167-94fa-2a79c5f3329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir(\"/kaggle/input/12epoch/pytorch/default/1\")\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c05a6c5a-fb14-4f62-9d71-72e552877db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce GTX 1070\n",
      "Total VRAM (GB): 8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "print(\"Total VRAM (GB):\", round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58813217-babd-4193-ba98-cf2300b8de81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: ['test', 'train', 'val']\n",
      "TRAIN: ['edges', 'images']\n",
      "TRAIN/IMAGES: ['hed_thick_train_000000.png', 'hed_thick_train_000001.png', 'hed_thick_train_000002.png', 'hed_thick_train_000003.png', 'hed_thick_train_000004.png', 'hed_thick_train_000005.png', 'hed_thick_train_000006.png', 'hed_thick_train_000007.png', 'hed_thick_train_000008.png', 'hed_thick_train_000009.png']\n",
      "TRAIN/EDGES: ['hed_thick_train_000000.png', 'hed_thick_train_000001.png', 'hed_thick_train_000002.png', 'hed_thick_train_000003.png', 'hed_thick_train_000004.png', 'hed_thick_train_000005.png', 'hed_thick_train_000006.png', 'hed_thick_train_000007.png', 'hed_thick_train_000008.png', 'hed_thick_train_000009.png']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# BASE = \"/kaggle/input/hed-bsds-v2/processed_HED_v2\"\n",
    "BASE = \"./datasets/HED_Thick\"\n",
    "\n",
    "print(\"ROOT:\", os.listdir(BASE))\n",
    "print(\"TRAIN:\", os.listdir(BASE + \"/train\"))\n",
    "print(\"TRAIN/IMAGES:\", os.listdir(BASE + \"/train/images\")[:10])\n",
    "print(\"TRAIN/EDGES:\", os.listdir(BASE + \"/train/edges\")[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13521090-5c7a-412f-9792-3a1b3ae41e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying split: train\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking train: 0img [00:00, ?img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verification Report for split: train\n",
      "============================================================\n",
      "Total images checked: 0\n",
      "Missing edge files: 0\n",
      "Unreadable edge files: 0\n",
      "Empty edge maps: 0\n",
      "Size mismatches: 0\n",
      "\n",
      "All images have valid, non-empty, correctly-sized edge maps.\n",
      "\n",
      "Verifying split: val\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking val: 0img [00:00, ?img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verification Report for split: val\n",
      "============================================================\n",
      "Total images checked: 0\n",
      "Missing edge files: 0\n",
      "Unreadable edge files: 0\n",
      "Empty edge maps: 0\n",
      "Size mismatches: 0\n",
      "\n",
      "All images have valid, non-empty, correctly-sized edge maps.\n",
      "\n",
      "Verifying split: test\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking test: 0img [00:00, ?img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verification Report for split: test\n",
      "============================================================\n",
      "Total images checked: 0\n",
      "Missing edge files: 0\n",
      "Unreadable edge files: 0\n",
      "Empty edge maps: 0\n",
      "Size mismatches: 0\n",
      "\n",
      "All images have valid, non-empty, correctly-sized edge maps.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def verify_edges(root_dir, split=\"train\"):\n",
    "    root_dir = Path(root_dir)\n",
    "    img_dir = root_dir / split / \"images\"\n",
    "    edge_dir = root_dir / split / \"edges\"\n",
    "\n",
    "    image_files = sorted(list(img_dir.glob(\"*.png\")))\n",
    "\n",
    "    missing_edges = []\n",
    "    unreadable_edges = []\n",
    "    empty_edges = []\n",
    "    size_mismatch = []\n",
    "\n",
    "    print(f\"\\nVerifying split: {split}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for img_path in tqdm(image_files, desc=f\"Checking {split}\", unit=\"img\"):\n",
    "        edge_path = edge_dir / img_path.name\n",
    "\n",
    "        # 1. Check if edge file exists\n",
    "        if not edge_path.exists():\n",
    "            missing_edges.append(img_path.name)\n",
    "            continue\n",
    "\n",
    "        # 2. Try loading edge\n",
    "        edge = cv2.imread(str(edge_path), cv2.IMREAD_GRAYSCALE)\n",
    "        if edge is None:\n",
    "            unreadable_edges.append(img_path.name)\n",
    "            continue\n",
    "\n",
    "        # 3. Check if edge is empty (all zeros)\n",
    "        if np.sum(edge) == 0:\n",
    "            empty_edges.append(img_path.name)\n",
    "\n",
    "        # 4. Check size match\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None or edge.shape != img.shape[:2]:\n",
    "            size_mismatch.append(img_path.name)\n",
    "\n",
    "    # ===== REPORT =====\n",
    "    print(f\"\\nVerification Report for split: {split}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total images checked: {len(image_files)}\")\n",
    "    print(f\"Missing edge files: {len(missing_edges)}\")\n",
    "    print(f\"Unreadable edge files: {len(unreadable_edges)}\")\n",
    "    print(f\"Empty edge maps: {len(empty_edges)}\")\n",
    "    print(f\"Size mismatches: {len(size_mismatch)}\")\n",
    "\n",
    "    if missing_edges:\n",
    "        print(\"\\nMissing edge files (first 10):\")\n",
    "        print(missing_edges[:10])\n",
    "\n",
    "    if unreadable_edges:\n",
    "        print(\"\\nUnreadable edge files (first 10):\")\n",
    "        print(unreadable_edges[:10])\n",
    "\n",
    "    if empty_edges:\n",
    "        print(\"\\nEmpty edge maps (first 10):\")\n",
    "        print(empty_edges[:10])\n",
    "\n",
    "    if size_mismatch:\n",
    "        print(\"\\nSize mismatches (first 10):\")\n",
    "        print(size_mismatch[:10])\n",
    "\n",
    "    if not (missing_edges or unreadable_edges or empty_edges or size_mismatch):\n",
    "        print(\"\\nAll images have valid, non-empty, correctly-sized edge maps.\")\n",
    "\n",
    "# =========================\n",
    "# RUN FOR ALL SPLITS\n",
    "# =========================\n",
    "\n",
    "DATA_ROOT = r\"/kaggle/input/hed-bsds-v2/processed_HED_v2\"\n",
    "\n",
    "verify_edges(DATA_ROOT, \"train\")\n",
    "verify_edges(DATA_ROOT, \"val\")\n",
    "verify_edges(DATA_ROOT, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85adc490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 35 samples\n",
      "Val:   7 samples\n",
      "Test:  7 samples\n",
      "Image shape: torch.Size([3, 512, 512])\n",
      "Edge shape: torch.Size([1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Cell 2: Dataset Loader\n",
    "class ProcessedDataset(Dataset):\n",
    "    \"\"\"Load processed PNG images and edge maps\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, split='train'):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.split = split\n",
    "        self.img_dir = self.root_dir / split / 'images'\n",
    "        self.edge_dir = self.root_dir / split / 'edges'\n",
    "        self.samples = sorted(list(self.img_dir.glob('*.png')))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.samples[idx]\n",
    "        edge_path = self.edge_dir / img_path.name\n",
    "\n",
    "        # Load image\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "\n",
    "        # Load edge map\n",
    "        edge = cv2.imread(str(edge_path), cv2.IMREAD_GRAYSCALE)\n",
    "        edge = edge.astype(np.float32) / 255.0\n",
    "\n",
    "        # To tensors\n",
    "        img = torch.from_numpy(img).permute(2, 0, 1)\n",
    "        edge = torch.from_numpy(edge).unsqueeze(0)\n",
    "\n",
    "        return {'images': img, 'labels': edge, 'filename': img_path.stem}\n",
    "\n",
    "\n",
    "#  KAGGLE DATASET ROOT\n",
    "# DATA_ROOT = \"/kaggle/input/hed-bsds-v2/processed_HED_v2\"\n",
    "DATA_ROOT = \"./datasets/HED_Thick\"\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_dataset = ProcessedDataset(DATA_ROOT, split='train')\n",
    "# val_dataset   = ProcessedDataset(DATA_ROOT, split='val')\n",
    "val_dataset   = ProcessedDataset(DATA_ROOT, split='test')\n",
    "\n",
    "test_dataset  = ProcessedDataset(DATA_ROOT, split='test')\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2)\n",
    "# val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "# test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "print(f\"Train: {len(train_dataset)} samples\")\n",
    "print(f\"Val:   {len(val_dataset)} samples\")\n",
    "print(f\"Test:  {len(test_dataset)} samples\")\n",
    "\n",
    "# Verify sample\n",
    "sample = train_dataset[0]\n",
    "print(\"Image shape:\", sample[\"images\"].shape)\n",
    "print(\"Edge shape:\", sample[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f812c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 3: XYW-Net Model (Complete)\n",
    "\n",
    "# # ============ PDC Convolution ============\n",
    "# def createPDCFunc(PDC_type):\n",
    "#     \"\"\"Create Pixel Difference Convolution function\"\"\"\n",
    "#     assert PDC_type in ['cv', 'cd', 'ad', 'rd', 'sd', 'p2d', '2sd', '2cd']\n",
    "    \n",
    "#     if PDC_type == 'cv':\n",
    "#         return F.conv2d\n",
    "    \n",
    "#     if PDC_type == '2sd':\n",
    "#         def func(x, weights, bias=None, stride=1, padding=0, dilation=1, groups=1):\n",
    "#             assert weights.size(2) == 3 and weights.size(3) == 3\n",
    "#             shape = weights.shape\n",
    "#             if groups == shape[0]:\n",
    "#                 weights_conv = (weights - weights[:, :, [1, 1, 1, 0, 0, 0, 2, 2, 2], [0, 1, 2, 0, 1, 2, 0, 1, 2]].view(shape))\n",
    "#             else:\n",
    "#                 weights_conv = (weights - weights[:, :, [1, 1, 1, 0, 0, 0, 2, 2, 2], [0, 1, 2, 0, 1, 2, 0, 1, 2]].view(shape).flip(0))\n",
    "#             y = F.conv2d(x, weights_conv, bias, stride=stride, padding=padding, dilation=dilation, groups=groups)\n",
    "#             return y\n",
    "#         return func\n",
    "    \n",
    "#     return F.conv2d\n",
    "\n",
    "# class Conv2d(nn.Module):\n",
    "#     \"\"\"PDC-enabled Conv2d\"\"\"\n",
    "#     def __init__(self, pdc_func='cv', in_channels=1, out_channels=1, kernel_size=3, \n",
    "#                  stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "#         super(Conv2d, self).__init__()\n",
    "#         self.pdc = createPDCFunc(pdc_func)\n",
    "#         self.stride = stride\n",
    "#         self.padding = padding\n",
    "#         self.dilation = dilation\n",
    "#         self.groups = groups\n",
    "#         self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels // groups, kernel_size, kernel_size))\n",
    "#         if bias:\n",
    "#             self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
    "#         else:\n",
    "#             self.register_parameter('bias', None)\n",
    "#         self.reset_parameters()\n",
    "    \n",
    "#     def reset_parameters(self):\n",
    "#         nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "#         if self.bias is not None:\n",
    "#             fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "#             bound = 1 / math.sqrt(fan_in)\n",
    "#             nn.init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.pdc(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "# # ============ Core XYW Components ============\n",
    "# class Xc1x1(nn.Module):\n",
    "#     \"\"\"X pathway: Local contrast (center-surround with 3x3)\"\"\"\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(Xc1x1, self).__init__()\n",
    "#         self.Xcenter = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "#         self.Xcenter_relu = nn.ReLU(inplace=True)\n",
    "#         self.Xsurround = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, groups=in_channels)\n",
    "#         self.conv1_1 = nn.Conv2d(out_channels, out_channels, kernel_size=1)\n",
    "#         self.Xsurround_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         xcenter = self.Xcenter_relu(self.Xcenter(input))\n",
    "#         xsurround = self.Xsurround_relu(self.Xsurround(input))\n",
    "#         xsurround = self.conv1_1(xsurround)\n",
    "#         return xsurround - xcenter\n",
    "\n",
    "# class Yc1x1(nn.Module):\n",
    "#     \"\"\"Y pathway: Large receptive field (center-surround with 5x5 dilated)\"\"\"\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(Yc1x1, self).__init__()\n",
    "#         self.Ycenter = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "#         self.Ycenter_relu = nn.ReLU(inplace=True)\n",
    "#         self.Ysurround = nn.Conv2d(in_channels, out_channels, kernel_size=5, padding=4, dilation=2, groups=in_channels)\n",
    "#         self.conv1_1 = nn.Conv2d(out_channels, out_channels, kernel_size=1)\n",
    "#         self.Ysurround_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         ycenter = self.Ycenter_relu(self.Ycenter(input))\n",
    "#         ysurround = self.Ysurround_relu(self.Ysurround(input))\n",
    "#         ysurround = self.conv1_1(ysurround)\n",
    "#         return ysurround - ycenter\n",
    "\n",
    "# class W(nn.Module):\n",
    "#     \"\"\"W pathway: Directional (horizontal + vertical)\"\"\"\n",
    "#     def __init__(self, inchannel, outchannel):\n",
    "#         super(W, self).__init__()\n",
    "#         self.h = nn.Conv2d(inchannel, inchannel, kernel_size=(1, 3), padding=(0, 1), groups=inchannel)\n",
    "#         self.v = nn.Conv2d(inchannel, inchannel, kernel_size=(3, 1), padding=(1, 0), groups=inchannel)\n",
    "#         self.convh_1 = nn.Conv2d(inchannel, inchannel, kernel_size=1, bias=False)\n",
    "#         self.convv_1 = nn.Conv2d(inchannel, outchannel, kernel_size=1, bias=False)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         h = self.relu(self.h(x))\n",
    "#         h = self.convh_1(h)\n",
    "#         v = self.relu(self.v(h))\n",
    "#         v = self.convv_1(v)\n",
    "#         return v\n",
    "\n",
    "# # ============ XYW Blocks ============\n",
    "# class XYW_S(nn.Module):\n",
    "#     \"\"\"XYW Start block\"\"\"\n",
    "#     def __init__(self, inchannel, outchannel, stride=1):\n",
    "#         super(XYW_S, self).__init__()\n",
    "#         self.y_c = Yc1x1(inchannel, outchannel)\n",
    "#         self.x_c = Xc1x1(inchannel, outchannel)\n",
    "#         self.w = W(inchannel, outchannel)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.x_c(x), self.y_c(x), self.w(x)\n",
    "\n",
    "# class XYW(nn.Module):\n",
    "#     \"\"\"XYW middle block\"\"\"\n",
    "#     def __init__(self, inchannel, outchannel, stride=1):\n",
    "#         super(XYW, self).__init__()\n",
    "#         self.y_c = Yc1x1(inchannel, outchannel)\n",
    "#         self.x_c = Xc1x1(inchannel, outchannel)\n",
    "#         self.w = W(inchannel, outchannel)\n",
    "\n",
    "#     def forward(self, xc, yc, w):\n",
    "#         return self.x_c(xc), self.y_c(yc), self.w(w)\n",
    "\n",
    "# class XYW_E(nn.Module):\n",
    "#     \"\"\"XYW End block (combines X+Y+W)\"\"\"\n",
    "#     def __init__(self, inchannel, outchannel):\n",
    "#         super(XYW_E, self).__init__()\n",
    "#         self.y_c = Yc1x1(inchannel, outchannel)\n",
    "#         self.x_c = Xc1x1(inchannel, outchannel)\n",
    "#         self.w = W(inchannel, outchannel)\n",
    "\n",
    "#     def forward(self, xc, yc, w):\n",
    "#         return self.x_c(xc) + self.y_c(yc) + self.w(w)\n",
    "\n",
    "# # ============ Encoder Stages ============\n",
    "# class s1(nn.Module):\n",
    "#     def __init__(self, channel=30):\n",
    "#         super(s1, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, channel, kernel_size=7, padding=6, dilation=2)\n",
    "#         self.xyw1_1 = XYW_S(channel, channel)\n",
    "#         self.xyw1_2 = XYW(channel, channel)\n",
    "#         self.xyw1_3 = XYW_E(channel, channel)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         temp = self.relu(self.conv1(x))\n",
    "#         xc, yc, w = self.xyw1_1(temp)\n",
    "#         xc, yc, w = self.xyw1_2(xc, yc, w)\n",
    "#         xyw1_3 = self.xyw1_3(xc, yc, w)\n",
    "#         return xyw1_3 + temp\n",
    "\n",
    "# class s2(nn.Module):\n",
    "#     def __init__(self, channel=60):\n",
    "#         super(s2, self).__init__()\n",
    "#         self.xyw2_1 = XYW_S(channel//2, channel, stride=2)\n",
    "#         self.xyw2_2 = XYW(channel, channel)\n",
    "#         self.xyw2_3 = XYW_E(channel, channel)\n",
    "#         self.shortcut = nn.Conv2d(channel//2, channel, kernel_size=1)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(x)\n",
    "#         xc, yc, w = self.xyw2_1(x)\n",
    "#         xc, yc, w = self.xyw2_2(xc, yc, w)\n",
    "#         xyw2_3 = self.xyw2_3(xc, yc, w)\n",
    "#         return xyw2_3 + self.shortcut(x)\n",
    "\n",
    "# class s3(nn.Module):\n",
    "#     def __init__(self, channel=120):\n",
    "#         super(s3, self).__init__()\n",
    "#         self.xyw3_1 = XYW_S(channel//2, channel, stride=2)\n",
    "#         self.xyw3_2 = XYW(channel, channel)\n",
    "#         self.xyw3_3 = XYW_E(channel, channel)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.shortcut = nn.Conv2d(channel//2, channel, kernel_size=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(x)\n",
    "#         shortcut = self.shortcut(x)\n",
    "#         xc, yc, w = self.xyw3_1(x)\n",
    "#         xc, yc, w = self.xyw3_2(xc, yc, w)\n",
    "#         xyw3_3 = self.xyw3_3(xc, yc, w)\n",
    "#         return xyw3_3 + shortcut\n",
    "\n",
    "# class s4(nn.Module):\n",
    "#     def __init__(self, channel=120):\n",
    "#         super(s4, self).__init__()\n",
    "#         self.xyw4_1 = XYW_S(channel, channel, stride=2)\n",
    "#         self.xyw4_2 = XYW(channel, channel)\n",
    "#         self.xyw4_3 = XYW_E(channel, channel)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.shortcut = nn.Conv2d(channel, channel, kernel_size=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(x)\n",
    "#         shortcut = self.shortcut(x)\n",
    "#         xc, yc, w = self.xyw4_1(x)\n",
    "#         xc, yc, w = self.xyw4_2(xc, yc, w)\n",
    "#         xyw4_3 = self.xyw4_3(xc, yc, w)\n",
    "#         return xyw4_3 + shortcut\n",
    "\n",
    "# # ============ Encoder ============\n",
    "# class encode(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(encode, self).__init__()\n",
    "#         self.s1 = s1()\n",
    "#         self.s2 = s2()\n",
    "#         self.s3 = s3()\n",
    "#         self.s4 = s4()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         s1_out = self.s1(x)\n",
    "#         s2_out = self.s2(s1_out)\n",
    "#         s3_out = self.s3(s2_out)\n",
    "#         s4_out = self.s4(s3_out)\n",
    "#         return s1_out, s2_out, s3_out, s4_out\n",
    "\n",
    "# # ============ Adaptive Convolution ============\n",
    "# def upsample_filt(size):\n",
    "#     factor = (size + 1) // 2\n",
    "#     center = factor - 1 if size % 2 == 1 else factor - 0.5\n",
    "#     og = np.ogrid[:size, :size]\n",
    "#     return (1 - abs(og[0] - center) / factor) * (1 - abs(og[1] - center) / factor)\n",
    "\n",
    "# def bilinear_upsample_weights(factor, num_classes):\n",
    "#     filter_size = 2 * factor - factor % 2\n",
    "#     weights = np.zeros((num_classes, num_classes, filter_size, filter_size), dtype=np.float32)\n",
    "#     upsample_kernel = upsample_filt(filter_size)\n",
    "#     for i in range(num_classes):\n",
    "#         weights[i, i, :, :] = upsample_kernel\n",
    "#     return torch.Tensor(weights)\n",
    "\n",
    "# class adap_conv(nn.Module):\n",
    "#     \"\"\"Adaptive convolution with learnable weight\"\"\"\n",
    "#     def __init__(self, in_channels, out_channels, kz=3, pd=1):\n",
    "#         super(adap_conv, self).__init__()\n",
    "#         self.conv = nn.Sequential(\n",
    "#             Conv2d(pdc_func='2sd', in_channels=in_channels, out_channels=out_channels, kernel_size=kz, padding=pd),\n",
    "#             nn.InstanceNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "#         self.weight = nn.Parameter(torch.Tensor([0.]))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.conv(x) * self.weight.sigmoid()\n",
    "\n",
    "# class Refine_block2_1(nn.Module):\n",
    "#     \"\"\"Refinement block for decoder\"\"\"\n",
    "#     def __init__(self, in_channel, out_channel, factor, require_grad=False):\n",
    "#         super(Refine_block2_1, self).__init__()\n",
    "#         self.pre_conv1 = adap_conv(in_channel[0], out_channel, kz=3, pd=1)\n",
    "#         self.pre_conv2 = adap_conv(in_channel[1], out_channel, kz=3, pd=1)\n",
    "#         self.factor = factor\n",
    "#         self.deconv_weight = nn.Parameter(bilinear_upsample_weights(factor, out_channel), requires_grad=require_grad)\n",
    "\n",
    "#     def forward(self, *input):\n",
    "#         x1 = self.pre_conv1(input[0])\n",
    "#         x2 = self.pre_conv2(input[1])\n",
    "#         x2 = F.conv_transpose2d(x2, self.deconv_weight, stride=self.factor, \n",
    "#                                 padding=int(self.factor/2),\n",
    "#                                 output_padding=(x1.size(2) - x2.size(2)*self.factor, \n",
    "#                                                x1.size(3) - x2.size(3)*self.factor))\n",
    "#         return x1 + x2\n",
    "\n",
    "# # ============ RCF Decoder ============\n",
    "# class decode_rcf(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(decode_rcf, self).__init__()\n",
    "#         self.f43 = Refine_block2_1(in_channel=(120, 120), out_channel=60, factor=2)\n",
    "#         self.f32 = Refine_block2_1(in_channel=(60, 60), out_channel=30, factor=2)\n",
    "#         self.f21 = Refine_block2_1(in_channel=(30, 30), out_channel=24, factor=2)\n",
    "#         self.f = nn.Conv2d(24, 1, kernel_size=1, padding=0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         s3 = self.f43(x[2], x[3])\n",
    "#         s2 = self.f32(x[1], s3)\n",
    "#         s1 = self.f21(x[0], s2)\n",
    "#         out = self.f(s1)\n",
    "#         return out.sigmoid()\n",
    "\n",
    "# # ============ Full XYW-Net ============\n",
    "# class XYWNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(XYWNet, self).__init__()\n",
    "#         self.encode = encode()\n",
    "#         self.decode = decode_rcf()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         endpoints = self.encode(x)\n",
    "#         out = self.decode(endpoints)\n",
    "#         return out\n",
    "    \n",
    "#     def forward_with_stages(self, x):\n",
    "#         \"\"\"Forward pass returning intermediate stage outputs for visualization\"\"\"\n",
    "#         s1, s2, s3, s4 = self.encode(x)\n",
    "#         final = self.decode((s1, s2, s3, s4))\n",
    "#         return final, (s1, s2, s3, s4)\n",
    "\n",
    "# # Create model\n",
    "# model = XYWNet().to(device)\n",
    "# print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f315dfd1-c8d1-40d2-b492-d09b4eb1cb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#   XYW-NET: COMPLETE ARCHITECTURE (ENCODER + ITM + ELC)\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#  PDC CONVOLUTION (Pixel Difference Convolution)\n",
    "# ============================================================\n",
    "def createPDCFunc(PDC_type):\n",
    "    assert PDC_type in ['cv', '2sd']\n",
    "    \n",
    "    if PDC_type == 'cv':\n",
    "        return F.conv2d\n",
    "    \n",
    "    if PDC_type == '2sd':\n",
    "        # Pixel difference convolution (paper uses this for ELC)\n",
    "        def func(x, weights, bias=None, stride=1, padding=0, dilation=1, groups=1):\n",
    "            assert weights.size(2) == 3 and weights.size(3) == 3\n",
    "            shape = weights.shape\n",
    "            offset = weights[:, :, \n",
    "                             [1,1,1,0,0,0,2,2,2],\n",
    "                             [0,1,2,0,1,2,0,1,2]\n",
    "                             ].view(shape)\n",
    "            \n",
    "            diff_weights = weights - offset\n",
    "            return F.conv2d(x, diff_weights, bias, stride, padding, dilation, groups)\n",
    "        \n",
    "        return func\n",
    "\n",
    "\n",
    "class Conv2d(nn.Module):\n",
    "    \"\"\"PDC-enabled conv layer\"\"\"\n",
    "    def __init__(self, pdc_func='cv', in_channels=1, out_channels=1, kernel_size=3,\n",
    "                 stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "        super().__init__()\n",
    "        self.pdc = createPDCFunc(pdc_func)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels // groups, kernel_size, kernel_size))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pdc(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#  XYW COMPONENTS (ENCODER)\n",
    "# ============================================================\n",
    "class Xc1x1(nn.Module):\n",
    "    \"\"\"X: small RF center-surround\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.center = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.surround = nn.Conv2d(in_channels, out_channels, 3, padding=1, groups=in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.proj = nn.Conv2d(out_channels, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c = self.relu(self.center(x))\n",
    "        s = self.relu(self.surround(x))\n",
    "        s = self.proj(s)\n",
    "        return s - c\n",
    "\n",
    "\n",
    "class Yc1x1(nn.Module):\n",
    "    \"\"\"Y: large RF dilated center-surround\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.center = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.surround = nn.Conv2d(in_channels, out_channels, 5, padding=4, dilation=2, groups=in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.proj = nn.Conv2d(out_channels, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c = self.relu(self.center(x))\n",
    "        s = self.relu(self.surround(x))\n",
    "        s = self.proj(s)\n",
    "        return s - c\n",
    "\n",
    "\n",
    "class W(nn.Module):\n",
    "    \"\"\"W: directional horizontal + vertical pathway\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.h = nn.Conv2d(in_ch, in_ch, (1,3), padding=(0,1), groups=in_ch)\n",
    "        self.v = nn.Conv2d(in_ch, in_ch, (3,1), padding=(1,0), groups=in_ch)\n",
    "        self.proj1 = nn.Conv2d(in_ch, in_ch, 1)\n",
    "        self.proj2 = nn.Conv2d(in_ch, out_ch, 1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.h(x))\n",
    "        h = self.proj1(h)\n",
    "        v = self.relu(self.v(h))\n",
    "        return self.proj2(v)\n",
    "\n",
    "\n",
    "class XYW_S(nn.Module):\n",
    "    \"\"\"Start block\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.x = Xc1x1(in_ch, out_ch)\n",
    "        self.y = Yc1x1(in_ch, out_ch)\n",
    "        self.w = W(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.x(x), self.y(x), self.w(x)\n",
    "\n",
    "\n",
    "class XYW(nn.Module):\n",
    "    \"\"\"Middle block\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.x = Xc1x1(in_ch, out_ch)\n",
    "        self.y = Yc1x1(in_ch, out_ch)\n",
    "        self.w = W(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, xc, yc, w):\n",
    "        return self.x(xc), self.y(yc), self.w(w)\n",
    "\n",
    "\n",
    "class XYW_E(nn.Module):\n",
    "    \"\"\"End block: fusion of X+Y+W\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.x = Xc1x1(in_ch, out_ch)\n",
    "        self.y = Yc1x1(in_ch, out_ch)\n",
    "        self.w = W(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, xc, yc, w):\n",
    "        return self.x(xc) + self.y(yc) + self.w(w)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#  ENCODER STAGES (4 RESOLUTION LEVELS)\n",
    "# ============================================================\n",
    "class s1(nn.Module):\n",
    "    def __init__(self, ch=30):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Conv2d(3, ch, 7, padding=6, dilation=2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.b1 = XYW_S(ch, ch)\n",
    "        self.b2 = XYW(ch, ch)\n",
    "        self.b3 = XYW_E(ch, ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = self.relu(self.stem(x))\n",
    "        xc, yc, w = self.b1(t)\n",
    "        xc, yc, w = self.b2(xc, yc, w)\n",
    "        out = self.b3(xc, yc, w)\n",
    "        return out + t\n",
    "\n",
    "\n",
    "class s2(nn.Module):\n",
    "    def __init__(self, ch=60):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.b1 = XYW_S(ch//2, ch)\n",
    "        self.b2 = XYW(ch, ch)\n",
    "        self.b3 = XYW_E(ch, ch)\n",
    "        self.short = nn.Conv2d(ch//2, ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        xc, yc, w = self.b1(x)\n",
    "        xc, yc, w = self.b2(xc, yc, w)\n",
    "        out = self.b3(xc, yc, w)\n",
    "        return out + self.short(x)\n",
    "\n",
    "\n",
    "class s3(nn.Module):\n",
    "    def __init__(self, ch=120):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.b1 = XYW_S(ch//2, ch)\n",
    "        self.b2 = XYW(ch, ch)\n",
    "        self.b3 = XYW_E(ch, ch)\n",
    "        self.short = nn.Conv2d(ch//2, ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        sc = self.short(x)\n",
    "        xc, yc, w = self.b1(x)\n",
    "        xc, yc, w = self.b2(xc, yc, w)\n",
    "        return self.b3(xc, yc, w) + sc\n",
    "\n",
    "\n",
    "class s4(nn.Module):\n",
    "    def __init__(self, ch=120):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.b1 = XYW_S(ch, ch)\n",
    "        self.b2 = XYW(ch, ch)\n",
    "        self.b3 = XYW_E(ch, ch)\n",
    "        self.short = nn.Conv2d(ch, ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        sc = self.short(x)\n",
    "        xc, yc, w = self.b1(x)\n",
    "        xc, yc, w = self.b2(xc, yc, w)\n",
    "        return self.b3(xc, yc, w) + sc\n",
    "\n",
    "\n",
    "class encode(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.s1 = s1()\n",
    "        self.s2 = s2()\n",
    "        self.s3 = s3()\n",
    "        self.s4 = s4()\n",
    "\n",
    "    def forward(self, x):\n",
    "        s1_out = self.s1(x)\n",
    "        s2_out = self.s2(s1_out)\n",
    "        s3_out = self.s3(s2_out)\n",
    "        s4_out = self.s4(s3_out)\n",
    "        return s1_out, s2_out, s3_out, s4_out\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#  UPSAMPLE HELPERS \n",
    "# ============================================================\n",
    "def upsample_filt(size):\n",
    "    factor = (size + 1) // 2\n",
    "    center = factor - 1 if size % 2 == 1 else factor - 0.5\n",
    "    og = np.ogrid[:size, :size]\n",
    "    return (1 - abs(og[0] - center) / factor) * (1 - abs(og[1] - center) / factor)\n",
    "\n",
    "\n",
    "def bilinear_upsample_weights(factor, C):\n",
    "    fs = 2 * factor - factor % 2\n",
    "    w = np.zeros((C, C, fs, fs), dtype=np.float32)\n",
    "    kern = upsample_filt(fs)\n",
    "    for i in range(C):\n",
    "        w[i, i] = kern\n",
    "    return torch.Tensor(w)\n",
    "\n",
    "\n",
    "class Refine_block2_1(nn.Module):\n",
    "    \"\"\"Refinement block used in ITM\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, factor):\n",
    "        super().__init__()\n",
    "        self.pre1 = Conv2d('2sd', in_ch[0], out_ch, 3, padding=1)\n",
    "        self.pre2 = Conv2d('2sd', in_ch[1], out_ch, 3, padding=1)\n",
    "        self.deconv_w = nn.Parameter(bilinear_upsample_weights(factor, out_ch), requires_grad=False)\n",
    "        self.factor = factor\n",
    "\n",
    "    def forward(self, x_high, x_low):\n",
    "        h = self.pre1(x_high)\n",
    "        l = self.pre2(x_low)\n",
    "        l = F.conv_transpose2d(l, self.deconv_w, stride=self.factor,\n",
    "                               padding=int(self.factor/2),\n",
    "                               output_padding=(h.size(2) - l.size(2)*self.factor,\n",
    "                                               h.size(3) - l.size(3)*self.factor))\n",
    "        return h + l\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#  ELC BLOCK (Edge Localization Convolution)\n",
    "# ============================================================\n",
    "class ELCBlock(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.pdc = Conv2d('2sd', ch, ch, 3, padding=1)\n",
    "        self.norm = nn.InstanceNorm2d(ch)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.out = nn.Conv2d(ch, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pdc(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.relu(x)\n",
    "        return torch.sigmoid(self.out(x))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#  ITM + ELC DECODER (XYW-Net)\n",
    "# ============================================================\n",
    "class decode_xyw(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.f43 = Refine_block2_1((120,120), 64, 2)\n",
    "        self.f32 = Refine_block2_1((60,64),   48, 2)\n",
    "        self.f21 = Refine_block2_1((30,48),   32, 2)\n",
    "        self.elc = ELCBlock(32)\n",
    "\n",
    "    def forward(self, endpoints):\n",
    "        s1, s2, s3, s4 = endpoints\n",
    "        x3 = self.f43(s3, s4)\n",
    "        x2 = self.f32(s2, x3)\n",
    "        x1 = self.f21(s1, x2)\n",
    "        return self.elc(x1)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#  FULL XYW-NET (ENCODER + DECODER)\n",
    "# ============================================================\n",
    "class XYWNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = encode()\n",
    "        self.decoder = decode_xyw()\n",
    "\n",
    "    def forward(self, x):\n",
    "        endpoints = self.encoder(x)\n",
    "        return self.decoder(endpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac73f4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function and metrics defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Loss Function and Metrics\n",
    "\n",
    "class EdgeLoss(nn.Module):\n",
    "    \"\"\"Weighted cross-entropy loss for edge detection\"\"\"\n",
    "    def __init__(self):\n",
    "        super(EdgeLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, pred, label):\n",
    "        pred_flat = pred.view(-1)\n",
    "        label_flat = label.view(-1)\n",
    "        eps = 1e-6\n",
    "        \n",
    "        # Positive and negative pixels\n",
    "        pos_mask = label_flat > 0\n",
    "        neg_mask = label_flat == 0\n",
    "        \n",
    "        pred_pos = pred_flat[pos_mask].clamp(eps, 1.0 - eps)\n",
    "        pred_neg = pred_flat[neg_mask].clamp(eps, 1.0 - eps)\n",
    "        \n",
    "        # Weighted by annotation strength\n",
    "        w_pos = label_flat[pos_mask]\n",
    "        \n",
    "        if len(pred_pos) > 0 and len(pred_neg) > 0:\n",
    "            loss = (-pred_pos.log() * w_pos).mean() + (-(1.0 - pred_neg).log()).mean()\n",
    "        elif len(pred_pos) > 0:\n",
    "            loss = (-pred_pos.log() * w_pos).mean()\n",
    "        else:\n",
    "            loss = (-(1.0 - pred_neg).log()).mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "def compute_ods_ois_ap(preds, labels, thresholds=99):\n",
    "    \"\"\"\n",
    "    Compute ODS (Optimal Dataset Scale), OIS (Optimal Image Scale), and AP.\n",
    "    \n",
    "    ODS: Best F-score using a single threshold for all images\n",
    "    OIS: Average of best F-score per image\n",
    "    AP: Average Precision\n",
    "    \"\"\"\n",
    "    threshs = np.linspace(0.01, 0.99, thresholds)\n",
    "    \n",
    "    # For ODS (global)\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # For OIS (per-image best)\n",
    "    ois_f1_scores = []\n",
    "    \n",
    "    for pred, label in zip(preds, labels):\n",
    "        pred_np = pred.flatten()\n",
    "        label_np = (label.flatten() > 0.5).astype(np.float32)\n",
    "        \n",
    "        all_preds.append(pred_np)\n",
    "        all_labels.append(label_np)\n",
    "        \n",
    "        # Per-image best F1\n",
    "        best_f1 = 0\n",
    "        for t in threshs:\n",
    "            pred_bin = (pred_np >= t).astype(np.float32)\n",
    "            tp = np.sum(pred_bin * label_np)\n",
    "            fp = np.sum(pred_bin * (1 - label_np))\n",
    "            fn = np.sum((1 - pred_bin) * label_np)\n",
    "            \n",
    "            precision = tp / (tp + fp + 1e-8)\n",
    "            recall = tp / (tp + fn + 1e-8)\n",
    "            f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "            best_f1 = max(best_f1, f1)\n",
    "        \n",
    "        ois_f1_scores.append(best_f1)\n",
    "    \n",
    "    # Concatenate all for global metrics\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    # ODS: Best global threshold\n",
    "    best_ods = 0\n",
    "    for t in threshs:\n",
    "        pred_bin = (all_preds >= t).astype(np.float32)\n",
    "        tp = np.sum(pred_bin * all_labels)\n",
    "        fp = np.sum(pred_bin * (1 - all_labels))\n",
    "        fn = np.sum((1 - pred_bin) * all_labels)\n",
    "        \n",
    "        precision = tp / (tp + fp + 1e-8)\n",
    "        recall = tp / (tp + fn + 1e-8)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "        best_ods = max(best_ods, f1)\n",
    "    \n",
    "    # OIS: Average of per-image best\n",
    "    ois = np.mean(ois_f1_scores)\n",
    "    \n",
    "    # AP: Average Precision\n",
    "    try:\n",
    "        ap = average_precision_score(all_labels, all_preds)\n",
    "    except:\n",
    "        ap = 0.0\n",
    "    \n",
    "    return best_ods, ois, ap\n",
    "\n",
    "print(\"Loss function and metrics defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "044dd9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Training Function\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for batch in pbar:\n",
    "        images = batch['images'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(loader, desc='Evaluating'):\n",
    "        images = batch['images'].to(device)\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        for i in range(outputs.shape[0]):\n",
    "            all_preds.append(outputs[i, 0].cpu().numpy())\n",
    "            all_labels.append(labels[i, 0].numpy())\n",
    "    \n",
    "    ods, ois, ap = compute_ods_ois_ap(all_preds, all_labels)\n",
    "    return ods, ois, ap\n",
    "\n",
    "print(\"Training functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72bc00dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XYW-Net for 3 epochs...\n",
      "Train samples: 35, Val samples: 7\n",
      "============================================================\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████| 9/9 [00:13<00:00,  1.51s/it, loss=0.9621]\n",
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9551 | ODS: 0.0859 | OIS: 0.0913 | AP: 0.0372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imed\\AppData\\Local\\Temp\\ipykernel_15804\\3560366582.py:24: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert weights.size(2) == 3 and weights.size(3) == 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved best model (ODS: 0.0859)\n",
      "  -> Saved epoch checkpoint: models/1epoch.pth\n",
      "  -> Saved traced model: models/xywnet_model.pt\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████| 9/9 [00:11<00:00,  1.26s/it, loss=0.9216]\n",
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9378 | ODS: 0.0841 | OIS: 0.0894 | AP: 0.0340\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████| 9/9 [00:11<00:00,  1.28s/it, loss=0.9477]\n",
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9273 | ODS: 0.0800 | OIS: 0.0871 | AP: 0.0316\n",
      "\n",
      "============================================================\n",
      "Training complete. Best ODS: 0.0859\n",
      "All files saved to models/ folder\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Train XYW-Net (FROM SCRATCH)\n",
    "\n",
    "# -------------------------------\n",
    "# Hyperparameters\n",
    "# -------------------------------\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# -------------------------------\n",
    "# Initialize\n",
    "# -------------------------------\n",
    "model = XYWNet().to(device)\n",
    "criterion = EdgeLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# -------------------------------\n",
    "# Training history\n",
    "# -------------------------------\n",
    "history = {'train_loss': [], 'val_ods': [], 'val_ois': [], 'val_ap': []}\n",
    "best_ods = 0\n",
    "\n",
    "# -------------------------------\n",
    "# Create models directory if it doesn't exist\n",
    "# -------------------------------\n",
    "import os\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "print(f\"Training XYW-Net for {NUM_EPOCHS} epochs...\")\n",
    "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# -------------------------------\n",
    "#  FULL CHECKPOINT SAVE FUNCTION\n",
    "# -------------------------------\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, best_ods, path):\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"scheduler_state\": scheduler.state_dict(),\n",
    "        \"best_ods\": best_ods\n",
    "    }, path)\n",
    "\n",
    "# -------------------------------\n",
    "# TRAINING LOOP\n",
    "# -------------------------------\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    \n",
    "    # Evaluate on validation\n",
    "    val_ods, val_ois, val_ap = evaluate(model, val_loader, device)\n",
    "    history['val_ods'].append(val_ods)\n",
    "    history['val_ois'].append(val_ois)\n",
    "    history['val_ap'].append(val_ap)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Loss: {train_loss:.4f} | ODS: {val_ods:.4f} | OIS: {val_ois:.4f} | AP: {val_ap:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_ods > best_ods:\n",
    "        best_ods = val_ods\n",
    "        \n",
    "        # Save state dict\n",
    "        torch.save(model.state_dict(), f'models/{epoch+1}epoch.pth')\n",
    "        torch.save(model.state_dict(), 'models/best_xyw_net.pth')\n",
    "        \n",
    "        # Save full checkpoint\n",
    "        save_checkpoint(\n",
    "            model,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            epoch,\n",
    "            best_ods,\n",
    "            \"models/xywnet_full_checkpoint.pth\"\n",
    "        )\n",
    "        \n",
    "        # -------------------------------\n",
    "        #  EXPORT DEPLOYMENT .pt MODEL\n",
    "        # -------------------------------\n",
    "        # Use a real image from the dataset for tracing (better than random)\n",
    "        real_sample = train_dataset[0]\n",
    "        example_input = real_sample['images'].unsqueeze(0).to(device)\n",
    "        \n",
    "        # Trace the model with real data\n",
    "        traced_model = torch.jit.trace(model, example_input)\n",
    "        traced_model.save(\"models/xywnet_model.pt\")\n",
    "        \n",
    "        print(f\"  -> Saved best model (ODS: {best_ods:.4f})\")\n",
    "        print(f\"  -> Saved epoch checkpoint: models/{epoch+1}epoch.pth\")\n",
    "        print(f\"  -> Saved traced model: models/xywnet_model.pt\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Training complete. Best ODS: {best_ods:.4f}\")\n",
    "print(\"All files saved to models/ folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "529e9a74-832e-4efa-bbd7-ca7e322637ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained weights from 20epoch.pth ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imed\\AppData\\Local\\Temp\\ipykernel_15804\\3452599927.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"models/20epoch.pth\", map_location=device))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for XYWNet:\n\tMissing key(s) in state_dict: \"encoder.s1.stem.weight\", \"encoder.s1.stem.bias\", \"encoder.s1.b1.x.center.weight\", \"encoder.s1.b1.x.center.bias\", \"encoder.s1.b1.x.surround.weight\", \"encoder.s1.b1.x.surround.bias\", \"encoder.s1.b1.x.proj.weight\", \"encoder.s1.b1.x.proj.bias\", \"encoder.s1.b1.y.center.weight\", \"encoder.s1.b1.y.center.bias\", \"encoder.s1.b1.y.surround.weight\", \"encoder.s1.b1.y.surround.bias\", \"encoder.s1.b1.y.proj.weight\", \"encoder.s1.b1.y.proj.bias\", \"encoder.s1.b1.w.h.weight\", \"encoder.s1.b1.w.h.bias\", \"encoder.s1.b1.w.v.weight\", \"encoder.s1.b1.w.v.bias\", \"encoder.s1.b1.w.proj1.weight\", \"encoder.s1.b1.w.proj1.bias\", \"encoder.s1.b1.w.proj2.weight\", \"encoder.s1.b1.w.proj2.bias\", \"encoder.s1.b2.x.center.weight\", \"encoder.s1.b2.x.center.bias\", \"encoder.s1.b2.x.surround.weight\", \"encoder.s1.b2.x.surround.bias\", \"encoder.s1.b2.x.proj.weight\", \"encoder.s1.b2.x.proj.bias\", \"encoder.s1.b2.y.center.weight\", \"encoder.s1.b2.y.center.bias\", \"encoder.s1.b2.y.surround.weight\", \"encoder.s1.b2.y.surround.bias\", \"encoder.s1.b2.y.proj.weight\", \"encoder.s1.b2.y.proj.bias\", \"encoder.s1.b2.w.h.weight\", \"encoder.s1.b2.w.h.bias\", \"encoder.s1.b2.w.v.weight\", \"encoder.s1.b2.w.v.bias\", \"encoder.s1.b2.w.proj1.weight\", \"encoder.s1.b2.w.proj1.bias\", \"encoder.s1.b2.w.proj2.weight\", \"encoder.s1.b2.w.proj2.bias\", \"encoder.s1.b3.x.center.weight\", \"encoder.s1.b3.x.center.bias\", \"encoder.s1.b3.x.surround.weight\", \"encoder.s1.b3.x.surround.bias\", \"encoder.s1.b3.x.proj.weight\", \"encoder.s1.b3.x.proj.bias\", \"encoder.s1.b3.y.center.weight\", \"encoder.s1.b3.y.center.bias\", \"encoder.s1.b3.y.surround.weight\", \"encoder.s1.b3.y.surround.bias\", \"encoder.s1.b3.y.proj.weight\", \"encoder.s1.b3.y.proj.bias\", \"encoder.s1.b3.w.h.weight\", \"encoder.s1.b3.w.h.bias\", \"encoder.s1.b3.w.v.weight\", \"encoder.s1.b3.w.v.bias\", \"encoder.s1.b3.w.proj1.weight\", \"encoder.s1.b3.w.proj1.bias\", \"encoder.s1.b3.w.proj2.weight\", \"encoder.s1.b3.w.proj2.bias\", \"encoder.s2.b1.x.center.weight\", \"encoder.s2.b1.x.center.bias\", \"encoder.s2.b1.x.surround.weight\", \"encoder.s2.b1.x.surround.bias\", \"encoder.s2.b1.x.proj.weight\", \"encoder.s2.b1.x.proj.bias\", \"encoder.s2.b1.y.center.weight\", \"encoder.s2.b1.y.center.bias\", \"encoder.s2.b1.y.surround.weight\", \"encoder.s2.b1.y.surround.bias\", \"encoder.s2.b1.y.proj.weight\", \"encoder.s2.b1.y.proj.bias\", \"encoder.s2.b1.w.h.weight\", \"encoder.s2.b1.w.h.bias\", \"encoder.s2.b1.w.v.weight\", \"encoder.s2.b1.w.v.bias\", \"encoder.s2.b1.w.proj1.weight\", \"encoder.s2.b1.w.proj1.bias\", \"encoder.s2.b1.w.proj2.weight\", \"encoder.s2.b1.w.proj2.bias\", \"encoder.s2.b2.x.center.weight\", \"encoder.s2.b2.x.center.bias\", \"encoder.s2.b2.x.surround.weight\", \"encoder.s2.b2.x.surround.bias\", \"encoder.s2.b2.x.proj.weight\", \"encoder.s2.b2.x.proj.bias\", \"encoder.s2.b2.y.center.weight\", \"encoder.s2.b2.y.center.bias\", \"encoder.s2.b2.y.surround.weight\", \"encoder.s2.b2.y.surround.bias\", \"encoder.s2.b2.y.proj.weight\", \"encoder.s2.b2.y.proj.bias\", \"encoder.s2.b2.w.h.weight\", \"encoder.s2.b2.w.h.bias\", \"encoder.s2.b2.w.v.weight\", \"encoder.s2.b2.w.v.bias\", \"encoder.s2.b2.w.proj1.weight\", \"encoder.s2.b2.w.proj1.bias\", \"encoder.s2.b2.w.proj2.weight\", \"encoder.s2.b2.w.proj2.bias\", \"encoder.s2.b3.x.center.weight\", \"encoder.s2.b3.x.center.bias\", \"encoder.s2.b3.x.surround.weight\", \"encoder.s2.b3.x.surround.bias\", \"encoder.s2.b3.x.proj.weight\", \"encoder.s2.b3.x.proj.bias\", \"encoder.s2.b3.y.center.weight\", \"encoder.s2.b3.y.center.bias\", \"encoder.s2.b3.y.surround.weight\", \"encoder.s2.b3.y.surround.bias\", \"encoder.s2.b3.y.proj.weight\", \"encoder.s2.b3.y.proj.bias\", \"encoder.s2.b3.w.h.weight\", \"encoder.s2.b3.w.h.bias\", \"encoder.s2.b3.w.v.weight\", \"encoder.s2.b3.w.v.bias\", \"encoder.s2.b3.w.proj1.weight\", \"encoder.s2.b3.w.proj1.bias\", \"encoder.s2.b3.w.proj2.weight\", \"encoder.s2.b3.w.proj2.bias\", \"encoder.s2.short.weight\", \"encoder.s2.short.bias\", \"encoder.s3.b1.x.center.weight\", \"encoder.s3.b1.x.center.bias\", \"encoder.s3.b1.x.surround.weight\", \"encoder.s3.b1.x.surround.bias\", \"encoder.s3.b1.x.proj.weight\", \"encoder.s3.b1.x.proj.bias\", \"encoder.s3.b1.y.center.weight\", \"encoder.s3.b1.y.center.bias\", \"encoder.s3.b1.y.surround.weight\", \"encoder.s3.b1.y.surround.bias\", \"encoder.s3.b1.y.proj.weight\", \"encoder.s3.b1.y.proj.bias\", \"encoder.s3.b1.w.h.weight\", \"encoder.s3.b1.w.h.bias\", \"encoder.s3.b1.w.v.weight\", \"encoder.s3.b1.w.v.bias\", \"encoder.s3.b1.w.proj1.weight\", \"encoder.s3.b1.w.proj1.bias\", \"encoder.s3.b1.w.proj2.weight\", \"encoder.s3.b1.w.proj2.bias\", \"encoder.s3.b2.x.center.weight\", \"encoder.s3.b2.x.center.bias\", \"encoder.s3.b2.x.surround.weight\", \"encoder.s3.b2.x.surround.bias\", \"encoder.s3.b2.x.proj.weight\", \"encoder.s3.b2.x.proj.bias\", \"encoder.s3.b2.y.center.weight\", \"encoder.s3.b2.y.center.bias\", \"encoder.s3.b2.y.surround.weight\", \"encoder.s3.b2.y.surround.bias\", \"encoder.s3.b2.y.proj.weight\", \"encoder.s3.b2.y.proj.bias\", \"encoder.s3.b2.w.h.weight\", \"encoder.s3.b2.w.h.bias\", \"encoder.s3.b2.w.v.weight\", \"encoder.s3.b2.w.v.bias\", \"encoder.s3.b2.w.proj1.weight\", \"encoder.s3.b2.w.proj1.bias\", \"encoder.s3.b2.w.proj2.weight\", \"encoder.s3.b2.w.proj2.bias\", \"encoder.s3.b3.x.center.weight\", \"encoder.s3.b3.x.center.bias\", \"encoder.s3.b3.x.surround.weight\", \"encoder.s3.b3.x.surround.bias\", \"encoder.s3.b3.x.proj.weight\", \"encoder.s3.b3.x.proj.bias\", \"encoder.s3.b3.y.center.weight\", \"encoder.s3.b3.y.center.bias\", \"encoder.s3.b3.y.surround.weight\", \"encoder.s3.b3.y.surround.bias\", \"encoder.s3.b3.y.proj.weight\", \"encoder.s3.b3.y.proj.bias\", \"encoder.s3.b3.w.h.weight\", \"encoder.s3.b3.w.h.bias\", \"encoder.s3.b3.w.v.weight\", \"encoder.s3.b3.w.v.bias\", \"encoder.s3.b3.w.proj1.weight\", \"encoder.s3.b3.w.proj1.bias\", \"encoder.s3.b3.w.proj2.weight\", \"encoder.s3.b3.w.proj2.bias\", \"encoder.s3.short.weight\", \"encoder.s3.short.bias\", \"encoder.s4.b1.x.center.weight\", \"encoder.s4.b1.x.center.bias\", \"encoder.s4.b1.x.surround.weight\", \"encoder.s4.b1.x.surround.bias\", \"encoder.s4.b1.x.proj.weight\", \"encoder.s4.b1.x.proj.bias\", \"encoder.s4.b1.y.center.weight\", \"encoder.s4.b1.y.center.bias\", \"encoder.s4.b1.y.surround.weight\", \"encoder.s4.b1.y.surround.bias\", \"encoder.s4.b1.y.proj.weight\", \"encoder.s4.b1.y.proj.bias\", \"encoder.s4.b1.w.h.weight\", \"encoder.s4.b1.w.h.bias\", \"encoder.s4.b1.w.v.weight\", \"encoder.s4.b1.w.v.bias\", \"encoder.s4.b1.w.proj1.weight\", \"encoder.s4.b1.w.proj1.bias\", \"encoder.s4.b1.w.proj2.weight\", \"encoder.s4.b1.w.proj2.bias\", \"encoder.s4.b2.x.center.weight\", \"encoder.s4.b2.x.center.bias\", \"encoder.s4.b2.x.surround.weight\", \"encoder.s4.b2.x.surround.bias\", \"encoder.s4.b2.x.proj.weight\", \"encoder.s4.b2.x.proj.bias\", \"encoder.s4.b2.y.center.weight\", \"encoder.s4.b2.y.center.bias\", \"encoder.s4.b2.y.surround.weight\", \"encoder.s4.b2.y.surround.bias\", \"encoder.s4.b2.y.proj.weight\", \"encoder.s4.b2.y.proj.bias\", \"encoder.s4.b2.w.h.weight\", \"encoder.s4.b2.w.h.bias\", \"encoder.s4.b2.w.v.weight\", \"encoder.s4.b2.w.v.bias\", \"encoder.s4.b2.w.proj1.weight\", \"encoder.s4.b2.w.proj1.bias\", \"encoder.s4.b2.w.proj2.weight\", \"encoder.s4.b2.w.proj2.bias\", \"encoder.s4.b3.x.center.weight\", \"encoder.s4.b3.x.center.bias\", \"encoder.s4.b3.x.surround.weight\", \"encoder.s4.b3.x.surround.bias\", \"encoder.s4.b3.x.proj.weight\", \"encoder.s4.b3.x.proj.bias\", \"encoder.s4.b3.y.center.weight\", \"encoder.s4.b3.y.center.bias\", \"encoder.s4.b3.y.surround.weight\", \"encoder.s4.b3.y.surround.bias\", \"encoder.s4.b3.y.proj.weight\", \"encoder.s4.b3.y.proj.bias\", \"encoder.s4.b3.w.h.weight\", \"encoder.s4.b3.w.h.bias\", \"encoder.s4.b3.w.v.weight\", \"encoder.s4.b3.w.v.bias\", \"encoder.s4.b3.w.proj1.weight\", \"encoder.s4.b3.w.proj1.bias\", \"encoder.s4.b3.w.proj2.weight\", \"encoder.s4.b3.w.proj2.bias\", \"encoder.s4.short.weight\", \"encoder.s4.short.bias\", \"decoder.f43.deconv_w\", \"decoder.f43.pre1.weight\", \"decoder.f43.pre1.bias\", \"decoder.f43.pre2.weight\", \"decoder.f43.pre2.bias\", \"decoder.f32.deconv_w\", \"decoder.f32.pre1.weight\", \"decoder.f32.pre1.bias\", \"decoder.f32.pre2.weight\", \"decoder.f32.pre2.bias\", \"decoder.f21.deconv_w\", \"decoder.f21.pre1.weight\", \"decoder.f21.pre1.bias\", \"decoder.f21.pre2.weight\", \"decoder.f21.pre2.bias\", \"decoder.elc.pdc.weight\", \"decoder.elc.pdc.bias\", \"decoder.elc.out.weight\", \"decoder.elc.out.bias\". \n\tUnexpected key(s) in state_dict: \"encode.s1.conv1.weight\", \"encode.s1.conv1.bias\", \"encode.s1.xyw1_1.y_c.Ycenter.weight\", \"encode.s1.xyw1_1.y_c.Ycenter.bias\", \"encode.s1.xyw1_1.y_c.Ysurround.weight\", \"encode.s1.xyw1_1.y_c.Ysurround.bias\", \"encode.s1.xyw1_1.y_c.conv1_1.weight\", \"encode.s1.xyw1_1.y_c.conv1_1.bias\", \"encode.s1.xyw1_1.x_c.Xcenter.weight\", \"encode.s1.xyw1_1.x_c.Xcenter.bias\", \"encode.s1.xyw1_1.x_c.Xsurround.weight\", \"encode.s1.xyw1_1.x_c.Xsurround.bias\", \"encode.s1.xyw1_1.x_c.conv1_1.weight\", \"encode.s1.xyw1_1.x_c.conv1_1.bias\", \"encode.s1.xyw1_1.w.h.weight\", \"encode.s1.xyw1_1.w.h.bias\", \"encode.s1.xyw1_1.w.v.weight\", \"encode.s1.xyw1_1.w.v.bias\", \"encode.s1.xyw1_1.w.convh_1.weight\", \"encode.s1.xyw1_1.w.convv_1.weight\", \"encode.s1.xyw1_2.y_c.Ycenter.weight\", \"encode.s1.xyw1_2.y_c.Ycenter.bias\", \"encode.s1.xyw1_2.y_c.Ysurround.weight\", \"encode.s1.xyw1_2.y_c.Ysurround.bias\", \"encode.s1.xyw1_2.y_c.conv1_1.weight\", \"encode.s1.xyw1_2.y_c.conv1_1.bias\", \"encode.s1.xyw1_2.x_c.Xcenter.weight\", \"encode.s1.xyw1_2.x_c.Xcenter.bias\", \"encode.s1.xyw1_2.x_c.Xsurround.weight\", \"encode.s1.xyw1_2.x_c.Xsurround.bias\", \"encode.s1.xyw1_2.x_c.conv1_1.weight\", \"encode.s1.xyw1_2.x_c.conv1_1.bias\", \"encode.s1.xyw1_2.w.h.weight\", \"encode.s1.xyw1_2.w.h.bias\", \"encode.s1.xyw1_2.w.v.weight\", \"encode.s1.xyw1_2.w.v.bias\", \"encode.s1.xyw1_2.w.convh_1.weight\", \"encode.s1.xyw1_2.w.convv_1.weight\", \"encode.s1.xyw1_3.y_c.Ycenter.weight\", \"encode.s1.xyw1_3.y_c.Ycenter.bias\", \"encode.s1.xyw1_3.y_c.Ysurround.weight\", \"encode.s1.xyw1_3.y_c.Ysurround.bias\", \"encode.s1.xyw1_3.y_c.conv1_1.weight\", \"encode.s1.xyw1_3.y_c.conv1_1.bias\", \"encode.s1.xyw1_3.x_c.Xcenter.weight\", \"encode.s1.xyw1_3.x_c.Xcenter.bias\", \"encode.s1.xyw1_3.x_c.Xsurround.weight\", \"encode.s1.xyw1_3.x_c.Xsurround.bias\", \"encode.s1.xyw1_3.x_c.conv1_1.weight\", \"encode.s1.xyw1_3.x_c.conv1_1.bias\", \"encode.s1.xyw1_3.w.h.weight\", \"encode.s1.xyw1_3.w.h.bias\", \"encode.s1.xyw1_3.w.v.weight\", \"encode.s1.xyw1_3.w.v.bias\", \"encode.s1.xyw1_3.w.convh_1.weight\", \"encode.s1.xyw1_3.w.convv_1.weight\", \"encode.s2.xyw2_1.y_c.Ycenter.weight\", \"encode.s2.xyw2_1.y_c.Ycenter.bias\", \"encode.s2.xyw2_1.y_c.Ysurround.weight\", \"encode.s2.xyw2_1.y_c.Ysurround.bias\", \"encode.s2.xyw2_1.y_c.conv1_1.weight\", \"encode.s2.xyw2_1.y_c.conv1_1.bias\", \"encode.s2.xyw2_1.x_c.Xcenter.weight\", \"encode.s2.xyw2_1.x_c.Xcenter.bias\", \"encode.s2.xyw2_1.x_c.Xsurround.weight\", \"encode.s2.xyw2_1.x_c.Xsurround.bias\", \"encode.s2.xyw2_1.x_c.conv1_1.weight\", \"encode.s2.xyw2_1.x_c.conv1_1.bias\", \"encode.s2.xyw2_1.w.h.weight\", \"encode.s2.xyw2_1.w.h.bias\", \"encode.s2.xyw2_1.w.v.weight\", \"encode.s2.xyw2_1.w.v.bias\", \"encode.s2.xyw2_1.w.convh_1.weight\", \"encode.s2.xyw2_1.w.convv_1.weight\", \"encode.s2.xyw2_2.y_c.Ycenter.weight\", \"encode.s2.xyw2_2.y_c.Ycenter.bias\", \"encode.s2.xyw2_2.y_c.Ysurround.weight\", \"encode.s2.xyw2_2.y_c.Ysurround.bias\", \"encode.s2.xyw2_2.y_c.conv1_1.weight\", \"encode.s2.xyw2_2.y_c.conv1_1.bias\", \"encode.s2.xyw2_2.x_c.Xcenter.weight\", \"encode.s2.xyw2_2.x_c.Xcenter.bias\", \"encode.s2.xyw2_2.x_c.Xsurround.weight\", \"encode.s2.xyw2_2.x_c.Xsurround.bias\", \"encode.s2.xyw2_2.x_c.conv1_1.weight\", \"encode.s2.xyw2_2.x_c.conv1_1.bias\", \"encode.s2.xyw2_2.w.h.weight\", \"encode.s2.xyw2_2.w.h.bias\", \"encode.s2.xyw2_2.w.v.weight\", \"encode.s2.xyw2_2.w.v.bias\", \"encode.s2.xyw2_2.w.convh_1.weight\", \"encode.s2.xyw2_2.w.convv_1.weight\", \"encode.s2.xyw2_3.y_c.Ycenter.weight\", \"encode.s2.xyw2_3.y_c.Ycenter.bias\", \"encode.s2.xyw2_3.y_c.Ysurround.weight\", \"encode.s2.xyw2_3.y_c.Ysurround.bias\", \"encode.s2.xyw2_3.y_c.conv1_1.weight\", \"encode.s2.xyw2_3.y_c.conv1_1.bias\", \"encode.s2.xyw2_3.x_c.Xcenter.weight\", \"encode.s2.xyw2_3.x_c.Xcenter.bias\", \"encode.s2.xyw2_3.x_c.Xsurround.weight\", \"encode.s2.xyw2_3.x_c.Xsurround.bias\", \"encode.s2.xyw2_3.x_c.conv1_1.weight\", \"encode.s2.xyw2_3.x_c.conv1_1.bias\", \"encode.s2.xyw2_3.w.h.weight\", \"encode.s2.xyw2_3.w.h.bias\", \"encode.s2.xyw2_3.w.v.weight\", \"encode.s2.xyw2_3.w.v.bias\", \"encode.s2.xyw2_3.w.convh_1.weight\", \"encode.s2.xyw2_3.w.convv_1.weight\", \"encode.s2.shortcut.weight\", \"encode.s2.shortcut.bias\", \"encode.s3.xyw3_1.y_c.Ycenter.weight\", \"encode.s3.xyw3_1.y_c.Ycenter.bias\", \"encode.s3.xyw3_1.y_c.Ysurround.weight\", \"encode.s3.xyw3_1.y_c.Ysurround.bias\", \"encode.s3.xyw3_1.y_c.conv1_1.weight\", \"encode.s3.xyw3_1.y_c.conv1_1.bias\", \"encode.s3.xyw3_1.x_c.Xcenter.weight\", \"encode.s3.xyw3_1.x_c.Xcenter.bias\", \"encode.s3.xyw3_1.x_c.Xsurround.weight\", \"encode.s3.xyw3_1.x_c.Xsurround.bias\", \"encode.s3.xyw3_1.x_c.conv1_1.weight\", \"encode.s3.xyw3_1.x_c.conv1_1.bias\", \"encode.s3.xyw3_1.w.h.weight\", \"encode.s3.xyw3_1.w.h.bias\", \"encode.s3.xyw3_1.w.v.weight\", \"encode.s3.xyw3_1.w.v.bias\", \"encode.s3.xyw3_1.w.convh_1.weight\", \"encode.s3.xyw3_1.w.convv_1.weight\", \"encode.s3.xyw3_2.y_c.Ycenter.weight\", \"encode.s3.xyw3_2.y_c.Ycenter.bias\", \"encode.s3.xyw3_2.y_c.Ysurround.weight\", \"encode.s3.xyw3_2.y_c.Ysurround.bias\", \"encode.s3.xyw3_2.y_c.conv1_1.weight\", \"encode.s3.xyw3_2.y_c.conv1_1.bias\", \"encode.s3.xyw3_2.x_c.Xcenter.weight\", \"encode.s3.xyw3_2.x_c.Xcenter.bias\", \"encode.s3.xyw3_2.x_c.Xsurround.weight\", \"encode.s3.xyw3_2.x_c.Xsurround.bias\", \"encode.s3.xyw3_2.x_c.conv1_1.weight\", \"encode.s3.xyw3_2.x_c.conv1_1.bias\", \"encode.s3.xyw3_2.w.h.weight\", \"encode.s3.xyw3_2.w.h.bias\", \"encode.s3.xyw3_2.w.v.weight\", \"encode.s3.xyw3_2.w.v.bias\", \"encode.s3.xyw3_2.w.convh_1.weight\", \"encode.s3.xyw3_2.w.convv_1.weight\", \"encode.s3.xyw3_3.y_c.Ycenter.weight\", \"encode.s3.xyw3_3.y_c.Ycenter.bias\", \"encode.s3.xyw3_3.y_c.Ysurround.weight\", \"encode.s3.xyw3_3.y_c.Ysurround.bias\", \"encode.s3.xyw3_3.y_c.conv1_1.weight\", \"encode.s3.xyw3_3.y_c.conv1_1.bias\", \"encode.s3.xyw3_3.x_c.Xcenter.weight\", \"encode.s3.xyw3_3.x_c.Xcenter.bias\", \"encode.s3.xyw3_3.x_c.Xsurround.weight\", \"encode.s3.xyw3_3.x_c.Xsurround.bias\", \"encode.s3.xyw3_3.x_c.conv1_1.weight\", \"encode.s3.xyw3_3.x_c.conv1_1.bias\", \"encode.s3.xyw3_3.w.h.weight\", \"encode.s3.xyw3_3.w.h.bias\", \"encode.s3.xyw3_3.w.v.weight\", \"encode.s3.xyw3_3.w.v.bias\", \"encode.s3.xyw3_3.w.convh_1.weight\", \"encode.s3.xyw3_3.w.convv_1.weight\", \"encode.s3.shortcut.weight\", \"encode.s3.shortcut.bias\", \"encode.s4.xyw4_1.y_c.Ycenter.weight\", \"encode.s4.xyw4_1.y_c.Ycenter.bias\", \"encode.s4.xyw4_1.y_c.Ysurround.weight\", \"encode.s4.xyw4_1.y_c.Ysurround.bias\", \"encode.s4.xyw4_1.y_c.conv1_1.weight\", \"encode.s4.xyw4_1.y_c.conv1_1.bias\", \"encode.s4.xyw4_1.x_c.Xcenter.weight\", \"encode.s4.xyw4_1.x_c.Xcenter.bias\", \"encode.s4.xyw4_1.x_c.Xsurround.weight\", \"encode.s4.xyw4_1.x_c.Xsurround.bias\", \"encode.s4.xyw4_1.x_c.conv1_1.weight\", \"encode.s4.xyw4_1.x_c.conv1_1.bias\", \"encode.s4.xyw4_1.w.h.weight\", \"encode.s4.xyw4_1.w.h.bias\", \"encode.s4.xyw4_1.w.v.weight\", \"encode.s4.xyw4_1.w.v.bias\", \"encode.s4.xyw4_1.w.convh_1.weight\", \"encode.s4.xyw4_1.w.convv_1.weight\", \"encode.s4.xyw4_2.y_c.Ycenter.weight\", \"encode.s4.xyw4_2.y_c.Ycenter.bias\", \"encode.s4.xyw4_2.y_c.Ysurround.weight\", \"encode.s4.xyw4_2.y_c.Ysurround.bias\", \"encode.s4.xyw4_2.y_c.conv1_1.weight\", \"encode.s4.xyw4_2.y_c.conv1_1.bias\", \"encode.s4.xyw4_2.x_c.Xcenter.weight\", \"encode.s4.xyw4_2.x_c.Xcenter.bias\", \"encode.s4.xyw4_2.x_c.Xsurround.weight\", \"encode.s4.xyw4_2.x_c.Xsurround.bias\", \"encode.s4.xyw4_2.x_c.conv1_1.weight\", \"encode.s4.xyw4_2.x_c.conv1_1.bias\", \"encode.s4.xyw4_2.w.h.weight\", \"encode.s4.xyw4_2.w.h.bias\", \"encode.s4.xyw4_2.w.v.weight\", \"encode.s4.xyw4_2.w.v.bias\", \"encode.s4.xyw4_2.w.convh_1.weight\", \"encode.s4.xyw4_2.w.convv_1.weight\", \"encode.s4.xyw4_3.y_c.Ycenter.weight\", \"encode.s4.xyw4_3.y_c.Ycenter.bias\", \"encode.s4.xyw4_3.y_c.Ysurround.weight\", \"encode.s4.xyw4_3.y_c.Ysurround.bias\", \"encode.s4.xyw4_3.y_c.conv1_1.weight\", \"encode.s4.xyw4_3.y_c.conv1_1.bias\", \"encode.s4.xyw4_3.x_c.Xcenter.weight\", \"encode.s4.xyw4_3.x_c.Xcenter.bias\", \"encode.s4.xyw4_3.x_c.Xsurround.weight\", \"encode.s4.xyw4_3.x_c.Xsurround.bias\", \"encode.s4.xyw4_3.x_c.conv1_1.weight\", \"encode.s4.xyw4_3.x_c.conv1_1.bias\", \"encode.s4.xyw4_3.w.h.weight\", \"encode.s4.xyw4_3.w.h.bias\", \"encode.s4.xyw4_3.w.v.weight\", \"encode.s4.xyw4_3.w.v.bias\", \"encode.s4.xyw4_3.w.convh_1.weight\", \"encode.s4.xyw4_3.w.convv_1.weight\", \"encode.s4.shortcut.weight\", \"encode.s4.shortcut.bias\", \"decode.f43.deconv_weight\", \"decode.f43.pre_conv1.weight\", \"decode.f43.pre_conv1.conv.0.weight\", \"decode.f43.pre_conv1.conv.0.bias\", \"decode.f43.pre_conv2.weight\", \"decode.f43.pre_conv2.conv.0.weight\", \"decode.f43.pre_conv2.conv.0.bias\", \"decode.f32.deconv_weight\", \"decode.f32.pre_conv1.weight\", \"decode.f32.pre_conv1.conv.0.weight\", \"decode.f32.pre_conv1.conv.0.bias\", \"decode.f32.pre_conv2.weight\", \"decode.f32.pre_conv2.conv.0.weight\", \"decode.f32.pre_conv2.conv.0.bias\", \"decode.f21.deconv_weight\", \"decode.f21.pre_conv1.weight\", \"decode.f21.pre_conv1.conv.0.weight\", \"decode.f21.pre_conv1.conv.0.bias\", \"decode.f21.pre_conv2.weight\", \"decode.f21.pre_conv2.conv.0.weight\", \"decode.f21.pre_conv2.conv.0.bias\", \"decode.f.weight\", \"decode.f.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 33\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading pretrained weights from 20epoch.pth ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# KAGGLE PATH (comment out when not using Kaggle):\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# model.load_state_dict(torch.load(\"/kaggle/input/20epoch/pytorch/default/1/20epoch.pth\", map_location=device))\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# LOCAL PATH:\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/20epoch.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResuming training from epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Training history\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2580\u001b[0m             ),\n\u001b[0;32m   2581\u001b[0m         )\n\u001b[0;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2587\u001b[0m         )\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for XYWNet:\n\tMissing key(s) in state_dict: \"encoder.s1.stem.weight\", \"encoder.s1.stem.bias\", \"encoder.s1.b1.x.center.weight\", \"encoder.s1.b1.x.center.bias\", \"encoder.s1.b1.x.surround.weight\", \"encoder.s1.b1.x.surround.bias\", \"encoder.s1.b1.x.proj.weight\", \"encoder.s1.b1.x.proj.bias\", \"encoder.s1.b1.y.center.weight\", \"encoder.s1.b1.y.center.bias\", \"encoder.s1.b1.y.surround.weight\", \"encoder.s1.b1.y.surround.bias\", \"encoder.s1.b1.y.proj.weight\", \"encoder.s1.b1.y.proj.bias\", \"encoder.s1.b1.w.h.weight\", \"encoder.s1.b1.w.h.bias\", \"encoder.s1.b1.w.v.weight\", \"encoder.s1.b1.w.v.bias\", \"encoder.s1.b1.w.proj1.weight\", \"encoder.s1.b1.w.proj1.bias\", \"encoder.s1.b1.w.proj2.weight\", \"encoder.s1.b1.w.proj2.bias\", \"encoder.s1.b2.x.center.weight\", \"encoder.s1.b2.x.center.bias\", \"encoder.s1.b2.x.surround.weight\", \"encoder.s1.b2.x.surround.bias\", \"encoder.s1.b2.x.proj.weight\", \"encoder.s1.b2.x.proj.bias\", \"encoder.s1.b2.y.center.weight\", \"encoder.s1.b2.y.center.bias\", \"encoder.s1.b2.y.surround.weight\", \"encoder.s1.b2.y.surround.bias\", \"encoder.s1.b2.y.proj.weight\", \"encoder.s1.b2.y.proj.bias\", \"encoder.s1.b2.w.h.weight\", \"encoder.s1.b2.w.h.bias\", \"encoder.s1.b2.w.v.weight\", \"encoder.s1.b2.w.v.bias\", \"encoder.s1.b2.w.proj1.weight\", \"encoder.s1.b2.w.proj1.bias\", \"encoder.s1.b2.w.proj2.weight\", \"encoder.s1.b2.w.proj2.bias\", \"encoder.s1.b3.x.center.weight\", \"encoder.s1.b3.x.center.bias\", \"encoder.s1.b3.x.surround.weight\", \"encoder.s1.b3.x.surround.bias\", \"encoder.s1.b3.x.proj.weight\", \"encoder.s1.b3.x.proj.bias\", \"encoder.s1.b3.y.center.weight\", \"encoder.s1.b3.y.center.bias\", \"encoder.s1.b3.y.surround.weight\", \"encoder.s1.b3.y.surround.bias\", \"encoder.s1.b3.y.proj.weight\", \"encoder.s1.b3.y.proj.bias\", \"encoder.s1.b3.w.h.weight\", \"encoder.s1.b3.w.h.bias\", \"encoder.s1.b3.w.v.weight\", \"encoder.s1.b3.w.v.bias\", \"encoder.s1.b3.w.proj1.weight\", \"encoder.s1.b3.w.proj1.bias\", \"encoder.s1.b3.w.proj2.weight\", \"encoder.s1.b3.w.proj2.bias\", \"encoder.s2.b1.x.center.weight\", \"encoder.s2.b1.x.center.bias\", \"encoder.s2.b1.x.surround.weight\", \"encoder.s2.b1.x.surround.bias\", \"encoder.s2.b1.x.proj.weight\", \"encoder.s2.b1.x.proj.bias\", \"encoder.s2.b1.y.center.weight\", \"encoder.s2.b1.y.center.bias\", \"encoder.s2.b1.y.surround.weight\", \"encoder.s2.b1.y.surround.bias\", \"encoder.s2.b1.y.proj.weight\", \"encoder.s2.b1.y.proj.bias\", \"encoder.s2.b1.w.h.weight\", \"encoder.s2.b1.w.h.bias\", \"encoder.s2.b1.w.v.weight\", \"encoder.s2.b1.w.v.bias\", \"encoder.s2.b1.w.proj1.weight\", \"encoder.s2.b1.w.proj1.bias\", \"encoder.s2.b1.w.proj2.weight\", \"encoder.s2.b1.w.proj2.bias\", \"encoder.s2.b2.x.center.weight\", \"encoder.s2.b2.x.center.bias\", \"encoder.s2.b2.x.surround.weight\", \"encoder.s2.b2.x.surround.bias\", \"encoder.s2.b2.x.proj.weight\", \"encoder.s2.b2.x.proj.bias\", \"encoder.s2.b2.y.center.weight\", \"encoder.s2.b2.y.center.bias\", \"encoder.s2.b2.y.surround.weight\", \"encoder.s2.b2.y.surround.bias\", \"encoder.s2.b2.y.proj.weight\", \"encoder.s2.b2.y.proj.bias\", \"encoder.s2.b2.w.h.weight\", \"encoder.s2.b2.w.h.bias\", \"encoder.s2.b2.w.v.weight\", \"encoder.s2.b2.w.v.bias\", \"encoder.s2.b2.w.proj1.weight\", \"encoder.s2.b2.w.proj1.bias\", \"encoder.s2.b2.w.proj2.weight\", \"encoder.s2.b2.w.proj2.bias\", \"encoder.s2.b3.x.center.weight\", \"encoder.s2.b3.x.center.bias\", \"encoder.s2.b3.x.surround.weight\", \"encoder.s2.b3.x.surround.bias\", \"encoder.s2.b3.x.proj.weight\", \"encoder.s2.b3.x.proj.bias\", \"encoder.s2.b3.y.center.weight\", \"encoder.s2.b3.y.center.bias\", \"encoder.s2.b3.y.surround.weight\", \"encoder.s2.b3.y.surround.bias\", \"encoder.s2.b3.y.proj.weight\", \"encoder.s2.b3.y.proj.bias\", \"encoder.s2.b3.w.h.weight\", \"encoder.s2.b3.w.h.bias\", \"encoder.s2.b3.w.v.weight\", \"encoder.s2.b3.w.v.bias\", \"encoder.s2.b3.w.proj1.weight\", \"encoder.s2.b3.w.proj1.bias\", \"encoder.s2.b3.w.proj2.weight\", \"encoder.s2.b3.w.proj2.bias\", \"encoder.s2.short.weight\", \"encoder.s2.short.bias\", \"encoder.s3.b1.x.center.weight\", \"encoder.s3.b1.x.center.bias\", \"encoder.s3.b1.x.surround.weight\", \"encoder.s3.b1.x.surround.bias\", \"encoder.s3.b1.x.proj.weight\", \"encoder.s3.b1.x.proj.bias\", \"encoder.s3.b1.y.center.weight\", \"encoder.s3.b1.y.center.bias\", \"encoder.s3.b1.y.surround.weight\", \"encoder.s3.b1.y.surround.bias\", \"encoder.s3.b1.y.proj.weight\", \"encoder.s3.b1.y.proj.bias\", \"encoder.s3.b1.w.h.weight\", \"encoder.s3.b1.w.h.bias\", \"encoder.s3.b1.w.v.weight\", \"encoder.s3.b1.w.v.bias\", \"encoder.s3.b1.w.proj1.weight\", \"encoder.s3.b1.w.proj1.bias\", \"encoder.s3.b1.w.proj2.weight\", \"encoder.s3.b1.w.proj2.bias\", \"encoder.s3.b2.x.center.weight\", \"encoder.s3.b2.x.center.bias\", \"encoder.s3.b2.x.surround.weight\", \"encoder.s3.b2.x.surround.bias\", \"encoder.s3.b2.x.proj.weight\", \"encoder.s3.b2.x.proj.bias\", \"encoder.s3.b2.y.center.weight\", \"encoder.s3.b2.y.center.bias\", \"encoder.s3.b2.y.surround.weight\", \"encoder.s3.b2.y.surround.bias\", \"encoder.s3.b2.y.proj.weight\", \"encoder.s3.b2.y.proj.bias\", \"encoder.s3.b2.w.h.weight\", \"encoder.s3.b2.w.h.bias\", \"encoder.s3.b2.w.v.weight\", \"encoder.s3.b2.w.v.bias\", \"encoder.s3.b2.w.proj1.weight\", \"encoder.s3.b2.w.proj1.bias\", \"encoder.s3.b2.w.proj2.weight\", \"encoder.s3.b2.w.proj2.bias\", \"encoder.s3.b3.x.center.weight\", \"encoder.s3.b3.x.center.bias\", \"encoder.s3.b3.x.surround.weight\", \"encoder.s3.b3.x.surround.bias\", \"encoder.s3.b3.x.proj.weight\", \"encoder.s3.b3.x.proj.bias\", \"encoder.s3.b3.y.center.weight\", \"encoder.s3.b3.y.center.bias\", \"encoder.s3.b3.y.surround.weight\", \"encoder.s3.b3.y.surround.bias\", \"encoder.s3.b3.y.proj.weight\", \"encoder.s3.b3.y.proj.bias\", \"encoder.s3.b3.w.h.weight\", \"encoder.s3.b3.w.h.bias\", \"encoder.s3.b3.w.v.weight\", \"encoder.s3.b3.w.v.bias\", \"encoder.s3.b3.w.proj1.weight\", \"encoder.s3.b3.w.proj1.bias\", \"encoder.s3.b3.w.proj2.weight\", \"encoder.s3.b3.w.proj2.bias\", \"encoder.s3.short.weight\", \"encoder.s3.short.bias\", \"encoder.s4.b1.x.center.weight\", \"encoder.s4.b1.x.center.bias\", \"encoder.s4.b1.x.surround.weight\", \"encoder.s4.b1.x.surround.bias\", \"encoder.s4.b1.x.proj.weight\", \"encoder.s4.b1.x.proj.bias\", \"encoder.s4.b1.y.center.weight\", \"encoder.s4.b1.y.center.bias\", \"encoder.s4.b1.y.surround.weight\", \"encoder.s4.b1.y.surround.bias\", \"encoder.s4.b1.y.proj.weight\", \"encoder.s4.b1.y.proj.bias\", \"encoder.s4.b1.w.h.weight\", \"encoder.s4.b1.w.h.bias\", \"encoder.s4.b1.w.v.weight\", \"encoder.s4.b1.w.v.bias\", \"encoder.s4.b1.w.proj1.weight\", \"encoder.s4.b1.w.proj1.bias\", \"encoder.s4.b1.w.proj2.weight\", \"encoder.s4.b1.w.proj2.bias\", \"encoder.s4.b2.x.center.weight\", \"encoder.s4.b2.x.center.bias\", \"encoder.s4.b2.x.surround.weight\", \"encoder.s4.b2.x.surround.bias\", \"encoder.s4.b2.x.proj.weight\", \"encoder.s4.b2.x.proj.bias\", \"encoder.s4.b2.y.center.weight\", \"encoder.s4.b2.y.center.bias\", \"encoder.s4.b2.y.surround.weight\", \"encoder.s4.b2.y.surround.bias\", \"encoder.s4.b2.y.proj.weight\", \"encoder.s4.b2.y.proj.bias\", \"encoder.s4.b2.w.h.weight\", \"encoder.s4.b2.w.h.bias\", \"encoder.s4.b2.w.v.weight\", \"encoder.s4.b2.w.v.bias\", \"encoder.s4.b2.w.proj1.weight\", \"encoder.s4.b2.w.proj1.bias\", \"encoder.s4.b2.w.proj2.weight\", \"encoder.s4.b2.w.proj2.bias\", \"encoder.s4.b3.x.center.weight\", \"encoder.s4.b3.x.center.bias\", \"encoder.s4.b3.x.surround.weight\", \"encoder.s4.b3.x.surround.bias\", \"encoder.s4.b3.x.proj.weight\", \"encoder.s4.b3.x.proj.bias\", \"encoder.s4.b3.y.center.weight\", \"encoder.s4.b3.y.center.bias\", \"encoder.s4.b3.y.surround.weight\", \"encoder.s4.b3.y.surround.bias\", \"encoder.s4.b3.y.proj.weight\", \"encoder.s4.b3.y.proj.bias\", \"encoder.s4.b3.w.h.weight\", \"encoder.s4.b3.w.h.bias\", \"encoder.s4.b3.w.v.weight\", \"encoder.s4.b3.w.v.bias\", \"encoder.s4.b3.w.proj1.weight\", \"encoder.s4.b3.w.proj1.bias\", \"encoder.s4.b3.w.proj2.weight\", \"encoder.s4.b3.w.proj2.bias\", \"encoder.s4.short.weight\", \"encoder.s4.short.bias\", \"decoder.f43.deconv_w\", \"decoder.f43.pre1.weight\", \"decoder.f43.pre1.bias\", \"decoder.f43.pre2.weight\", \"decoder.f43.pre2.bias\", \"decoder.f32.deconv_w\", \"decoder.f32.pre1.weight\", \"decoder.f32.pre1.bias\", \"decoder.f32.pre2.weight\", \"decoder.f32.pre2.bias\", \"decoder.f21.deconv_w\", \"decoder.f21.pre1.weight\", \"decoder.f21.pre1.bias\", \"decoder.f21.pre2.weight\", \"decoder.f21.pre2.bias\", \"decoder.elc.pdc.weight\", \"decoder.elc.pdc.bias\", \"decoder.elc.out.weight\", \"decoder.elc.out.bias\". \n\tUnexpected key(s) in state_dict: \"encode.s1.conv1.weight\", \"encode.s1.conv1.bias\", \"encode.s1.xyw1_1.y_c.Ycenter.weight\", \"encode.s1.xyw1_1.y_c.Ycenter.bias\", \"encode.s1.xyw1_1.y_c.Ysurround.weight\", \"encode.s1.xyw1_1.y_c.Ysurround.bias\", \"encode.s1.xyw1_1.y_c.conv1_1.weight\", \"encode.s1.xyw1_1.y_c.conv1_1.bias\", \"encode.s1.xyw1_1.x_c.Xcenter.weight\", \"encode.s1.xyw1_1.x_c.Xcenter.bias\", \"encode.s1.xyw1_1.x_c.Xsurround.weight\", \"encode.s1.xyw1_1.x_c.Xsurround.bias\", \"encode.s1.xyw1_1.x_c.conv1_1.weight\", \"encode.s1.xyw1_1.x_c.conv1_1.bias\", \"encode.s1.xyw1_1.w.h.weight\", \"encode.s1.xyw1_1.w.h.bias\", \"encode.s1.xyw1_1.w.v.weight\", \"encode.s1.xyw1_1.w.v.bias\", \"encode.s1.xyw1_1.w.convh_1.weight\", \"encode.s1.xyw1_1.w.convv_1.weight\", \"encode.s1.xyw1_2.y_c.Ycenter.weight\", \"encode.s1.xyw1_2.y_c.Ycenter.bias\", \"encode.s1.xyw1_2.y_c.Ysurround.weight\", \"encode.s1.xyw1_2.y_c.Ysurround.bias\", \"encode.s1.xyw1_2.y_c.conv1_1.weight\", \"encode.s1.xyw1_2.y_c.conv1_1.bias\", \"encode.s1.xyw1_2.x_c.Xcenter.weight\", \"encode.s1.xyw1_2.x_c.Xcenter.bias\", \"encode.s1.xyw1_2.x_c.Xsurround.weight\", \"encode.s1.xyw1_2.x_c.Xsurround.bias\", \"encode.s1.xyw1_2.x_c.conv1_1.weight\", \"encode.s1.xyw1_2.x_c.conv1_1.bias\", \"encode.s1.xyw1_2.w.h.weight\", \"encode.s1.xyw1_2.w.h.bias\", \"encode.s1.xyw1_2.w.v.weight\", \"encode.s1.xyw1_2.w.v.bias\", \"encode.s1.xyw1_2.w.convh_1.weight\", \"encode.s1.xyw1_2.w.convv_1.weight\", \"encode.s1.xyw1_3.y_c.Ycenter.weight\", \"encode.s1.xyw1_3.y_c.Ycenter.bias\", \"encode.s1.xyw1_3.y_c.Ysurround.weight\", \"encode.s1.xyw1_3.y_c.Ysurround.bias\", \"encode.s1.xyw1_3.y_c.conv1_1.weight\", \"encode.s1.xyw1_3.y_c.conv1_1.bias\", \"encode.s1.xyw1_3.x_c.Xcenter.weight\", \"encode.s1.xyw1_3.x_c.Xcenter.bias\", \"encode.s1.xyw1_3.x_c.Xsurround.weight\", \"encode.s1.xyw1_3.x_c.Xsurround.bias\", \"encode.s1.xyw1_3.x_c.conv1_1.weight\", \"encode.s1.xyw1_3.x_c.conv1_1.bias\", \"encode.s1.xyw1_3.w.h.weight\", \"encode.s1.xyw1_3.w.h.bias\", \"encode.s1.xyw1_3.w.v.weight\", \"encode.s1.xyw1_3.w.v.bias\", \"encode.s1.xyw1_3.w.convh_1.weight\", \"encode.s1.xyw1_3.w.convv_1.weight\", \"encode.s2.xyw2_1.y_c.Ycenter.weight\", \"encode.s2.xyw2_1.y_c.Ycenter.bias\", \"encode.s2.xyw2_1.y_c.Ysurround.weight\", \"encode.s2.xyw2_1.y_c.Ysurround.bias\", \"encode.s2.xyw2_1.y_c.conv1_1.weight\", \"encode.s2.xyw2_1.y_c.conv1_1.bias\", \"encode.s2.xyw2_1.x_c.Xcenter.weight\", \"encode.s2.xyw2_1.x_c.Xcenter.bias\", \"encode.s2.xyw2_1.x_c.Xsurround.weight\", \"encode.s2.xyw2_1.x_c.Xsurround.bias\", \"encode.s2.xyw2_1.x_c.conv1_1.weight\", \"encode.s2.xyw2_1.x_c.conv1_1.bias\", \"encode.s2.xyw2_1.w.h.weight\", \"encode.s2.xyw2_1.w.h.bias\", \"encode.s2.xyw2_1.w.v.weight\", \"encode.s2.xyw2_1.w.v.bias\", \"encode.s2.xyw2_1.w.convh_1.weight\", \"encode.s2.xyw2_1.w.convv_1.weight\", \"encode.s2.xyw2_2.y_c.Ycenter.weight\", \"encode.s2.xyw2_2.y_c.Ycenter.bias\", \"encode.s2.xyw2_2.y_c.Ysurround.weight\", \"encode.s2.xyw2_2.y_c.Ysurround.bias\", \"encode.s2.xyw2_2.y_c.conv1_1.weight\", \"encode.s2.xyw2_2.y_c.conv1_1.bias\", \"encode.s2.xyw2_2.x_c.Xcenter.weight\", \"encode.s2.xyw2_2.x_c.Xcenter.bias\", \"encode.s2.xyw2_2.x_c.Xsurround.weight\", \"encode.s2.xyw2_2.x_c.Xsurround.bias\", \"encode.s2.xyw2_2.x_c.conv1_1.weight\", \"encode.s2.xyw2_2.x_c.conv1_1.bias\", \"encode.s2.xyw2_2.w.h.weight\", \"encode.s2.xyw2_2.w.h.bias\", \"encode.s2.xyw2_2.w.v.weight\", \"encode.s2.xyw2_2.w.v.bias\", \"encode.s2.xyw2_2.w.convh_1.weight\", \"encode.s2.xyw2_2.w.convv_1.weight\", \"encode.s2.xyw2_3.y_c.Ycenter.weight\", \"encode.s2.xyw2_3.y_c.Ycenter.bias\", \"encode.s2.xyw2_3.y_c.Ysurround.weight\", \"encode.s2.xyw2_3.y_c.Ysurround.bias\", \"encode.s2.xyw2_3.y_c.conv1_1.weight\", \"encode.s2.xyw2_3.y_c.conv1_1.bias\", \"encode.s2.xyw2_3.x_c.Xcenter.weight\", \"encode.s2.xyw2_3.x_c.Xcenter.bias\", \"encode.s2.xyw2_3.x_c.Xsurround.weight\", \"encode.s2.xyw2_3.x_c.Xsurround.bias\", \"encode.s2.xyw2_3.x_c.conv1_1.weight\", \"encode.s2.xyw2_3.x_c.conv1_1.bias\", \"encode.s2.xyw2_3.w.h.weight\", \"encode.s2.xyw2_3.w.h.bias\", \"encode.s2.xyw2_3.w.v.weight\", \"encode.s2.xyw2_3.w.v.bias\", \"encode.s2.xyw2_3.w.convh_1.weight\", \"encode.s2.xyw2_3.w.convv_1.weight\", \"encode.s2.shortcut.weight\", \"encode.s2.shortcut.bias\", \"encode.s3.xyw3_1.y_c.Ycenter.weight\", \"encode.s3.xyw3_1.y_c.Ycenter.bias\", \"encode.s3.xyw3_1.y_c.Ysurround.weight\", \"encode.s3.xyw3_1.y_c.Ysurround.bias\", \"encode.s3.xyw3_1.y_c.conv1_1.weight\", \"encode.s3.xyw3_1.y_c.conv1_1.bias\", \"encode.s3.xyw3_1.x_c.Xcenter.weight\", \"encode.s3.xyw3_1.x_c.Xcenter.bias\", \"encode.s3.xyw3_1.x_c.Xsurround.weight\", \"encode.s3.xyw3_1.x_c.Xsurround.bias\", \"encode.s3.xyw3_1.x_c.conv1_1.weight\", \"encode.s3.xyw3_1.x_c.conv1_1.bias\", \"encode.s3.xyw3_1.w.h.weight\", \"encode.s3.xyw3_1.w.h.bias\", \"encode.s3.xyw3_1.w.v.weight\", \"encode.s3.xyw3_1.w.v.bias\", \"encode.s3.xyw3_1.w.convh_1.weight\", \"encode.s3.xyw3_1.w.convv_1.weight\", \"encode.s3.xyw3_2.y_c.Ycenter.weight\", \"encode.s3.xyw3_2.y_c.Ycenter.bias\", \"encode.s3.xyw3_2.y_c.Ysurround.weight\", \"encode.s3.xyw3_2.y_c.Ysurround.bias\", \"encode.s3.xyw3_2.y_c.conv1_1.weight\", \"encode.s3.xyw3_2.y_c.conv1_1.bias\", \"encode.s3.xyw3_2.x_c.Xcenter.weight\", \"encode.s3.xyw3_2.x_c.Xcenter.bias\", \"encode.s3.xyw3_2.x_c.Xsurround.weight\", \"encode.s3.xyw3_2.x_c.Xsurround.bias\", \"encode.s3.xyw3_2.x_c.conv1_1.weight\", \"encode.s3.xyw3_2.x_c.conv1_1.bias\", \"encode.s3.xyw3_2.w.h.weight\", \"encode.s3.xyw3_2.w.h.bias\", \"encode.s3.xyw3_2.w.v.weight\", \"encode.s3.xyw3_2.w.v.bias\", \"encode.s3.xyw3_2.w.convh_1.weight\", \"encode.s3.xyw3_2.w.convv_1.weight\", \"encode.s3.xyw3_3.y_c.Ycenter.weight\", \"encode.s3.xyw3_3.y_c.Ycenter.bias\", \"encode.s3.xyw3_3.y_c.Ysurround.weight\", \"encode.s3.xyw3_3.y_c.Ysurround.bias\", \"encode.s3.xyw3_3.y_c.conv1_1.weight\", \"encode.s3.xyw3_3.y_c.conv1_1.bias\", \"encode.s3.xyw3_3.x_c.Xcenter.weight\", \"encode.s3.xyw3_3.x_c.Xcenter.bias\", \"encode.s3.xyw3_3.x_c.Xsurround.weight\", \"encode.s3.xyw3_3.x_c.Xsurround.bias\", \"encode.s3.xyw3_3.x_c.conv1_1.weight\", \"encode.s3.xyw3_3.x_c.conv1_1.bias\", \"encode.s3.xyw3_3.w.h.weight\", \"encode.s3.xyw3_3.w.h.bias\", \"encode.s3.xyw3_3.w.v.weight\", \"encode.s3.xyw3_3.w.v.bias\", \"encode.s3.xyw3_3.w.convh_1.weight\", \"encode.s3.xyw3_3.w.convv_1.weight\", \"encode.s3.shortcut.weight\", \"encode.s3.shortcut.bias\", \"encode.s4.xyw4_1.y_c.Ycenter.weight\", \"encode.s4.xyw4_1.y_c.Ycenter.bias\", \"encode.s4.xyw4_1.y_c.Ysurround.weight\", \"encode.s4.xyw4_1.y_c.Ysurround.bias\", \"encode.s4.xyw4_1.y_c.conv1_1.weight\", \"encode.s4.xyw4_1.y_c.conv1_1.bias\", \"encode.s4.xyw4_1.x_c.Xcenter.weight\", \"encode.s4.xyw4_1.x_c.Xcenter.bias\", \"encode.s4.xyw4_1.x_c.Xsurround.weight\", \"encode.s4.xyw4_1.x_c.Xsurround.bias\", \"encode.s4.xyw4_1.x_c.conv1_1.weight\", \"encode.s4.xyw4_1.x_c.conv1_1.bias\", \"encode.s4.xyw4_1.w.h.weight\", \"encode.s4.xyw4_1.w.h.bias\", \"encode.s4.xyw4_1.w.v.weight\", \"encode.s4.xyw4_1.w.v.bias\", \"encode.s4.xyw4_1.w.convh_1.weight\", \"encode.s4.xyw4_1.w.convv_1.weight\", \"encode.s4.xyw4_2.y_c.Ycenter.weight\", \"encode.s4.xyw4_2.y_c.Ycenter.bias\", \"encode.s4.xyw4_2.y_c.Ysurround.weight\", \"encode.s4.xyw4_2.y_c.Ysurround.bias\", \"encode.s4.xyw4_2.y_c.conv1_1.weight\", \"encode.s4.xyw4_2.y_c.conv1_1.bias\", \"encode.s4.xyw4_2.x_c.Xcenter.weight\", \"encode.s4.xyw4_2.x_c.Xcenter.bias\", \"encode.s4.xyw4_2.x_c.Xsurround.weight\", \"encode.s4.xyw4_2.x_c.Xsurround.bias\", \"encode.s4.xyw4_2.x_c.conv1_1.weight\", \"encode.s4.xyw4_2.x_c.conv1_1.bias\", \"encode.s4.xyw4_2.w.h.weight\", \"encode.s4.xyw4_2.w.h.bias\", \"encode.s4.xyw4_2.w.v.weight\", \"encode.s4.xyw4_2.w.v.bias\", \"encode.s4.xyw4_2.w.convh_1.weight\", \"encode.s4.xyw4_2.w.convv_1.weight\", \"encode.s4.xyw4_3.y_c.Ycenter.weight\", \"encode.s4.xyw4_3.y_c.Ycenter.bias\", \"encode.s4.xyw4_3.y_c.Ysurround.weight\", \"encode.s4.xyw4_3.y_c.Ysurround.bias\", \"encode.s4.xyw4_3.y_c.conv1_1.weight\", \"encode.s4.xyw4_3.y_c.conv1_1.bias\", \"encode.s4.xyw4_3.x_c.Xcenter.weight\", \"encode.s4.xyw4_3.x_c.Xcenter.bias\", \"encode.s4.xyw4_3.x_c.Xsurround.weight\", \"encode.s4.xyw4_3.x_c.Xsurround.bias\", \"encode.s4.xyw4_3.x_c.conv1_1.weight\", \"encode.s4.xyw4_3.x_c.conv1_1.bias\", \"encode.s4.xyw4_3.w.h.weight\", \"encode.s4.xyw4_3.w.h.bias\", \"encode.s4.xyw4_3.w.v.weight\", \"encode.s4.xyw4_3.w.v.bias\", \"encode.s4.xyw4_3.w.convh_1.weight\", \"encode.s4.xyw4_3.w.convv_1.weight\", \"encode.s4.shortcut.weight\", \"encode.s4.shortcut.bias\", \"decode.f43.deconv_weight\", \"decode.f43.pre_conv1.weight\", \"decode.f43.pre_conv1.conv.0.weight\", \"decode.f43.pre_conv1.conv.0.bias\", \"decode.f43.pre_conv2.weight\", \"decode.f43.pre_conv2.conv.0.weight\", \"decode.f43.pre_conv2.conv.0.bias\", \"decode.f32.deconv_weight\", \"decode.f32.pre_conv1.weight\", \"decode.f32.pre_conv1.conv.0.weight\", \"decode.f32.pre_conv1.conv.0.bias\", \"decode.f32.pre_conv2.weight\", \"decode.f32.pre_conv2.conv.0.weight\", \"decode.f32.pre_conv2.conv.0.bias\", \"decode.f21.deconv_weight\", \"decode.f21.pre_conv1.weight\", \"decode.f21.pre_conv1.conv.0.weight\", \"decode.f21.pre_conv1.conv.0.bias\", \"decode.f21.pre_conv2.weight\", \"decode.f21.pre_conv2.conv.0.weight\", \"decode.f21.pre_conv2.conv.0.bias\", \"decode.f.weight\", \"decode.f.bias\". "
     ]
    }
   ],
   "source": [
    "# # Cell 7: Train XYW-Net (RESUME + FULL CHECKPOINT + PT EXPORT)\n",
    "\n",
    "# # -------------------------------\n",
    "# # Hyperparameters\n",
    "# # -------------------------------\n",
    "# NUM_EPOCHS = 20          # Total epochs you want after resume\n",
    "# LEARNING_RATE = 1e-4\n",
    "# WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# # -------------------------------\n",
    "# # Initialize\n",
    "# # -------------------------------\n",
    "# model = XYWNet().to(device)\n",
    "# criterion = EdgeLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# # -------------------------------\n",
    "# # Create models directory if it doesn't exist\n",
    "# # -------------------------------\n",
    "# import os\n",
    "# os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# # -------------------------------\n",
    "# #  RESUME FROM STATE_DICT (19 EPOCHS DONE)\n",
    "# # -------------------------------\n",
    "# start_epoch = 19  # You already trained 19 epochs\n",
    "\n",
    "# print(\"Loading pretrained weights from 20epoch.pth ...\")\n",
    "# # KAGGLE PATH (comment out when not using Kaggle):\n",
    "# # model.load_state_dict(torch.load(\"/kaggle/input/20epoch/pytorch/default/1/20epoch.pth\", map_location=device))\n",
    "# # LOCAL PATH:\n",
    "# model.load_state_dict(torch.load(\"models/20epoch.pth\", map_location=device))\n",
    "\n",
    "# print(f\"Resuming training from epoch {start_epoch+1}\")\n",
    "\n",
    "# # -------------------------------\n",
    "# # Training history\n",
    "# # -------------------------------\n",
    "# history = {'train_loss': [], 'val_ods': [], 'val_ois': [], 'val_ap': []}\n",
    "# best_ods = 0\n",
    "\n",
    "# print(f\"Training XYW-Net for {NUM_EPOCHS} total epochs...\")\n",
    "# print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# # -------------------------------\n",
    "# #  FULL CHECKPOINT SAVE FUNCTION\n",
    "# # -------------------------------\n",
    "# def save_checkpoint(model, optimizer, scheduler, epoch, best_ods, path):\n",
    "#     torch.save({\n",
    "#         \"epoch\": epoch,\n",
    "#         \"model_state\": model.state_dict(),\n",
    "#         \"optimizer_state\": optimizer.state_dict(),\n",
    "#         \"scheduler_state\": scheduler.state_dict(),\n",
    "#         \"best_ods\": best_ods\n",
    "#     }, path)\n",
    "\n",
    "# # -------------------------------\n",
    "# #  TRAINING LOOP\n",
    "# # -------------------------------\n",
    "# for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "#     print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "#     # Train\n",
    "#     train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "#     history['train_loss'].append(train_loss)\n",
    "    \n",
    "#     # Evaluate\n",
    "#     val_ods, val_ois, val_ap = evaluate(model, val_loader, device)\n",
    "#     history['val_ods'].append(val_ods)\n",
    "#     history['val_ois'].append(val_ois)\n",
    "#     history['val_ap'].append(val_ap)\n",
    "    \n",
    "#     scheduler.step()\n",
    "    \n",
    "#     print(f\"Loss: {train_loss:.4f} | ODS: {val_ods:.4f} | OIS: {val_ois:.4f} | AP: {val_ap:.4f}\")\n",
    "    \n",
    "#     #  SAVE BEST MODEL (STATE_DICT)\n",
    "#     if val_ods > best_ods: \n",
    "#         best_ods = val_ods\n",
    "        \n",
    "#         # Save epoch checkpoint and best model\n",
    "#         torch.save(model.state_dict(), f\"models/{epoch+1}epoch.pth\")\n",
    "#         torch.save(model.state_dict(), \"models/best_xyw_net.pth\")\n",
    "        \n",
    "#         print(f\"  -> Saved best model (ODS: {best_ods:.4f})\")\n",
    "#         print(\"------------\")\n",
    "        \n",
    "#         #  SAVE FULL CHECKPOINT\n",
    "#         save_checkpoint(\n",
    "#             model,\n",
    "#             optimizer,\n",
    "#             scheduler,\n",
    "#             epoch,\n",
    "#             best_ods,\n",
    "#             \"models/xywnet_full_checkpoint.pth\"\n",
    "#         )\n",
    "        \n",
    "#         # -------------------------------\n",
    "#         #  EXPORT DEPLOYMENT .pt MODEL\n",
    "#         # -------------------------------\n",
    "#         # Use a real image from the dataset for tracing (better than random)\n",
    "#         real_sample = train_dataset[0]\n",
    "#         example_input = real_sample['images'].unsqueeze(0).to(device)\n",
    "        \n",
    "#         # Trace the model with real data\n",
    "#         traced_model = torch.jit.trace(model, example_input)\n",
    "#         traced_model.save(\"models/xywnet_model.pt\")\n",
    "        \n",
    "#         print(f\"  -> Saved epoch checkpoint: models/{epoch+1}epoch.pth\")\n",
    "#         print(f\"  -> Saved traced model: models/xywnet_model.pt\")\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "# print(f\"Training complete. Best ODS: {best_ods:.4f}\")\n",
    "# print(\"All files saved to models/ folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d8b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Plot Training History\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], 'b-', linewidth=2, label='Train Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Metrics\n",
    "axes[1].plot(history['val_ods'], 'g-', linewidth=2, label='ODS')\n",
    "axes[1].plot(history['val_ois'], 'b-', linewidth=2, label='OIS')\n",
    "axes[1].plot(history['val_ap'], 'r-', linewidth=2, label='AP')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Validation Metrics')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb43ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Final Test Evaluation\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_xyw_net.pth'))\n",
    "print(\"Loaded best model weights.\\n\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating on TEST set...\")\n",
    "test_ods, test_ois, test_ap = evaluate(model, test_loader, device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL TEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  ODS (Optimal Dataset Scale): {test_ods:.4f}\")\n",
    "print(f\"  OIS (Optimal Image Scale):   {test_ois:.4f}\")\n",
    "print(f\"  AP (Average Precision):      {test_ap:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ff6571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Visualize Edge Predictions\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_predictions(model, dataset, device, num_samples=6):\n",
    "    \"\"\"Visualize predictions on random samples\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        sample = dataset[idx]\n",
    "        img = sample['images'].unsqueeze(0).to(device)\n",
    "        label = sample['labels'][0].numpy()\n",
    "        \n",
    "        pred = model(img)[0, 0].cpu().numpy()\n",
    "        \n",
    "        # Original image\n",
    "        axes[i, 0].imshow(sample['images'].permute(1, 2, 0).numpy())\n",
    "        axes[i, 0].set_title(f'Input: {sample[\"filename\"]}')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Ground truth\n",
    "        axes[i, 1].imshow(label, cmap='gray')\n",
    "        axes[i, 1].set_title('Ground Truth')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Prediction\n",
    "        axes[i, 2].imshow(pred, cmap='gray')\n",
    "        axes[i, 2].set_title('XYW-Net Prediction')\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('predictions.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions(model, test_dataset, device, num_samples=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db648f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Visualize Encoder Stage Outputs\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_stages(model, dataset, device, sample_idx=0):\n",
    "    \"\"\"Visualize feature maps at each encoder stage\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    sample = dataset[sample_idx]\n",
    "    img = sample['images'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get stage outputs\n",
    "    final, (s1, s2, s3, s4) = model.forward_with_stages(img)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0, 0].imshow(sample['images'].permute(1, 2, 0).numpy())\n",
    "    axes[0, 0].set_title('Input Image')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Ground truth\n",
    "    axes[0, 1].imshow(sample['labels'][0].numpy(), cmap='gray')\n",
    "    axes[0, 1].set_title('Ground Truth')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    # Final prediction\n",
    "    axes[0, 2].imshow(final[0, 0].cpu().numpy(), cmap='gray')\n",
    "    axes[0, 2].set_title('Final Prediction')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    # Thresholded prediction\n",
    "    pred_binary = (final[0, 0].cpu().numpy() > 0.5).astype(float)\n",
    "    axes[0, 3].imshow(pred_binary, cmap='gray')\n",
    "    axes[0, 3].set_title('Prediction (threshold=0.5)')\n",
    "    axes[0, 3].axis('off')\n",
    "    \n",
    "    # Stage feature maps (mean across channels)\n",
    "    stages = [s1, s2, s3, s4]\n",
    "    stage_names = ['Stage 1 (30ch)', 'Stage 2 (60ch)', 'Stage 3 (120ch)', 'Stage 4 (120ch)']\n",
    "    \n",
    "    for i, (stage, name) in enumerate(zip(stages, stage_names)):\n",
    "        feat_mean = stage[0].mean(dim=0).cpu().numpy()\n",
    "        feat_norm = (feat_mean - feat_mean.min()) / (feat_mean.max() - feat_mean.min() + 1e-8)\n",
    "        axes[1, i].imshow(feat_norm, cmap='viridis')\n",
    "        axes[1, i].set_title(f'{name}\\nShape: {tuple(stage.shape[2:])}')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'XYW-Net Encoder Stage Outputs - {sample[\"filename\"]}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('stage_outputs.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize for a few samples\n",
    "for idx in [0, 1, 2]:\n",
    "    if idx < len(test_dataset):\n",
    "        visualize_stages(model, test_dataset, device, sample_idx=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa0ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Precision-Recall Curve\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_pr_curve(model, loader, device):\n",
    "    \"\"\"Plot precision-recall curve\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(loader, desc='Computing PR curve'):\n",
    "        images = batch['images'].to(device)\n",
    "        labels = batch['labels']\n",
    "        outputs = model(images)\n",
    "        \n",
    "        for i in range(outputs.shape[0]):\n",
    "            all_preds.append(outputs[i, 0].cpu().numpy().flatten())\n",
    "            all_labels.append((labels[i, 0].numpy().flatten() > 0.5).astype(int))\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(all_labels, all_preds)\n",
    "    ap = average_precision_score(all_labels, all_preds)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, 'b-', linewidth=2, label=f'XYW-Net (AP={ap:.4f})')\n",
    "    plt.xlabel('Recall', fontsize=12)\n",
    "    plt.ylabel('Precision', fontsize=12)\n",
    "    plt.title('Precision-Recall Curve', fontsize=14)\n",
    "    plt.legend(loc='lower left', fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.savefig('pr_curve.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    return ap\n",
    "\n",
    "ap = plot_pr_curve(model, test_loader, device)\n",
    "print(f\"Average Precision: {ap:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94973424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Summary Results Table\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                    XYW-NET EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\")\n",
    "print(f\"  Dataset:          {DATA_ROOT}\")\n",
    "print(f\"  Train samples:    {len(train_dataset)}\")\n",
    "print(f\"  Val samples:      {len(val_dataset)}\")\n",
    "print(f\"  Test samples:     {len(test_dataset)}\")\n",
    "print(f\"\")\n",
    "print(f\"  Training epochs:  {NUM_EPOCHS}\")\n",
    "print(f\"  Learning rate:    {LEARNING_RATE}\")\n",
    "print(f\"  Batch size:       {BATCH_SIZE}\")\n",
    "print(f\"\")\n",
    "print(\"  \" + \"-\"*50)\n",
    "print(f\"  TEST SET METRICS:\")\n",
    "print(\"  \" + \"-\"*50)\n",
    "print(f\"  ODS (Optimal Dataset Scale):   {test_ods:.4f}\")\n",
    "print(f\"  OIS (Optimal Image Scale):     {test_ois:.4f}\")\n",
    "print(f\"  AP (Average Precision):        {test_ap:.4f}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\")\n",
    "print(\"Saved files:\")\n",
    "print(\"  - best_xyw_net.pth      (model weights)\")\n",
    "print(\"  - training_history.png  (loss/metrics plot)\")\n",
    "print(\"  - predictions.png       (sample predictions)\")\n",
    "print(\"  - stage_outputs.png     (encoder stages)\")\n",
    "print(\"  - pr_curve.png          (precision-recall curve)\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8942655,
     "sourceId": 14046899,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8943377,
     "sourceId": 14048368,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 527488,
     "modelInstanceId": 512848,
     "sourceId": 676420,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 528001,
     "modelInstanceId": 513371,
     "sourceId": 677020,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
