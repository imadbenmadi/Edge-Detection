{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83d26a5c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Pathway | What it does | Implementation |\n",
    "|---------|--------------|----------------|\n",
    "| **X** | Local contrast (center-surround) | `3×3 Conv` - `1×1 Conv` |\n",
    "| **Y** | Large RF contrast | `5×5 Dilated Conv` - `1×1 Conv` |\n",
    "| **W** | Directional | `1×3 Conv` (horizontal) + `3×1 Conv` (vertical) |\n",
    "\n",
    "**W pathway is NOT Gabor filters** - it's separable directional convolutions.\n",
    "\n",
    "**Gabor filters** are specifically oriented edge detectors with sinusoidal patterns at different angles (0°, 45°, 90°, 135°, etc.). They're biologically inspired by V1 cortex cells.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab8640a",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce GTX 1070\n",
      "Memory: 8.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import os\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "119b7c09-85de-4e45-84b6-7f5dfd51c20a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:22:08.814662Z",
     "iopub.status.busy": "2025-12-08T22:22:08.813677Z",
     "iopub.status.idle": "2025-12-08T22:22:08.837178Z",
     "shell.execute_reply": "2025-12-08T22:22:08.836487Z",
     "shell.execute_reply.started": "2025-12-08T22:22:08.814627Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# os.listdir(\"/kaggle/input/hed-bsds-v2/processed_HED_v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6665db5e-b036-4167-94fa-2a79c5f3329e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:22:10.515328Z",
     "iopub.status.busy": "2025-12-08T22:22:10.514609Z",
     "iopub.status.idle": "2025-12-08T22:22:10.519955Z",
     "shell.execute_reply": "2025-12-08T22:22:10.519381Z",
     "shell.execute_reply.started": "2025-12-08T22:22:10.515305Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# os.listdir(\"/kaggle/input/12epoch/pytorch/default/1\")\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c05a6c5a-fb14-4f62-9d71-72e552877db0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:22:11.963883Z",
     "iopub.status.busy": "2025-12-08T22:22:11.963576Z",
     "iopub.status.idle": "2025-12-08T22:22:11.968808Z",
     "shell.execute_reply": "2025-12-08T22:22:11.968262Z",
     "shell.execute_reply.started": "2025-12-08T22:22:11.963860Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce GTX 1070\n",
      "Total VRAM (GB): 8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "print(\"Total VRAM (GB):\", round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58813217-babd-4193-ba98-cf2300b8de81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:22:13.503138Z",
     "iopub.status.busy": "2025-12-08T22:22:13.502547Z",
     "iopub.status.idle": "2025-12-08T22:22:13.813680Z",
     "shell.execute_reply": "2025-12-08T22:22:13.813113Z",
     "shell.execute_reply.started": "2025-12-08T22:22:13.503114Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: ['test', 'train', 'val']\n",
      "TRAIN: ['edges', 'images']\n",
      "TRAIN/IMAGES: ['hedbsds_test_000010.png', 'hedbsds_test_000033.png', 'hedbsds_test_000038.png', 'hedbsds_test_000048.png', 'hedbsds_test_000085.png', 'hedbsds_test_000089.png', 'hedbsds_test_000094.png', 'hedbsds_test_000115.png', 'hedbsds_test_000123.png', 'hedbsds_test_000127.png']\n",
      "TRAIN/EDGES: ['hedbsds_test_000010.png', 'hedbsds_test_000033.png', 'hedbsds_test_000038.png', 'hedbsds_test_000048.png', 'hedbsds_test_000085.png', 'hedbsds_test_000089.png', 'hedbsds_test_000094.png', 'hedbsds_test_000115.png', 'hedbsds_test_000123.png', 'hedbsds_test_000127.png']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# BASE = \"/kaggle/input/hed-bsds-v2/processed_HED_v2\"\n",
    "BASE = \"./datasets/HED_Small\"\n",
    "\n",
    "print(\"ROOT:\", os.listdir(BASE))\n",
    "print(\"TRAIN:\", os.listdir(BASE + \"/train\"))\n",
    "print(\"TRAIN/IMAGES:\", os.listdir(BASE + \"/train/images\")[:10])\n",
    "print(\"TRAIN/EDGES:\", os.listdir(BASE + \"/train/edges\")[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13521090-5c7a-412f-9792-3a1b3ae41e6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:22:15.018354Z",
     "iopub.status.busy": "2025-12-08T22:22:15.017632Z",
     "iopub.status.idle": "2025-12-08T22:27:13.236450Z",
     "shell.execute_reply": "2025-12-08T22:27:13.235687Z",
     "shell.execute_reply.started": "2025-12-08T22:22:15.018325Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying split: train\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking train: 0img [00:00, ?img/s]\n",
      "Checking train: 0img [00:00, ?img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verification Report for split: train\n",
      "============================================================\n",
      "Total images checked: 0\n",
      "Missing edge files: 0\n",
      "Unreadable edge files: 0\n",
      "Empty edge maps: 0\n",
      "Size mismatches: 0\n",
      "\n",
      "All images have valid, non-empty, correctly-sized edge maps.\n",
      "\n",
      "Verifying split: val\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking val: 0img [00:00, ?img/s]\n",
      "Checking val: 0img [00:00, ?img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verification Report for split: val\n",
      "============================================================\n",
      "Total images checked: 0\n",
      "Missing edge files: 0\n",
      "Unreadable edge files: 0\n",
      "Empty edge maps: 0\n",
      "Size mismatches: 0\n",
      "\n",
      "All images have valid, non-empty, correctly-sized edge maps.\n",
      "\n",
      "Verifying split: test\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking test: 0img [00:00, ?img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verification Report for split: test\n",
      "============================================================\n",
      "Total images checked: 0\n",
      "Missing edge files: 0\n",
      "Unreadable edge files: 0\n",
      "Empty edge maps: 0\n",
      "Size mismatches: 0\n",
      "\n",
      "All images have valid, non-empty, correctly-sized edge maps.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def verify_edges(root_dir, split=\"train\"):\n",
    "    root_dir = Path(root_dir)\n",
    "    img_dir = root_dir / split / \"images\"\n",
    "    edge_dir = root_dir / split / \"edges\"\n",
    "\n",
    "    image_files = sorted(list(img_dir.glob(\"*.png\")))\n",
    "\n",
    "    missing_edges = []\n",
    "    unreadable_edges = []\n",
    "    empty_edges = []\n",
    "    size_mismatch = []\n",
    "\n",
    "    print(f\"\\nVerifying split: {split}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for img_path in tqdm(image_files, desc=f\"Checking {split}\", unit=\"img\"):\n",
    "        edge_path = edge_dir / img_path.name\n",
    "\n",
    "        # 1. Check if edge file exists\n",
    "        if not edge_path.exists():\n",
    "            missing_edges.append(img_path.name)\n",
    "            continue\n",
    "\n",
    "        # 2. Try loading edge\n",
    "        edge = cv2.imread(str(edge_path), cv2.IMREAD_GRAYSCALE)\n",
    "        if edge is None:\n",
    "            unreadable_edges.append(img_path.name)\n",
    "            continue\n",
    "\n",
    "        # 3. Check if edge is empty (all zeros)\n",
    "        if np.sum(edge) == 0:\n",
    "            empty_edges.append(img_path.name)\n",
    "\n",
    "        # 4. Check size match\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None or edge.shape != img.shape[:2]:\n",
    "            size_mismatch.append(img_path.name)\n",
    "\n",
    "    # ===== REPORT =====\n",
    "    print(f\"\\nVerification Report for split: {split}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total images checked: {len(image_files)}\")\n",
    "    print(f\"Missing edge files: {len(missing_edges)}\")\n",
    "    print(f\"Unreadable edge files: {len(unreadable_edges)}\")\n",
    "    print(f\"Empty edge maps: {len(empty_edges)}\")\n",
    "    print(f\"Size mismatches: {len(size_mismatch)}\")\n",
    "\n",
    "    if missing_edges:\n",
    "        print(\"\\nMissing edge files (first 10):\")\n",
    "        print(missing_edges[:10])\n",
    "\n",
    "    if unreadable_edges:\n",
    "        print(\"\\nUnreadable edge files (first 10):\")\n",
    "        print(unreadable_edges[:10])\n",
    "\n",
    "    if empty_edges:\n",
    "        print(\"\\nEmpty edge maps (first 10):\")\n",
    "        print(empty_edges[:10])\n",
    "\n",
    "    if size_mismatch:\n",
    "        print(\"\\nSize mismatches (first 10):\")\n",
    "        print(size_mismatch[:10])\n",
    "\n",
    "    if not (missing_edges or unreadable_edges or empty_edges or size_mismatch):\n",
    "        print(\"\\nAll images have valid, non-empty, correctly-sized edge maps.\")\n",
    "\n",
    "# =========================\n",
    "# RUN FOR ALL SPLITS\n",
    "# =========================\n",
    "\n",
    "DATA_ROOT = r\"/kaggle/input/hed-bsds-v2/processed_HED_v2\"\n",
    "\n",
    "verify_edges(DATA_ROOT, \"train\")\n",
    "verify_edges(DATA_ROOT, \"val\")\n",
    "verify_edges(DATA_ROOT, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85adc490",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:35:42.136303Z",
     "iopub.status.busy": "2025-12-08T22:35:42.136004Z",
     "iopub.status.idle": "2025-12-08T22:35:42.242039Z",
     "shell.execute_reply": "2025-12-08T22:35:42.241432Z",
     "shell.execute_reply.started": "2025-12-08T22:35:42.136279Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 531 samples\n",
      "Val:   30 samples\n",
      "Test:  30 samples\n",
      "Image shape: torch.Size([3, 512, 512])\n",
      "Edge shape: torch.Size([1, 512, 512])\n",
      "\n",
      "Val:   30 samples\n",
      "Test:  30 samples\n",
      "Image shape: torch.Size([3, 512, 512])\n",
      "Edge shape: torch.Size([1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Cell 2: Dataset Loader\n",
    "class ProcessedDataset(Dataset):\n",
    "    \"\"\"Load processed PNG images and edge maps\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, split='train'):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.split = split\n",
    "        self.img_dir = self.root_dir / split / 'images'\n",
    "        self.edge_dir = self.root_dir / split / 'edges'\n",
    "        self.samples = sorted(list(self.img_dir.glob('*.png')))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.samples[idx]\n",
    "        edge_path = self.edge_dir / img_path.name\n",
    "\n",
    "        # Load image\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "\n",
    "        # Load edge map\n",
    "        edge = cv2.imread(str(edge_path), cv2.IMREAD_GRAYSCALE)\n",
    "        edge = edge.astype(np.float32) / 255.0\n",
    "\n",
    "        # To tensors\n",
    "        img = torch.from_numpy(img).permute(2, 0, 1)\n",
    "        edge = torch.from_numpy(edge).unsqueeze(0)\n",
    "\n",
    "        return {'images': img, 'labels': edge, 'filename': img_path.stem}\n",
    "\n",
    "\n",
    "#  KAGGLE DATASET ROOT\n",
    "# DATA_ROOT = \"/kaggle/input/hed-bsds-v2/processed_HED_v2\"\n",
    "DATA_ROOT = \"./datasets/HED_Small\"\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_dataset = ProcessedDataset(DATA_ROOT, split='train')\n",
    "# val_dataset   = ProcessedDataset(DATA_ROOT, split='val')\n",
    "val_dataset   = ProcessedDataset(DATA_ROOT, split='test')\n",
    "\n",
    "test_dataset  = ProcessedDataset(DATA_ROOT, split='test')\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2)\n",
    "# val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "# test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "print(f\"Train: {len(train_dataset)} samples\")\n",
    "print(f\"Val:   {len(val_dataset)} samples\")\n",
    "print(f\"Test:  {len(test_dataset)} samples\")\n",
    "\n",
    "# Verify sample\n",
    "sample = train_dataset[0]\n",
    "print(\"Image shape:\", sample[\"images\"].shape)\n",
    "print(\"Edge shape:\", sample[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f812c784",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:34:24.521400Z",
     "iopub.status.busy": "2025-12-08T22:34:24.521038Z",
     "iopub.status.idle": "2025-12-08T22:34:24.775431Z",
     "shell.execute_reply": "2025-12-08T22:34:24.774604Z",
     "shell.execute_reply.started": "2025-12-08T22:34:24.521365Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Cell 3: XYW-Net Model (Complete)\n",
    "\n",
    "# # ============ PDC Convolution ============\n",
    "# def createPDCFunc(PDC_type):\n",
    "#     \"\"\"Create Pixel Difference Convolution function\"\"\"\n",
    "#     assert PDC_type in ['cv', 'cd', 'ad', 'rd', 'sd', 'p2d', '2sd', '2cd']\n",
    "    \n",
    "#     if PDC_type == 'cv':\n",
    "#         return F.conv2d\n",
    "    \n",
    "#     if PDC_type == '2sd':\n",
    "#         def func(x, weights, bias=None, stride=1, padding=0, dilation=1, groups=1):\n",
    "#             assert weights.size(2) == 3 and weights.size(3) == 3\n",
    "#             shape = weights.shape\n",
    "#             if groups == shape[0]:\n",
    "#                 weights_conv = (weights - weights[:, :, [1, 1, 1, 0, 0, 0, 2, 2, 2], [0, 1, 2, 0, 1, 2, 0, 1, 2]].view(shape))\n",
    "#             else:\n",
    "#                 weights_conv = (weights - weights[:, :, [1, 1, 1, 0, 0, 0, 2, 2, 2], [0, 1, 2, 0, 1, 2, 0, 1, 2]].view(shape).flip(0))\n",
    "#             y = F.conv2d(x, weights_conv, bias, stride=stride, padding=padding, dilation=dilation, groups=groups)\n",
    "#             return y\n",
    "#         return func\n",
    "    \n",
    "#     return F.conv2d\n",
    "\n",
    "# class Conv2d(nn.Module):\n",
    "#     \"\"\"PDC-enabled Conv2d\"\"\"\n",
    "#     def __init__(self, pdc_func='cv', in_channels=1, out_channels=1, kernel_size=3, \n",
    "#                  stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "#         super(Conv2d, self).__init__()\n",
    "#         self.pdc = createPDCFunc(pdc_func)\n",
    "#         self.stride = stride\n",
    "#         self.padding = padding\n",
    "#         self.dilation = dilation\n",
    "#         self.groups = groups\n",
    "#         self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels // groups, kernel_size, kernel_size))\n",
    "#         if bias:\n",
    "#             self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
    "#         else:\n",
    "#             self.register_parameter('bias', None)\n",
    "#         self.reset_parameters()\n",
    "    \n",
    "#     def reset_parameters(self):\n",
    "#         nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "#         if self.bias is not None:\n",
    "#             fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "#             bound = 1 / math.sqrt(fan_in)\n",
    "#             nn.init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.pdc(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "# # ============ Core XYW Components ============\n",
    "# class Xc1x1(nn.Module):\n",
    "#     \"\"\"X pathway: Local contrast (center-surround with 3x3)\"\"\"\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(Xc1x1, self).__init__()\n",
    "#         self.Xcenter = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "#         self.Xcenter_relu = nn.ReLU(inplace=True)\n",
    "#         self.Xsurround = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, groups=in_channels)\n",
    "#         self.conv1_1 = nn.Conv2d(out_channels, out_channels, kernel_size=1)\n",
    "#         self.Xsurround_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         xcenter = self.Xcenter_relu(self.Xcenter(input))\n",
    "#         xsurround = self.Xsurround_relu(self.Xsurround(input))\n",
    "#         xsurround = self.conv1_1(xsurround)\n",
    "#         return xsurround - xcenter\n",
    "\n",
    "# class Yc1x1(nn.Module):\n",
    "#     \"\"\"Y pathway: Large receptive field (center-surround with 5x5 dilated)\"\"\"\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(Yc1x1, self).__init__()\n",
    "#         self.Ycenter = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "#         self.Ycenter_relu = nn.ReLU(inplace=True)\n",
    "#         self.Ysurround = nn.Conv2d(in_channels, out_channels, kernel_size=5, padding=4, dilation=2, groups=in_channels)\n",
    "#         self.conv1_1 = nn.Conv2d(out_channels, out_channels, kernel_size=1)\n",
    "#         self.Ysurround_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         ycenter = self.Ycenter_relu(self.Ycenter(input))\n",
    "#         ysurround = self.Ysurround_relu(self.Ysurround(input))\n",
    "#         ysurround = self.conv1_1(ysurround)\n",
    "#         return ysurround - ycenter\n",
    "\n",
    "# class W(nn.Module):\n",
    "#     \"\"\"W pathway: Directional (horizontal + vertical)\"\"\"\n",
    "#     def __init__(self, inchannel, outchannel):\n",
    "#         super(W, self).__init__()\n",
    "#         self.h = nn.Conv2d(inchannel, inchannel, kernel_size=(1, 3), padding=(0, 1), groups=inchannel)\n",
    "#         self.v = nn.Conv2d(inchannel, inchannel, kernel_size=(3, 1), padding=(1, 0), groups=inchannel)\n",
    "#         self.convh_1 = nn.Conv2d(inchannel, inchannel, kernel_size=1, bias=False)\n",
    "#         self.convv_1 = nn.Conv2d(inchannel, outchannel, kernel_size=1, bias=False)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         h = self.relu(self.h(x))\n",
    "#         h = self.convh_1(h)\n",
    "#         v = self.relu(self.v(h))\n",
    "#         v = self.convv_1(v)\n",
    "#         return v\n",
    "\n",
    "# # ============ XYW Blocks ============\n",
    "# class XYW_S(nn.Module):\n",
    "#     \"\"\"XYW Start block\"\"\"\n",
    "#     def __init__(self, inchannel, outchannel, stride=1):\n",
    "#         super(XYW_S, self).__init__()\n",
    "#         self.y_c = Yc1x1(inchannel, outchannel)\n",
    "#         self.x_c = Xc1x1(inchannel, outchannel)\n",
    "#         self.w = W(inchannel, outchannel)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.x_c(x), self.y_c(x), self.w(x)\n",
    "\n",
    "# class XYW(nn.Module):\n",
    "#     \"\"\"XYW middle block\"\"\"\n",
    "#     def __init__(self, inchannel, outchannel, stride=1):\n",
    "#         super(XYW, self).__init__()\n",
    "#         self.y_c = Yc1x1(inchannel, outchannel)\n",
    "#         self.x_c = Xc1x1(inchannel, outchannel)\n",
    "#         self.w = W(inchannel, outchannel)\n",
    "\n",
    "#     def forward(self, xc, yc, w):\n",
    "#         return self.x_c(xc), self.y_c(yc), self.w(w)\n",
    "\n",
    "# class XYW_E(nn.Module):\n",
    "#     \"\"\"XYW End block (combines X+Y+W)\"\"\"\n",
    "#     def __init__(self, inchannel, outchannel):\n",
    "#         super(XYW_E, self).__init__()\n",
    "#         self.y_c = Yc1x1(inchannel, outchannel)\n",
    "#         self.x_c = Xc1x1(inchannel, outchannel)\n",
    "#         self.w = W(inchannel, outchannel)\n",
    "\n",
    "#     def forward(self, xc, yc, w):\n",
    "#         return self.x_c(xc) + self.y_c(yc) + self.w(w)\n",
    "\n",
    "# # ============ Encoder Stages ============\n",
    "# class s1(nn.Module):\n",
    "#     def __init__(self, channel=30):\n",
    "#         super(s1, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, channel, kernel_size=7, padding=6, dilation=2)\n",
    "#         self.xyw1_1 = XYW_S(channel, channel)\n",
    "#         self.xyw1_2 = XYW(channel, channel)\n",
    "#         self.xyw1_3 = XYW_E(channel, channel)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         temp = self.relu(self.conv1(x))\n",
    "#         xc, yc, w = self.xyw1_1(temp)\n",
    "#         xc, yc, w = self.xyw1_2(xc, yc, w)\n",
    "#         xyw1_3 = self.xyw1_3(xc, yc, w)\n",
    "#         return xyw1_3 + temp\n",
    "\n",
    "# class s2(nn.Module):\n",
    "#     def __init__(self, channel=60):\n",
    "#         super(s2, self).__init__()\n",
    "#         self.xyw2_1 = XYW_S(channel//2, channel, stride=2)\n",
    "#         self.xyw2_2 = XYW(channel, channel)\n",
    "#         self.xyw2_3 = XYW_E(channel, channel)\n",
    "#         self.shortcut = nn.Conv2d(channel//2, channel, kernel_size=1)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(x)\n",
    "#         xc, yc, w = self.xyw2_1(x)\n",
    "#         xc, yc, w = self.xyw2_2(xc, yc, w)\n",
    "#         xyw2_3 = self.xyw2_3(xc, yc, w)\n",
    "#         return xyw2_3 + self.shortcut(x)\n",
    "\n",
    "# class s3(nn.Module):\n",
    "#     def __init__(self, channel=120):\n",
    "#         super(s3, self).__init__()\n",
    "#         self.xyw3_1 = XYW_S(channel//2, channel, stride=2)\n",
    "#         self.xyw3_2 = XYW(channel, channel)\n",
    "#         self.xyw3_3 = XYW_E(channel, channel)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.shortcut = nn.Conv2d(channel//2, channel, kernel_size=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(x)\n",
    "#         shortcut = self.shortcut(x)\n",
    "#         xc, yc, w = self.xyw3_1(x)\n",
    "#         xc, yc, w = self.xyw3_2(xc, yc, w)\n",
    "#         xyw3_3 = self.xyw3_3(xc, yc, w)\n",
    "#         return xyw3_3 + shortcut\n",
    "\n",
    "# class s4(nn.Module):\n",
    "#     def __init__(self, channel=120):\n",
    "#         super(s4, self).__init__()\n",
    "#         self.xyw4_1 = XYW_S(channel, channel, stride=2)\n",
    "#         self.xyw4_2 = XYW(channel, channel)\n",
    "#         self.xyw4_3 = XYW_E(channel, channel)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.shortcut = nn.Conv2d(channel, channel, kernel_size=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(x)\n",
    "#         shortcut = self.shortcut(x)\n",
    "#         xc, yc, w = self.xyw4_1(x)\n",
    "#         xc, yc, w = self.xyw4_2(xc, yc, w)\n",
    "#         xyw4_3 = self.xyw4_3(xc, yc, w)\n",
    "#         return xyw4_3 + shortcut\n",
    "\n",
    "# # ============ Encoder ============\n",
    "# class encode(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(encode, self).__init__()\n",
    "#         self.s1 = s1()\n",
    "#         self.s2 = s2()\n",
    "#         self.s3 = s3()\n",
    "#         self.s4 = s4()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         s1_out = self.s1(x)\n",
    "#         s2_out = self.s2(s1_out)\n",
    "#         s3_out = self.s3(s2_out)\n",
    "#         s4_out = self.s4(s3_out)\n",
    "#         return s1_out, s2_out, s3_out, s4_out\n",
    "\n",
    "# # ============ Adaptive Convolution ============\n",
    "# def upsample_filt(size):\n",
    "#     factor = (size + 1) // 2\n",
    "#     center = factor - 1 if size % 2 == 1 else factor - 0.5\n",
    "#     og = np.ogrid[:size, :size]\n",
    "#     return (1 - abs(og[0] - center) / factor) * (1 - abs(og[1] - center) / factor)\n",
    "\n",
    "# def bilinear_upsample_weights(factor, num_classes):\n",
    "#     filter_size = 2 * factor - factor % 2\n",
    "#     weights = np.zeros((num_classes, num_classes, filter_size, filter_size), dtype=np.float32)\n",
    "#     upsample_kernel = upsample_filt(filter_size)\n",
    "#     for i in range(num_classes):\n",
    "#         weights[i, i, :, :] = upsample_kernel\n",
    "#     return torch.Tensor(weights)\n",
    "\n",
    "# class adap_conv(nn.Module):\n",
    "#     \"\"\"Adaptive convolution with learnable weight\"\"\"\n",
    "#     def __init__(self, in_channels, out_channels, kz=3, pd=1):\n",
    "#         super(adap_conv, self).__init__()\n",
    "#         self.conv = nn.Sequential(\n",
    "#             Conv2d(pdc_func='2sd', in_channels=in_channels, out_channels=out_channels, kernel_size=kz, padding=pd),\n",
    "#             nn.InstanceNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "#         self.weight = nn.Parameter(torch.Tensor([0.]))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.conv(x) * self.weight.sigmoid()\n",
    "\n",
    "# class Refine_block2_1(nn.Module):\n",
    "#     \"\"\"Refinement block for decoder\"\"\"\n",
    "#     def __init__(self, in_channel, out_channel, factor, require_grad=False):\n",
    "#         super(Refine_block2_1, self).__init__()\n",
    "#         self.pre_conv1 = adap_conv(in_channel[0], out_channel, kz=3, pd=1)\n",
    "#         self.pre_conv2 = adap_conv(in_channel[1], out_channel, kz=3, pd=1)\n",
    "#         self.factor = factor\n",
    "#         self.deconv_weight = nn.Parameter(bilinear_upsample_weights(factor, out_channel), requires_grad=require_grad)\n",
    "\n",
    "#     def forward(self, *input):\n",
    "#         x1 = self.pre_conv1(input[0])\n",
    "#         x2 = self.pre_conv2(input[1])\n",
    "#         x2 = F.conv_transpose2d(x2, self.deconv_weight, stride=self.factor, \n",
    "#                                 padding=int(self.factor/2),\n",
    "#                                 output_padding=(x1.size(2) - x2.size(2)*self.factor, \n",
    "#                                                x1.size(3) - x2.size(3)*self.factor))\n",
    "#         return x1 + x2\n",
    "\n",
    "# # ============ RCF Decoder ============\n",
    "# class decode_rcf(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(decode_rcf, self).__init__()\n",
    "#         self.f43 = Refine_block2_1(in_channel=(120, 120), out_channel=60, factor=2)\n",
    "#         self.f32 = Refine_block2_1(in_channel=(60, 60), out_channel=30, factor=2)\n",
    "#         self.f21 = Refine_block2_1(in_channel=(30, 30), out_channel=24, factor=2)\n",
    "#         self.f = nn.Conv2d(24, 1, kernel_size=1, padding=0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         s3 = self.f43(x[2], x[3])\n",
    "#         s2 = self.f32(x[1], s3)\n",
    "#         s1 = self.f21(x[0], s2)\n",
    "#         out = self.f(s1)\n",
    "#         return out.sigmoid()\n",
    "\n",
    "# # ============ Full XYW-Net ============\n",
    "# class XYWNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(XYWNet, self).__init__()\n",
    "#         self.encode = encode()\n",
    "#         self.decode = decode_rcf()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         endpoints = self.encode(x)\n",
    "#         out = self.decode(endpoints)\n",
    "#         return out\n",
    "    \n",
    "#     def forward_with_stages(self, x):\n",
    "#         \"\"\"Forward pass returning intermediate stage outputs for visualization\"\"\"\n",
    "#         s1, s2, s3, s4 = self.encode(x)\n",
    "#         final = self.decode((s1, s2, s3, s4))\n",
    "#         return final, (s1, s2, s3, s4)\n",
    "\n",
    "# # Create model\n",
    "# model = XYWNet().to(device)\n",
    "# print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f315dfd1-c8d1-40d2-b492-d09b4eb1cb0b",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#   XYW-NET: COMPLETE ARCHITECTURE (ENCODER + ITM + ELC)\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#  PDC CONVOLUTION (Pixel Difference Convolution)\n",
    "# ============================================================\n",
    "def createPDCFunc(PDC_type):\n",
    "    assert PDC_type in ['cv', '2sd']\n",
    "    \n",
    "    if PDC_type == 'cv':\n",
    "        return F.conv2d\n",
    "    \n",
    "    if PDC_type == '2sd':\n",
    "        # Pixel difference convolution (paper uses this for ELC)\n",
    "        def func(x, weights, bias=None, stride=1, padding=0, dilation=1, groups=1):\n",
    "            assert weights.size(2) == 3 and weights.size(3) == 3\n",
    "            shape = weights.shape\n",
    "            offset = weights[:, :, \n",
    "                             [1,1,1,0,0,0,2,2,2],\n",
    "                             [0,1,2,0,1,2,0,1,2]\n",
    "                             ].view(shape)\n",
    "            \n",
    "            diff_weights = weights - offset\n",
    "            return F.conv2d(x, diff_weights, bias, stride, padding, dilation, groups)\n",
    "        \n",
    "        return func\n",
    "\n",
    "\n",
    "class Conv2d(nn.Module):\n",
    "    \"\"\"PDC-enabled conv layer\"\"\"\n",
    "    def __init__(self, pdc_func='cv', in_channels=1, out_channels=1, kernel_size=3,\n",
    "                 stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "        super().__init__()\n",
    "        self.pdc = createPDCFunc(pdc_func)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels // groups, kernel_size, kernel_size))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pdc(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#  XYW COMPONENTS (ENCODER)\n",
    "# ============================================================\n",
    "class Xc1x1(nn.Module):\n",
    "    \"\"\"X: small RF center-surround\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.center = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.surround = nn.Conv2d(in_channels, out_channels, 3, padding=1, groups=in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.proj = nn.Conv2d(out_channels, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c = self.relu(self.center(x))\n",
    "        s = self.relu(self.surround(x))\n",
    "        s = self.proj(s)\n",
    "        return s - c\n",
    "\n",
    "\n",
    "class Yc1x1(nn.Module):\n",
    "    \"\"\"Y: large RF dilated center-surround\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.center = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.surround = nn.Conv2d(in_channels, out_channels, 5, padding=4, dilation=2, groups=in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.proj = nn.Conv2d(out_channels, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        c = self.relu(self.center(x))\n",
    "        s = self.relu(self.surround(x))\n",
    "        s = self.proj(s)\n",
    "        return s - c\n",
    "\n",
    "\n",
    "class W(nn.Module):\n",
    "    \"\"\"W: directional horizontal + vertical pathway\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.h = nn.Conv2d(in_ch, in_ch, (1,3), padding=(0,1), groups=in_ch)\n",
    "        self.v = nn.Conv2d(in_ch, in_ch, (3,1), padding=(1,0), groups=in_ch)\n",
    "        self.proj1 = nn.Conv2d(in_ch, in_ch, 1)\n",
    "        self.proj2 = nn.Conv2d(in_ch, out_ch, 1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.h(x))\n",
    "        h = self.proj1(h)\n",
    "        v = self.relu(self.v(h))\n",
    "        return self.proj2(v)\n",
    "\n",
    "\n",
    "class XYW_S(nn.Module):\n",
    "    \"\"\"Start block\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.x = Xc1x1(in_ch, out_ch)\n",
    "        self.y = Yc1x1(in_ch, out_ch)\n",
    "        self.w = W(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.x(x), self.y(x), self.w(x)\n",
    "\n",
    "\n",
    "class XYW(nn.Module):\n",
    "    \"\"\"Middle block\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.x = Xc1x1(in_ch, out_ch)\n",
    "        self.y = Yc1x1(in_ch, out_ch)\n",
    "        self.w = W(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, xc, yc, w):\n",
    "        return self.x(xc), self.y(yc), self.w(w)\n",
    "\n",
    "\n",
    "class XYW_E(nn.Module):\n",
    "    \"\"\"End block: fusion of X+Y+W\"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.x = Xc1x1(in_ch, out_ch)\n",
    "        self.y = Yc1x1(in_ch, out_ch)\n",
    "        self.w = W(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, xc, yc, w):\n",
    "        return self.x(xc) + self.y(yc) + self.w(w)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#  ENCODER STAGES (4 RESOLUTION LEVELS)\n",
    "# ============================================================\n",
    "class s1(nn.Module):\n",
    "    def __init__(self, ch=30):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Conv2d(3, ch, 7, padding=6, dilation=2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.b1 = XYW_S(ch, ch)\n",
    "        self.b2 = XYW(ch, ch)\n",
    "        self.b3 = XYW_E(ch, ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = self.relu(self.stem(x))\n",
    "        xc, yc, w = self.b1(t)\n",
    "        xc, yc, w = self.b2(xc, yc, w)\n",
    "        out = self.b3(xc, yc, w)\n",
    "        return out + t\n",
    "\n",
    "\n",
    "class s2(nn.Module):\n",
    "    def __init__(self, ch=60):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.b1 = XYW_S(ch//2, ch)\n",
    "        self.b2 = XYW(ch, ch)\n",
    "        self.b3 = XYW_E(ch, ch)\n",
    "        self.short = nn.Conv2d(ch//2, ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        xc, yc, w = self.b1(x)\n",
    "        xc, yc, w = self.b2(xc, yc, w)\n",
    "        out = self.b3(xc, yc, w)\n",
    "        return out + self.short(x)\n",
    "\n",
    "\n",
    "class s3(nn.Module):\n",
    "    def __init__(self, ch=120):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.b1 = XYW_S(ch//2, ch)\n",
    "        self.b2 = XYW(ch, ch)\n",
    "        self.b3 = XYW_E(ch, ch)\n",
    "        self.short = nn.Conv2d(ch//2, ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        sc = self.short(x)\n",
    "        xc, yc, w = self.b1(x)\n",
    "        xc, yc, w = self.b2(xc, yc, w)\n",
    "        return self.b3(xc, yc, w) + sc\n",
    "\n",
    "\n",
    "class s4(nn.Module):\n",
    "    def __init__(self, ch=120):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.b1 = XYW_S(ch, ch)\n",
    "        self.b2 = XYW(ch, ch)\n",
    "        self.b3 = XYW_E(ch, ch)\n",
    "        self.short = nn.Conv2d(ch, ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        sc = self.short(x)\n",
    "        xc, yc, w = self.b1(x)\n",
    "        xc, yc, w = self.b2(xc, yc, w)\n",
    "        return self.b3(xc, yc, w) + sc\n",
    "\n",
    "\n",
    "class encode(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.s1 = s1()\n",
    "        self.s2 = s2()\n",
    "        self.s3 = s3()\n",
    "        self.s4 = s4()\n",
    "\n",
    "    def forward(self, x):\n",
    "        s1_out = self.s1(x)\n",
    "        s2_out = self.s2(s1_out)\n",
    "        s3_out = self.s3(s2_out)\n",
    "        s4_out = self.s4(s3_out)\n",
    "        return s1_out, s2_out, s3_out, s4_out\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#  UPSAMPLE HELPERS \n",
    "# ============================================================\n",
    "def upsample_filt(size):\n",
    "    factor = (size + 1) // 2\n",
    "    center = factor - 1 if size % 2 == 1 else factor - 0.5\n",
    "    og = np.ogrid[:size, :size]\n",
    "    return (1 - abs(og[0] - center) / factor) * (1 - abs(og[1] - center) / factor)\n",
    "\n",
    "\n",
    "def bilinear_upsample_weights(factor, C):\n",
    "    fs = 2 * factor - factor % 2\n",
    "    w = np.zeros((C, C, fs, fs), dtype=np.float32)\n",
    "    kern = upsample_filt(fs)\n",
    "    for i in range(C):\n",
    "        w[i, i] = kern\n",
    "    return torch.Tensor(w)\n",
    "\n",
    "\n",
    "class Refine_block2_1(nn.Module):\n",
    "    \"\"\"Refinement block used in ITM\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, factor):\n",
    "        super().__init__()\n",
    "        self.pre1 = Conv2d('2sd', in_ch[0], out_ch, 3, padding=1)\n",
    "        self.pre2 = Conv2d('2sd', in_ch[1], out_ch, 3, padding=1)\n",
    "        self.deconv_w = nn.Parameter(bilinear_upsample_weights(factor, out_ch), requires_grad=False)\n",
    "        self.factor = factor\n",
    "\n",
    "    def forward(self, x_high, x_low):\n",
    "        h = self.pre1(x_high)\n",
    "        l = self.pre2(x_low)\n",
    "        l = F.conv_transpose2d(l, self.deconv_w, stride=self.factor,\n",
    "                               padding=int(self.factor/2),\n",
    "                               output_padding=(h.size(2) - l.size(2)*self.factor,\n",
    "                                               h.size(3) - l.size(3)*self.factor))\n",
    "        return h + l\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#  ELC BLOCK (Edge Localization Convolution)\n",
    "# ============================================================\n",
    "class ELCBlock(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.pdc = Conv2d('2sd', ch, ch, 3, padding=1)\n",
    "        self.norm = nn.InstanceNorm2d(ch)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.out = nn.Conv2d(ch, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pdc(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.relu(x)\n",
    "        return torch.sigmoid(self.out(x))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#  ITM + ELC DECODER (XYW-Net)\n",
    "# ============================================================\n",
    "class decode_xyw(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.f43 = Refine_block2_1((120,120), 64, 2)\n",
    "        self.f32 = Refine_block2_1((60,64),   48, 2)\n",
    "        self.f21 = Refine_block2_1((30,48),   32, 2)\n",
    "        self.elc = ELCBlock(32)\n",
    "\n",
    "    def forward(self, endpoints):\n",
    "        s1, s2, s3, s4 = endpoints\n",
    "        x3 = self.f43(s3, s4)\n",
    "        x2 = self.f32(s2, x3)\n",
    "        x1 = self.f21(s1, x2)\n",
    "        return self.elc(x1)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#  FULL XYW-NET (ENCODER + DECODER)\n",
    "# ============================================================\n",
    "class XYWNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = encode()\n",
    "        self.decoder = decode_xyw()\n",
    "\n",
    "    def forward(self, x):\n",
    "        endpoints = self.encoder(x)\n",
    "        return self.decoder(endpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac73f4ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:34:29.530974Z",
     "iopub.status.busy": "2025-12-08T22:34:29.530695Z",
     "iopub.status.idle": "2025-12-08T22:34:29.543187Z",
     "shell.execute_reply": "2025-12-08T22:34:29.542570Z",
     "shell.execute_reply.started": "2025-12-08T22:34:29.530942Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function and metrics defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Loss Function and Metrics\n",
    "\n",
    "class EdgeLoss(nn.Module):\n",
    "    \"\"\"Weighted cross-entropy loss for edge detection\"\"\"\n",
    "    def __init__(self):\n",
    "        super(EdgeLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, pred, label):\n",
    "        pred_flat = pred.view(-1)\n",
    "        label_flat = label.view(-1)\n",
    "        eps = 1e-6\n",
    "        \n",
    "        # Positive and negative pixels\n",
    "        pos_mask = label_flat > 0\n",
    "        neg_mask = label_flat == 0\n",
    "        \n",
    "        pred_pos = pred_flat[pos_mask].clamp(eps, 1.0 - eps)\n",
    "        pred_neg = pred_flat[neg_mask].clamp(eps, 1.0 - eps)\n",
    "        \n",
    "        # Weighted by annotation strength\n",
    "        w_pos = label_flat[pos_mask]\n",
    "        \n",
    "        if len(pred_pos) > 0 and len(pred_neg) > 0:\n",
    "            loss = (-pred_pos.log() * w_pos).mean() + (-(1.0 - pred_neg).log()).mean()\n",
    "        elif len(pred_pos) > 0:\n",
    "            loss = (-pred_pos.log() * w_pos).mean()\n",
    "        else:\n",
    "            loss = (-(1.0 - pred_neg).log()).mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "def compute_ods_ois_ap(preds, labels, thresholds=99):\n",
    "    \"\"\"\n",
    "    Compute ODS (Optimal Dataset Scale), OIS (Optimal Image Scale), and AP.\n",
    "    \n",
    "    ODS: Best F-score using a single threshold for all images\n",
    "    OIS: Average of best F-score per image\n",
    "    AP: Average Precision\n",
    "    \"\"\"\n",
    "    threshs = np.linspace(0.01, 0.99, thresholds)\n",
    "    \n",
    "    # For ODS (global)\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # For OIS (per-image best)\n",
    "    ois_f1_scores = []\n",
    "    \n",
    "    for pred, label in zip(preds, labels):\n",
    "        pred_np = pred.flatten()\n",
    "        label_np = (label.flatten() > 0.5).astype(np.float32)\n",
    "        \n",
    "        all_preds.append(pred_np)\n",
    "        all_labels.append(label_np)\n",
    "        \n",
    "        # Per-image best F1\n",
    "        best_f1 = 0\n",
    "        for t in threshs:\n",
    "            pred_bin = (pred_np >= t).astype(np.float32)\n",
    "            tp = np.sum(pred_bin * label_np)\n",
    "            fp = np.sum(pred_bin * (1 - label_np))\n",
    "            fn = np.sum((1 - pred_bin) * label_np)\n",
    "            \n",
    "            precision = tp / (tp + fp + 1e-8)\n",
    "            recall = tp / (tp + fn + 1e-8)\n",
    "            f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "            best_f1 = max(best_f1, f1)\n",
    "        \n",
    "        ois_f1_scores.append(best_f1)\n",
    "    \n",
    "    # Concatenate all for global metrics\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    # ODS: Best global threshold\n",
    "    best_ods = 0\n",
    "    for t in threshs:\n",
    "        pred_bin = (all_preds >= t).astype(np.float32)\n",
    "        tp = np.sum(pred_bin * all_labels)\n",
    "        fp = np.sum(pred_bin * (1 - all_labels))\n",
    "        fn = np.sum((1 - pred_bin) * all_labels)\n",
    "        \n",
    "        precision = tp / (tp + fp + 1e-8)\n",
    "        recall = tp / (tp + fn + 1e-8)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "        best_ods = max(best_ods, f1)\n",
    "    \n",
    "    # OIS: Average of per-image best\n",
    "    ois = np.mean(ois_f1_scores)\n",
    "    \n",
    "    # AP: Average Precision\n",
    "    try:\n",
    "        ap = average_precision_score(all_labels, all_preds)\n",
    "    except:\n",
    "        ap = 0.0\n",
    "    \n",
    "    return best_ods, ois, ap\n",
    "\n",
    "print(\"Loss function and metrics defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "044dd9d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:34:33.508531Z",
     "iopub.status.busy": "2025-12-08T22:34:33.508266Z",
     "iopub.status.idle": "2025-12-08T22:34:33.515619Z",
     "shell.execute_reply": "2025-12-08T22:34:33.514929Z",
     "shell.execute_reply.started": "2025-12-08T22:34:33.508512Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Training Function\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for batch in pbar:\n",
    "        images = batch['images'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(loader, desc='Evaluating'):\n",
    "        images = batch['images'].to(device)\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        for i in range(outputs.shape[0]):\n",
    "            all_preds.append(outputs[i, 0].cpu().numpy())\n",
    "            all_labels.append(labels[i, 0].numpy())\n",
    "    \n",
    "    ods, ois, ap = compute_ods_ois_ap(all_preds, all_labels)\n",
    "    return ods, ois, ap\n",
    "\n",
    "print(\"Training functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bc00dc",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XYW-Net for 3 epochs...\n",
      "Train samples: 531, Val samples: 30\n",
      "============================================================\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 30/133 [06:03<09:38,  5.62s/it, loss=0.7636]"
     ]
    }
   ],
   "source": [
    "# Cell 6: Train XYW-Net (FROM SCRATCH)\n",
    "\n",
    "# -------------------------------\n",
    "# Hyperparameters\n",
    "# -------------------------------\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# -------------------------------\n",
    "# Initialize\n",
    "# -------------------------------\n",
    "model = XYWNet().to(device)\n",
    "criterion = EdgeLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# -------------------------------\n",
    "# Training history\n",
    "# -------------------------------\n",
    "history = {'train_loss': [], 'val_ods': [], 'val_ois': [], 'val_ap': []}\n",
    "best_ods = 0\n",
    "\n",
    "# -------------------------------\n",
    "# Create models directory if it doesn't exist\n",
    "# -------------------------------\n",
    "import os\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "print(f\"Training XYW-Net for {NUM_EPOCHS} epochs...\")\n",
    "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# -------------------------------\n",
    "#  FULL CHECKPOINT SAVE FUNCTION\n",
    "# -------------------------------\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, best_ods, path):\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"scheduler_state\": scheduler.state_dict(),\n",
    "        \"best_ods\": best_ods\n",
    "    }, path)\n",
    "\n",
    "# -------------------------------\n",
    "# TRAINING LOOP\n",
    "# -------------------------------\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    \n",
    "    # Evaluate on validation\n",
    "    val_ods, val_ois, val_ap = evaluate(model, val_loader, device)\n",
    "    history['val_ods'].append(val_ods)\n",
    "    history['val_ois'].append(val_ois)\n",
    "    history['val_ap'].append(val_ap)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Loss: {train_loss:.4f} | ODS: {val_ods:.4f} | OIS: {val_ois:.4f} | AP: {val_ap:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_ods > best_ods:\n",
    "        best_ods = val_ods\n",
    "        \n",
    "        # Save state dict\n",
    "        torch.save(model.state_dict(), f'models/{epoch+1}epoch.pth')\n",
    "        torch.save(model.state_dict(), 'models/best_xyw_net.pth')\n",
    "        \n",
    "        # Save full checkpoint\n",
    "        save_checkpoint(\n",
    "            model,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            epoch,\n",
    "            best_ods,\n",
    "            \"models/xywnet_full_checkpoint.pth\"\n",
    "        )\n",
    "        \n",
    "        # -------------------------------\n",
    "        #  EXPORT DEPLOYMENT .pt MODEL\n",
    "        # -------------------------------\n",
    "        # Use a real image from the dataset for tracing (better than random)\n",
    "        real_sample = train_dataset[0]\n",
    "        example_input = real_sample['images'].unsqueeze(0).to(device)\n",
    "        \n",
    "        # Trace the model with real data\n",
    "        traced_model = torch.jit.trace(model, example_input)\n",
    "        traced_model.save(\"models/xywnet_model.pt\")\n",
    "        \n",
    "        print(f\"  -> Saved best model (ODS: {best_ods:.4f})\")\n",
    "        print(f\"  -> Saved epoch checkpoint: models/{epoch+1}epoch.pth\")\n",
    "        print(f\"  -> Saved traced model: models/xywnet_model.pt\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Training complete. Best ODS: {best_ods:.4f}\")\n",
    "print(\"All files saved to models/ folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529e9a74-832e-4efa-bbd7-ca7e322637ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T22:35:56.302743Z",
     "iopub.status.busy": "2025-12-08T22:35:56.302465Z",
     "iopub.status.idle": "2025-12-08T23:07:03.524561Z",
     "shell.execute_reply": "2025-12-08T23:07:03.523825Z",
     "shell.execute_reply.started": "2025-12-08T22:35:56.302723Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 7: Train XYW-Net (RESUME + FULL CHECKPOINT + PT EXPORT)\n",
    "\n",
    "# -------------------------------\n",
    "# Hyperparameters\n",
    "# -------------------------------\n",
    "NUM_EPOCHS = 20          # Total epochs you want after resume\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# -------------------------------\n",
    "# Initialize\n",
    "# -------------------------------\n",
    "model = XYWNet().to(device)\n",
    "criterion = EdgeLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# -------------------------------\n",
    "# Create models directory if it doesn't exist\n",
    "# -------------------------------\n",
    "import os\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "#  RESUME FROM STATE_DICT (19 EPOCHS DONE)\n",
    "# -------------------------------\n",
    "start_epoch = 19  # You already trained 19 epochs\n",
    "\n",
    "print(\"Loading pretrained weights from 20epoch.pth ...\")\n",
    "# KAGGLE PATH (comment out when not using Kaggle):\n",
    "# model.load_state_dict(torch.load(\"/kaggle/input/20epoch/pytorch/default/1/20epoch.pth\", map_location=device))\n",
    "# LOCAL PATH:\n",
    "model.load_state_dict(torch.load(\"models/20epoch.pth\", map_location=device))\n",
    "\n",
    "print(f\"Resuming training from epoch {start_epoch+1}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Training history\n",
    "# -------------------------------\n",
    "history = {'train_loss': [], 'val_ods': [], 'val_ois': [], 'val_ap': []}\n",
    "best_ods = 0\n",
    "\n",
    "print(f\"Training XYW-Net for {NUM_EPOCHS} total epochs...\")\n",
    "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# -------------------------------\n",
    "#  FULL CHECKPOINT SAVE FUNCTION\n",
    "# -------------------------------\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, best_ods, path):\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"scheduler_state\": scheduler.state_dict(),\n",
    "        \"best_ods\": best_ods\n",
    "    }, path)\n",
    "\n",
    "# -------------------------------\n",
    "#  TRAINING LOOP\n",
    "# -------------------------------\n",
    "for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_ods, val_ois, val_ap = evaluate(model, val_loader, device)\n",
    "    history['val_ods'].append(val_ods)\n",
    "    history['val_ois'].append(val_ois)\n",
    "    history['val_ap'].append(val_ap)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Loss: {train_loss:.4f} | ODS: {val_ods:.4f} | OIS: {val_ois:.4f} | AP: {val_ap:.4f}\")\n",
    "    \n",
    "    #  SAVE BEST MODEL (STATE_DICT)\n",
    "    if val_ods > best_ods: \n",
    "        best_ods = val_ods\n",
    "        \n",
    "        # Save epoch checkpoint and best model\n",
    "        torch.save(model.state_dict(), f\"models/{epoch+1}epoch.pth\")\n",
    "        torch.save(model.state_dict(), \"models/best_xyw_net.pth\")\n",
    "        \n",
    "        print(f\"  -> Saved best model (ODS: {best_ods:.4f})\")\n",
    "        print(\"------------\")\n",
    "        \n",
    "        #  SAVE FULL CHECKPOINT\n",
    "        save_checkpoint(\n",
    "            model,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            epoch,\n",
    "            best_ods,\n",
    "            \"models/xywnet_full_checkpoint.pth\"\n",
    "        )\n",
    "        \n",
    "        # -------------------------------\n",
    "        #  EXPORT DEPLOYMENT .pt MODEL\n",
    "        # -------------------------------\n",
    "        # Use a real image from the dataset for tracing (better than random)\n",
    "        real_sample = train_dataset[0]\n",
    "        example_input = real_sample['images'].unsqueeze(0).to(device)\n",
    "        \n",
    "        # Trace the model with real data\n",
    "        traced_model = torch.jit.trace(model, example_input)\n",
    "        traced_model.save(\"models/xywnet_model.pt\")\n",
    "        \n",
    "        print(f\"  -> Saved epoch checkpoint: models/{epoch+1}epoch.pth\")\n",
    "        print(f\"  -> Saved traced model: models/xywnet_model.pt\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Training complete. Best ODS: {best_ods:.4f}\")\n",
    "print(\"All files saved to models/ folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d8b97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T23:19:37.590232Z",
     "iopub.status.busy": "2025-12-08T23:19:37.589542Z",
     "iopub.status.idle": "2025-12-08T23:19:38.252358Z",
     "shell.execute_reply": "2025-12-08T23:19:38.251711Z",
     "shell.execute_reply.started": "2025-12-08T23:19:37.590204Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 7: Plot Training History\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], 'b-', linewidth=2, label='Train Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Metrics\n",
    "axes[1].plot(history['val_ods'], 'g-', linewidth=2, label='ODS')\n",
    "axes[1].plot(history['val_ois'], 'b-', linewidth=2, label='OIS')\n",
    "axes[1].plot(history['val_ap'], 'r-', linewidth=2, label='AP')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Validation Metrics')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb43ffe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T02:50:21.903376Z",
     "iopub.status.busy": "2025-12-08T02:50:21.903128Z",
     "iopub.status.idle": "2025-12-08T02:51:57.927419Z",
     "shell.execute_reply": "2025-12-08T02:51:57.926311Z",
     "shell.execute_reply.started": "2025-12-08T02:50:21.903361Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 8: Final Test Evaluation\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_xyw_net.pth'))\n",
    "print(\"Loaded best model weights.\\n\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating on TEST set...\")\n",
    "test_ods, test_ois, test_ap = evaluate(model, test_loader, device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL TEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  ODS (Optimal Dataset Scale): {test_ods:.4f}\")\n",
    "print(f\"  OIS (Optimal Image Scale):   {test_ois:.4f}\")\n",
    "print(f\"  AP (Average Precision):      {test_ap:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ff6571",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T23:19:49.789134Z",
     "iopub.status.busy": "2025-12-08T23:19:49.788825Z",
     "iopub.status.idle": "2025-12-08T23:19:54.771524Z",
     "shell.execute_reply": "2025-12-08T23:19:54.770655Z",
     "shell.execute_reply.started": "2025-12-08T23:19:49.789111Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 9: Visualize Edge Predictions\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_predictions(model, dataset, device, num_samples=6):\n",
    "    \"\"\"Visualize predictions on random samples\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        sample = dataset[idx]\n",
    "        img = sample['images'].unsqueeze(0).to(device)\n",
    "        label = sample['labels'][0].numpy()\n",
    "        \n",
    "        pred = model(img)[0, 0].cpu().numpy()\n",
    "        \n",
    "        # Original image\n",
    "        axes[i, 0].imshow(sample['images'].permute(1, 2, 0).numpy())\n",
    "        axes[i, 0].set_title(f'Input: {sample[\"filename\"]}')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Ground truth\n",
    "        axes[i, 1].imshow(label, cmap='gray')\n",
    "        axes[i, 1].set_title('Ground Truth')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Prediction\n",
    "        axes[i, 2].imshow(pred, cmap='gray')\n",
    "        axes[i, 2].set_title('XYW-Net Prediction')\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('predictions.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions(model, test_dataset, device, num_samples=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db648f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T23:20:13.145122Z",
     "iopub.status.busy": "2025-12-08T23:20:13.144514Z",
     "iopub.status.idle": "2025-12-08T23:20:18.952991Z",
     "shell.execute_reply": "2025-12-08T23:20:18.952223Z",
     "shell.execute_reply.started": "2025-12-08T23:20:13.145100Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 10: Visualize Encoder Stage Outputs\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_stages(model, dataset, device, sample_idx=0):\n",
    "    \"\"\"Visualize feature maps at each encoder stage\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    sample = dataset[sample_idx]\n",
    "    img = sample['images'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get stage outputs\n",
    "    final, (s1, s2, s3, s4) = model.forward_with_stages(img)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0, 0].imshow(sample['images'].permute(1, 2, 0).numpy())\n",
    "    axes[0, 0].set_title('Input Image')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Ground truth\n",
    "    axes[0, 1].imshow(sample['labels'][0].numpy(), cmap='gray')\n",
    "    axes[0, 1].set_title('Ground Truth')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    # Final prediction\n",
    "    axes[0, 2].imshow(final[0, 0].cpu().numpy(), cmap='gray')\n",
    "    axes[0, 2].set_title('Final Prediction')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    # Thresholded prediction\n",
    "    pred_binary = (final[0, 0].cpu().numpy() > 0.5).astype(float)\n",
    "    axes[0, 3].imshow(pred_binary, cmap='gray')\n",
    "    axes[0, 3].set_title('Prediction (threshold=0.5)')\n",
    "    axes[0, 3].axis('off')\n",
    "    \n",
    "    # Stage feature maps (mean across channels)\n",
    "    stages = [s1, s2, s3, s4]\n",
    "    stage_names = ['Stage 1 (30ch)', 'Stage 2 (60ch)', 'Stage 3 (120ch)', 'Stage 4 (120ch)']\n",
    "    \n",
    "    for i, (stage, name) in enumerate(zip(stages, stage_names)):\n",
    "        feat_mean = stage[0].mean(dim=0).cpu().numpy()\n",
    "        feat_norm = (feat_mean - feat_mean.min()) / (feat_mean.max() - feat_mean.min() + 1e-8)\n",
    "        axes[1, i].imshow(feat_norm, cmap='viridis')\n",
    "        axes[1, i].set_title(f'{name}\\nShape: {tuple(stage.shape[2:])}')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'XYW-Net Encoder Stage Outputs - {sample[\"filename\"]}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('stage_outputs.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize for a few samples\n",
    "for idx in [0, 1, 2]:\n",
    "    if idx < len(test_dataset):\n",
    "        visualize_stages(model, test_dataset, device, sample_idx=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa0ddb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T02:57:24.535398Z",
     "iopub.status.busy": "2025-12-08T02:57:24.535097Z",
     "iopub.status.idle": "2025-12-08T02:58:29.333797Z",
     "shell.execute_reply": "2025-12-08T02:58:29.332943Z",
     "shell.execute_reply.started": "2025-12-08T02:57:24.535377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 11: Precision-Recall Curve\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_pr_curve(model, loader, device):\n",
    "    \"\"\"Plot precision-recall curve\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(loader, desc='Computing PR curve'):\n",
    "        images = batch['images'].to(device)\n",
    "        labels = batch['labels']\n",
    "        outputs = model(images)\n",
    "        \n",
    "        for i in range(outputs.shape[0]):\n",
    "            all_preds.append(outputs[i, 0].cpu().numpy().flatten())\n",
    "            all_labels.append((labels[i, 0].numpy().flatten() > 0.5).astype(int))\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(all_labels, all_preds)\n",
    "    ap = average_precision_score(all_labels, all_preds)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, 'b-', linewidth=2, label=f'XYW-Net (AP={ap:.4f})')\n",
    "    plt.xlabel('Recall', fontsize=12)\n",
    "    plt.ylabel('Precision', fontsize=12)\n",
    "    plt.title('Precision-Recall Curve', fontsize=14)\n",
    "    plt.legend(loc='lower left', fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.savefig('pr_curve.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    return ap\n",
    "\n",
    "ap = plot_pr_curve(model, test_loader, device)\n",
    "print(f\"Average Precision: {ap:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94973424",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-08T02:50:10.326928Z",
     "iopub.status.idle": "2025-12-08T02:50:10.327368Z",
     "shell.execute_reply": "2025-12-08T02:50:10.327255Z",
     "shell.execute_reply.started": "2025-12-08T02:50:10.327242Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 12: Summary Results Table\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                    XYW-NET EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\")\n",
    "print(f\"  Dataset:          {DATA_ROOT}\")\n",
    "print(f\"  Train samples:    {len(train_dataset)}\")\n",
    "print(f\"  Val samples:      {len(val_dataset)}\")\n",
    "print(f\"  Test samples:     {len(test_dataset)}\")\n",
    "print(f\"\")\n",
    "print(f\"  Training epochs:  {NUM_EPOCHS}\")\n",
    "print(f\"  Learning rate:    {LEARNING_RATE}\")\n",
    "print(f\"  Batch size:       {BATCH_SIZE}\")\n",
    "print(f\"\")\n",
    "print(\"  \" + \"-\"*50)\n",
    "print(f\"  TEST SET METRICS:\")\n",
    "print(\"  \" + \"-\"*50)\n",
    "print(f\"  ODS (Optimal Dataset Scale):   {test_ods:.4f}\")\n",
    "print(f\"  OIS (Optimal Image Scale):     {test_ois:.4f}\")\n",
    "print(f\"  AP (Average Precision):        {test_ap:.4f}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\")\n",
    "print(\"Saved files:\")\n",
    "print(\"  - best_xyw_net.pth      (model weights)\")\n",
    "print(\"  - training_history.png  (loss/metrics plot)\")\n",
    "print(\"  - predictions.png       (sample predictions)\")\n",
    "print(\"  - stage_outputs.png     (encoder stages)\")\n",
    "print(\"  - pr_curve.png          (precision-recall curve)\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8942655,
     "sourceId": 14046899,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8943377,
     "sourceId": 14048368,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 527488,
     "modelInstanceId": 512848,
     "sourceId": 676420,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 528001,
     "modelInstanceId": 513371,
     "sourceId": 677020,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
