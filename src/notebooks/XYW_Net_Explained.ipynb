{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1738d113",
   "metadata": {},
   "source": [
    "# XYW-Net: Edge Detection Inspired by Biology\n",
    "\n",
    "**Goal**: Understand how the brain's visual cortex detects edges and use that to build a deep learning model.\n",
    "\n",
    "This notebook explains the **XYW-Net** architecture - an edge detection network based on biological vision principles.\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Overview\n",
    "\n",
    "XYW-Net has three main ideas:\n",
    "1. **X and Y pathways**: Mimic how retinal neurons respond to small vs large stimuli\n",
    "2. **W pathway**: Detect directional edges (horizontal and vertical)\n",
    "3. **Multi-scale encoder-decoder**: Process at multiple image scales, then combine them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16dd382",
   "metadata": {},
   "source": [
    "## Section 1: Biology - How the Retina Works\n",
    "\n",
    "### The Center-Surround Receptive Field\n",
    "\n",
    "The retina has two types of cells that detect light:\n",
    "\n",
    "**ON-cells**: Fire when light hits the center\n",
    "**OFF-cells**: Fire when light hits the surround (background dims)\n",
    "\n",
    "Together they create a **center-surround** filter:\n",
    "\n",
    "```\n",
    "Response = Center - Surround\n",
    "```\n",
    "\n",
    "This is like asking: \"Is the center different from its neighbors?\"\n",
    "\n",
    "### Two Scales\n",
    "\n",
    "The retina actually has **two sizes** of center-surround filters:\n",
    "\n",
    "- **Small scale (X)**: Detects fine details (center ≈ 1×1, surround ≈ 3×3)\n",
    "- **Large scale (Y)**: Detects coarse features (center ≈ 1×1, surround ≈ 5×5 with dilation)\n",
    "\n",
    "**Math**:\n",
    "$$X = \\text{Center}_{\\text{small}} - \\text{Surround}_{\\text{small}}$$\n",
    "$$Y = \\text{Center}_{\\text{large}} - \\text{Surround}_{\\text{large}}$$\n",
    "\n",
    "### Directional Selectivity\n",
    "\n",
    "The visual cortex has **simple cells** that respond to edges in specific directions:\n",
    "\n",
    "- Horizontal edges → Horizontal filter (1×3 kernel)\n",
    "- Vertical edges → Vertical filter (3×1 kernel)\n",
    "\n",
    "**Biological basis**: These cells have elongated receptive fields aligned to one direction.\n",
    "\n",
    "**Math** (depthwise convolution):\n",
    "$$W_{\\text{horizontal}} = \\text{Conv}(x, [1, 3])$$\n",
    "$$W_{\\text{vertical}} = \\text{Conv}(x, [3, 1])$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0da6b9",
   "metadata": {},
   "source": [
    "## Section 2: Architecture Overview\n",
    "\n",
    "### The Four-Scale Encoder\n",
    "\n",
    "Images have edges at different scales. A small feature (50 pixels) is different from a boundary (500 pixels).\n",
    "\n",
    "XYW-Net processes at 4 scales (S1, S2, S3, S4):\n",
    "\n",
    "```\n",
    "Input Image (512×512)\n",
    "    ↓ (S1) No pooling\n",
    "  [30 channels] ← Detects fine edges\n",
    "    ↓ Pool 2× (S2)\n",
    "  [60 channels] ← Detects medium edges  \n",
    "    ↓ Pool 2× (S3)\n",
    "  [120 channels] ← Detects large edges\n",
    "    ↓ Pool 2× (S4)\n",
    "  [120 channels] ← Detects very large features\n",
    "```\n",
    "\n",
    "Each stage **S1, S2, S3, S4** applies the XYW operation.\n",
    "\n",
    "### The XYW Operation\n",
    "\n",
    "Each stage does the same thing - split, process, merge:\n",
    "\n",
    "```\n",
    "Input → [XYW_S]\n",
    "          ├─ X pathway (small center-surround)\n",
    "          ├─ Y pathway (large center-surround)\n",
    "          └─ W pathway (directional)\n",
    "             ↓\n",
    "        [XYW] Process each pathway\n",
    "             ↓\n",
    "        [XYW_E] Merge: X + Y + W\n",
    "             ↓\n",
    "        Output + Shortcut\n",
    "```\n",
    "\n",
    "**Why three pathways?**\n",
    "- **X** catches fine texture changes\n",
    "- **Y** catches large boundary changes  \n",
    "- **W** catches oriented edges\n",
    "- Together: Complete edge representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5707aadb",
   "metadata": {},
   "source": [
    "## Section 3: The X Pathway - Small Scale Details\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "The X pathway detects **local contrast** - is the center different from its immediate neighbors?\n",
    "\n",
    "**Xc block**:\n",
    "```python\n",
    "Center_1×1 = Conv(x, kernel_size=1)\n",
    "Surround_3×3 = GroupConv(x, kernel_size=3, padding=1)\n",
    "X = Surround - Center\n",
    "```\n",
    "\n",
    "**Why this order?**\n",
    "- Center is a **point sampling** (1×1) = sharp, focused\n",
    "- Surround is **neighborhood** (3×3) = blurred, local context\n",
    "- Difference emphasizes **edges** where contrast is high\n",
    "\n",
    "**Math**:\n",
    "$$X_c = S(x) - C(x)$$\n",
    "\n",
    "where $S$ is a 3×3 convolution and $C$ is a 1×1 convolution.\n",
    "\n",
    "### Biological Analog\n",
    "\n",
    "This mimics **OFF-center/ON-surround** retinal ganglion cells.\n",
    "- When a bright edge passes the center: C ↑↑, S ↑ → X is small\n",
    "- When edge is at surround: S ↑↑, C ↑ → X is large ✓\n",
    "\n",
    "**Result**: Detects **fine texture edges** and **small boundaries**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0257b2a7",
   "metadata": {},
   "source": [
    "## Section 4: The Y Pathway - Large Scale Boundaries\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "The Y pathway detects **global contrast** - large structural changes at object boundaries.\n",
    "\n",
    "**Yc block**:\n",
    "```python\n",
    "Center_1×1 = Conv(x, kernel_size=1)\n",
    "Surround_5×5 = GroupConv(x, kernel_size=5, dilation=2, padding=4)\n",
    "Y = Surround - Center\n",
    "```\n",
    "\n",
    "**Key difference from X**: \n",
    "- Uses **dilation=2** → 5×5 becomes 9×9 effective receptive field\n",
    "- Larger surround captures **bigger context**\n",
    "\n",
    "**Math**:\n",
    "$$Y_c = S_{\\text{large}}(x) - C(x)$$\n",
    "\n",
    "The dilation parameter $d$ expands the kernel:\n",
    "$$\\text{Effective size} = (k-1) \\times d + 1 = (5-1) \\times 2 + 1 = 9$$\n",
    "\n",
    "### Biological Analog\n",
    "\n",
    "Mimics **magnocellular** (large, motion-sensitive) retinal cells:\n",
    "- Respond to large, slow contrasts\n",
    "- Sensitive to object boundaries and structure\n",
    "- Have larger receptive fields than parvocellular cells\n",
    "\n",
    "**Result**: Detects **major object boundaries** and **large structural edges**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f48c9c",
   "metadata": {},
   "source": [
    "## Section 5: The W Pathway - Directional Edges\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "The W pathway detects **oriented edges** - is there an edge going left-right or up-down?\n",
    "\n",
    "**W block** (depthwise separable convolution):\n",
    "```python\n",
    "Horizontal = GroupConv(x, kernel_size=(1,3), padding=(0,1))\n",
    "Vertical = GroupConv(Horizontal, kernel_size=(3,1), padding=(1,0))\n",
    "W_output = Conv1×1(Vertical)\n",
    "```\n",
    "\n",
    "**Why depthwise?** Each channel is processed independently → low-parameter, efficient.\n",
    "\n",
    "**Math**:\n",
    "$$W_h = \\text{GroupConv}(x, [1,3])$$\n",
    "$$W_v = \\text{GroupConv}(W_h, [3,1])$$\n",
    "$$W = \\text{Conv}_{1×1}(W_v)$$\n",
    "\n",
    "### Biological Analog\n",
    "\n",
    "Mimics **simple cells** in primary visual cortex (V1):\n",
    "- Each neuron prefers a specific **orientation**: 0°, 45°, 90°, 135°...\n",
    "- Receptive fields are **elongated** in the preferred direction\n",
    "- Show strong **orientation selectivity**\n",
    "\n",
    "A vertical edge kernel detects **vertical changes**:\n",
    "```\n",
    "[+1]\n",
    "[+1]  ← Detects white-to-black transitions from left to right\n",
    "[+1]\n",
    "```\n",
    "\n",
    "**Result**: Detects **oriented structural edges** (boundaries between regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c2f12c",
   "metadata": {},
   "source": [
    "## Section 6: The Decoder - Combining Multi-Scale Information\n",
    "\n",
    "### Problem\n",
    "\n",
    "After the encoder (S1→S2→S3→S4), we have:\n",
    "- S1: High resolution, fine details (512×512)\n",
    "- S4: Low resolution, coarse structure (64×64)\n",
    "\n",
    "We need to **merge them back** to a single output map at S1 resolution.\n",
    "\n",
    "### Solution: Hierarchical Fusion\n",
    "\n",
    "```\n",
    "S4 (64×64) ──────────────────────┐\n",
    "                                   ↓\n",
    "S3 (128×128) ───────────────────[F43] Refine\n",
    "                                   ↓\n",
    "S2 (256×256) ───────────────────[F32] Refine\n",
    "                                   ↓\n",
    "S1 (512×512) ───────────────────[F21] Refine\n",
    "                                   ↓\n",
    "Output (512×512, 1 channel)\n",
    "```\n",
    "\n",
    "### The Refine Block (Bilinear Upsampling)\n",
    "\n",
    "Each Fij block:\n",
    "1. **Upsample** deeper layer with bilinear interpolation\n",
    "2. **Add** to shallower layer\n",
    "3. **Refine** with adaptive convolution\n",
    "\n",
    "**Math**:\n",
    "$$\\text{Fij} = S_i + \\text{Upsample}(S_j, \\text{factor}=2)$$\n",
    "\n",
    "where Upsample uses learned **bilinear weights**:\n",
    "\n",
    "$$U(x,y) = \\left(1 - |x|\\right) \\times \\left(1 - |y|\\right)$$\n",
    "\n",
    "**Result**: Combines coarse structure with fine details at each level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1925b433",
   "metadata": {},
   "source": [
    "## Section 7: Setup & Load Model\n",
    "\n",
    "Let's implement XYW-Net step by step.\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a16df4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting imports... (this may take 10-30 seconds on first run)\n",
      "------------------------------------------------------------\n",
      "Importing NumPy... ✓\n",
      "Importing PyTorch... ✓\n",
      "Importing PyTorch... ✓\n",
      "Importing torch.nn... ✓\n",
      "Importing torch.nn.functional... ✓\n",
      "Importing OpenCV... ✓\n",
      "Importing torch.nn... ✓\n",
      "Importing torch.nn.functional... ✓\n",
      "Importing OpenCV... ✓\n",
      "Importing Matplotlib... ✓\n",
      "Importing Matplotlib... ✓\n",
      "------------------------------------------------------------\n",
      "\n",
      "Detecting GPU... ✓ Using cuda\n",
      "  GPU: NVIDIA GeForce GTX 1070\n",
      "  Memory: 8.6 GB\n",
      "✓ XYW-Net folder found\n",
      "\n",
      "✓ All imports complete!\n",
      "✓\n",
      "------------------------------------------------------------\n",
      "\n",
      "Detecting GPU... ✓ Using cuda\n",
      "  GPU: NVIDIA GeForce GTX 1070\n",
      "  Memory: 8.6 GB\n",
      "✓ XYW-Net folder found\n",
      "\n",
      "✓ All imports complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"Starting imports... (this may take 10-30 seconds on first run)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"Importing NumPy...\", end=\" \", flush=True)\n",
    "import numpy as np\n",
    "print(\"✓\")\n",
    "\n",
    "# Import torch (slowest part - takes time)\n",
    "print(\"Importing PyTorch...\", end=\" \", flush=True)\n",
    "import torch\n",
    "print(\"✓\")\n",
    "\n",
    "print(\"Importing torch.nn...\", end=\" \", flush=True)\n",
    "import torch.nn as nn\n",
    "print(\"✓\")\n",
    "\n",
    "print(\"Importing torch.nn.functional...\", end=\" \", flush=True)\n",
    "import torch.nn.functional as F\n",
    "print(\"✓\")\n",
    "\n",
    "\n",
    "print(\"Importing OpenCV...\", end=\" \", flush=True)\n",
    "import cv2\n",
    "print(\"✓\")\n",
    "\n",
    "print(\"Importing Matplotlib...\", end=\" \", flush=True)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle, FancyBboxPatch\n",
    "print(\"✓\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Set device (this may take a few seconds if CUDA is present)\n",
    "print(\"\\nDetecting GPU...\", end=\" \", flush=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"✓ Using {device}\")\n",
    "\n",
    "# Quick CUDA info if available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Check if XYW-Net folder exists\n",
    "xyw_path = r\"c:\\Users\\imed\\Desktop\\dege detection implimentation\\XYW-Net\"\n",
    "if os.path.exists(xyw_path):\n",
    "    print(f\"✓ XYW-Net folder found\")\n",
    "else:\n",
    "    print(f\"⚠ XYW-Net folder not found\")\n",
    "\n",
    "print(\"\\n✓ All imports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950c959d",
   "metadata": {},
   "source": [
    "## Section 8: Implement Core Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fff1384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Biological pathways defined (X, Y, W)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BIOLOGICAL PATHWAYS\n",
    "# ============================================================================\n",
    "\n",
    "class Xc1x1(nn.Module):\n",
    "    \"\"\"\n",
    "    X Pathway: Small-scale center-surround\n",
    "    Detects fine texture edges.\n",
    "    \n",
    "    X = Surround_3x3(x) - Center_1x1(x)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Xc1x1, self).__init__()\n",
    "        \n",
    "        # Center: Sharp point sampling (1×1)\n",
    "        self.center = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        \n",
    "        # Surround: Local context (3×3, depthwise to stay local)\n",
    "        self.surround = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                                  padding=1, groups=in_channels)\n",
    "        self.surround_1x1 = nn.Conv2d(out_channels, out_channels, kernel_size=1)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        center = self.relu(self.center(x))\n",
    "        surround = self.relu(self.surround(x))\n",
    "        surround = self.surround_1x1(surround)\n",
    "        \n",
    "        # Key: Surround - Center detects edges\n",
    "        return surround - center\n",
    "\n",
    "\n",
    "class Yc1x1(nn.Module):\n",
    "    \"\"\"\n",
    "    Y Pathway: Large-scale center-surround\n",
    "    Detects large boundary changes.\n",
    "    \n",
    "    Y = Surround_5x5_dilated(x) - Center_1x1(x)\n",
    "    \n",
    "    Dilation=2 expands receptive field to ~9×9 without increasing parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Yc1x1, self).__init__()\n",
    "        \n",
    "        # Center: Point sampling\n",
    "        self.center = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        \n",
    "        # Surround: Large dilated context (5×5 with dilation=2 → ~9×9 RF)\n",
    "        self.surround = nn.Conv2d(in_channels, out_channels, kernel_size=5, \n",
    "                                  padding=4, dilation=2, groups=in_channels)\n",
    "        self.surround_1x1 = nn.Conv2d(out_channels, out_channels, kernel_size=1)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        center = self.relu(self.center(x))\n",
    "        surround = self.relu(self.surround(x))\n",
    "        surround = self.surround_1x1(surround)\n",
    "        \n",
    "        return surround - center\n",
    "\n",
    "\n",
    "class W(nn.Module):\n",
    "    \"\"\"\n",
    "    W Pathway: Directional edges (horizontal + vertical)\n",
    "    Detects oriented structural edges.\n",
    "    \n",
    "    Uses depthwise separable convolution:\n",
    "    - (1×3) kernel detects horizontal edges\n",
    "    - (3×1) kernel detects vertical edges\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(W, self).__init__()\n",
    "        \n",
    "        # Horizontal edge detection (1×3)\n",
    "        self.horizontal = nn.Conv2d(in_channels, in_channels, kernel_size=(1, 3), \n",
    "                                    padding=(0, 1), groups=in_channels)\n",
    "        self.h_1x1 = nn.Conv2d(in_channels, in_channels, kernel_size=1, bias=False)\n",
    "        \n",
    "        # Vertical edge detection (3×1, applied to horizontal output)\n",
    "        self.vertical = nn.Conv2d(in_channels, in_channels, kernel_size=(3, 1), \n",
    "                                  padding=(1, 0), groups=in_channels)\n",
    "        self.v_1x1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Horizontal edges\n",
    "        h = self.relu(self.horizontal(x))\n",
    "        h = self.h_1x1(h)\n",
    "        \n",
    "        # Vertical edges applied to horizontal result\n",
    "        v = self.relu(self.vertical(h))\n",
    "        v = self.v_1x1(v)\n",
    "        \n",
    "        return v\n",
    "\n",
    "print(\"✓ Biological pathways defined (X, Y, W)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83c20125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ XYW blocks defined (Start, Process, End)\n"
     ]
    }
   ],
   "source": [
    "class XYW_S(nn.Module):\n",
    "    \"\"\"\n",
    "    XYW_Start: Initialize the three pathways at the beginning of each stage.\n",
    "    \n",
    "    Input → [X pathway] → X features\n",
    "         → [Y pathway] → Y features\n",
    "         → [W pathway] → W features\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(XYW_S, self).__init__()\n",
    "        self.x_c = Xc1x1(in_channels, out_channels)\n",
    "        self.y_c = Yc1x1(in_channels, out_channels)\n",
    "        self.w = W(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_features = self.x_c(x)\n",
    "        y_features = self.y_c(x)\n",
    "        w_features = self.w(x)\n",
    "        return x_features, y_features, w_features\n",
    "\n",
    "\n",
    "class XYW(nn.Module):\n",
    "    \"\"\"\n",
    "    XYW_Process: Continue processing each pathway.\n",
    "    Each pathway updates independently.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(XYW, self).__init__()\n",
    "        self.x_c = Xc1x1(in_channels, out_channels)\n",
    "        self.y_c = Yc1x1(in_channels, out_channels)\n",
    "        self.w = W(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x, y, w):\n",
    "        x = self.x_c(x)\n",
    "        y = self.y_c(y)\n",
    "        w = self.w(w)\n",
    "        return x, y, w\n",
    "\n",
    "\n",
    "class XYW_E(nn.Module):\n",
    "    \"\"\"\n",
    "    XYW_End: Merge the three pathways.\n",
    "    Fuses X + Y + W into a single output.\n",
    "    \n",
    "    This is where we combine:\n",
    "    - Fine details (X)\n",
    "    - Large boundaries (Y)\n",
    "    - Oriented structures (W)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(XYW_E, self).__init__()\n",
    "        self.x_c = Xc1x1(in_channels, out_channels)\n",
    "        self.y_c = Yc1x1(in_channels, out_channels)\n",
    "        self.w = W(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x, y, w):\n",
    "        x = self.x_c(x)\n",
    "        y = self.y_c(y)\n",
    "        w = self.w(w)\n",
    "        # Merge: Each pathway contributes equally\n",
    "        return x + y + w\n",
    "\n",
    "print(\"✓ XYW blocks defined (Start, Process, End)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe885fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Encoder stages defined (S1, S2, S3, S4)\n"
     ]
    }
   ],
   "source": [
    "class s1(nn.Module):\n",
    "    \"\"\"\n",
    "    Stage 1 (S1): Full resolution (512×512)\n",
    "    \n",
    "    Flow:\n",
    "    Input → Initial Conv → XYW_S → XYW → XYW_E → Output + Input (residual)\n",
    "    \n",
    "    No pooling here - preserves fine detail.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, channel=30):\n",
    "        super(s1, self).__init__()\n",
    "        \n",
    "        # Initial feature extraction\n",
    "        self.conv1 = nn.Conv2d(in_channels, channel, kernel_size=7, \n",
    "                               padding=6, dilation=2)\n",
    "        \n",
    "        # XYW pathways\n",
    "        self.xyw_s = XYW_S(channel, channel)\n",
    "        self.xyw = XYW(channel, channel)\n",
    "        self.xyw_e = XYW_E(channel, channel)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        temp = self.relu(self.conv1(x))\n",
    "        \n",
    "        # Initialize pathways\n",
    "        x_feat, y_feat, w_feat = self.xyw_s(temp)\n",
    "        \n",
    "        # Process pathways\n",
    "        x_feat, y_feat, w_feat = self.xyw(x_feat, y_feat, w_feat)\n",
    "        \n",
    "        # Merge pathways\n",
    "        merged = self.xyw_e(x_feat, y_feat, w_feat)\n",
    "        \n",
    "        # Residual connection: add original features back\n",
    "        return merged + temp\n",
    "\n",
    "\n",
    "class s2(nn.Module):\n",
    "    \"\"\"\n",
    "    Stage 2 (S2): Half resolution (256×256)\n",
    "    \n",
    "    Applied after MaxPool(2×).\n",
    "    Captures medium-scale edges.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=30, out_channels=60):\n",
    "        super(s2, self).__init__()\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.xyw_s = XYW_S(in_channels, out_channels)\n",
    "        self.xyw = XYW(out_channels, out_channels)\n",
    "        self.xyw_e = XYW_E(out_channels, out_channels)\n",
    "        \n",
    "        # Shortcut to match channel dimensions\n",
    "        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Initialize, process, merge\n",
    "        x_feat, y_feat, w_feat = self.xyw_s(x)\n",
    "        x_feat, y_feat, w_feat = self.xyw(x_feat, y_feat, w_feat)\n",
    "        merged = self.xyw_e(x_feat, y_feat, w_feat)\n",
    "        \n",
    "        # Residual with dimension matching\n",
    "        shortcut = self.shortcut(x)\n",
    "        return merged + shortcut\n",
    "\n",
    "\n",
    "class s3(nn.Module):\n",
    "    \"\"\"Stage 3 (S3): Quarter resolution (128×128)\"\"\"\n",
    "    def __init__(self, in_channels=60, out_channels=120):\n",
    "        super(s3, self).__init__()\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.xyw_s = XYW_S(in_channels, out_channels)\n",
    "        self.xyw = XYW(out_channels, out_channels)\n",
    "        self.xyw_e = XYW_E(out_channels, out_channels)\n",
    "        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x_feat, y_feat, w_feat = self.xyw_s(x)\n",
    "        x_feat, y_feat, w_feat = self.xyw(x_feat, y_feat, w_feat)\n",
    "        merged = self.xyw_e(x_feat, y_feat, w_feat)\n",
    "        return merged + self.shortcut(x)\n",
    "\n",
    "\n",
    "class s4(nn.Module):\n",
    "    \"\"\"Stage 4 (S4): Eighth resolution (64×64)\"\"\"\n",
    "    def __init__(self, in_channels=120, out_channels=120):\n",
    "        super(s4, self).__init__()\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.xyw_s = XYW_S(in_channels, out_channels)\n",
    "        self.xyw = XYW(out_channels, out_channels)\n",
    "        self.xyw_e = XYW_E(out_channels, out_channels)\n",
    "        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x_feat, y_feat, w_feat = self.xyw_s(x)\n",
    "        x_feat, y_feat, w_feat = self.xyw(x_feat, y_feat, w_feat)\n",
    "        merged = self.xyw_e(x_feat, y_feat, w_feat)\n",
    "        return merged + self.shortcut(x)\n",
    "\n",
    "print(\"✓ Encoder stages defined (S1, S2, S3, S4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "764a1410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Decoder defined (Bilinear upsampling + RCF fusion)\n"
     ]
    }
   ],
   "source": [
    "def bilinear_upsample_weights(factor, out_channels):\n",
    "    \"\"\"\n",
    "    Create bilinear interpolation weights for transposed convolution.\n",
    "    \n",
    "    Bilinear formula for a point (x, y) in normalized [0, 1] space:\n",
    "    U(x, y) = (1 - |x|) × (1 - |y|)\n",
    "    \n",
    "    This is NOT learned - it's a fixed, mathematically defined kernel.\n",
    "    \"\"\"\n",
    "    filter_size = 2 * factor - factor % 2\n",
    "    weights = np.zeros((out_channels, out_channels, filter_size, filter_size))\n",
    "    \n",
    "    # Create the bilinear kernel\n",
    "    factor_val = (filter_size + 1) // 2\n",
    "    if filter_size % 2 == 1:\n",
    "        center = factor_val - 1\n",
    "    else:\n",
    "        center = factor_val - 0.5\n",
    "    \n",
    "    # Bilinear distance\n",
    "    og = np.ogrid[:filter_size, :filter_size]\n",
    "    bilinear_kernel = (1 - np.abs(og[0] - center) / factor_val) * \\\n",
    "                      (1 - np.abs(og[1] - center) / factor_val)\n",
    "    \n",
    "    # Apply to each channel (identity mapping)\n",
    "    for i in range(out_channels):\n",
    "        weights[i, i, :, :] = bilinear_kernel\n",
    "    \n",
    "    return torch.Tensor(weights)\n",
    "\n",
    "\n",
    "class Refine_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Fusion block: Combines features from adjacent scales.\n",
    "    \n",
    "    Operation:\n",
    "    Output = S_shallow + Upsample(S_deep, 2×)\n",
    "    \n",
    "    Example: F43 combines S3 and S4\n",
    "    - S4 (64×64) is upsampled 2× → 128×128\n",
    "    - Added to S3 (128×128)\n",
    "    - Result refined and passed to next stage\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels_shallow, in_channels_deep, out_channels, \n",
    "                 upsample_factor=2):\n",
    "        super(Refine_block, self).__init__()\n",
    "        \n",
    "        self.upsample_factor = upsample_factor\n",
    "        \n",
    "        # Simple convolution to prepare each stream\n",
    "        self.conv_shallow = nn.Conv2d(in_channels_shallow, out_channels, \n",
    "                                      kernel_size=3, padding=1)\n",
    "        self.conv_deep = nn.Conv2d(in_channels_deep, out_channels, \n",
    "                                   kernel_size=3, padding=1)\n",
    "        \n",
    "        # Bilinear upsampling weights (fixed, not learned)\n",
    "        self.register_buffer('upsample_weights', \n",
    "                            bilinear_upsample_weights(upsample_factor, out_channels))\n",
    "    \n",
    "    def forward(self, x_shallow, x_deep):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_shallow: Features at current scale\n",
    "            x_deep: Features from deeper (coarser) scale\n",
    "        \"\"\"\n",
    "        # Process each stream\n",
    "        shallow = self.conv_shallow(x_shallow)\n",
    "        deep = self.conv_deep(x_deep)\n",
    "        \n",
    "        # Upsample deep features using bilinear kernel\n",
    "        deep_upsampled = F.conv_transpose2d(\n",
    "            deep, \n",
    "            self.upsample_weights,\n",
    "            stride=self.upsample_factor,\n",
    "            padding=self.upsample_factor // 2,\n",
    "            output_padding=(shallow.size(2) - deep_upsampled.size(2) if hasattr(deep, 'size') else 0,\n",
    "                           shallow.size(3) - deep_upsampled.size(3) if hasattr(deep, 'size') else 0)\n",
    "        )\n",
    "        \n",
    "        # Fuse: Add shallow detail to deep structure\n",
    "        return shallow + deep_upsampled\n",
    "\n",
    "\n",
    "class decode_rcf(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder: Combine all scales back to high resolution.\n",
    "    \n",
    "    RCF-inspired approach (Richer Convolutional Features):\n",
    "    \n",
    "    S4 (64×64) ──[F43]──> (128×128)\n",
    "                             ↓ + S3\n",
    "    S3 (128×128)─[F32]──> (256×256)\n",
    "                             ↓ + S2\n",
    "    S2 (256×256)─[F21]──> (512×512)\n",
    "                             ↓ + S1\n",
    "    Output (512×512, 1 channel)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(decode_rcf, self).__init__()\n",
    "        \n",
    "        # Each fusion combines two scales\n",
    "        self.f43 = Refine_block(in_channels_shallow=120, in_channels_deep=120,\n",
    "                               out_channels=60, upsample_factor=2)\n",
    "        self.f32 = Refine_block(in_channels_shallow=60, in_channels_deep=60,\n",
    "                               out_channels=30, upsample_factor=2)\n",
    "        self.f21 = Refine_block(in_channels_shallow=30, in_channels_deep=30,\n",
    "                               out_channels=24, upsample_factor=2)\n",
    "        \n",
    "        # Final edge map (1 channel)\n",
    "        self.final = nn.Conv2d(24, 1, kernel_size=1, padding=0)\n",
    "    \n",
    "    def forward(self, s1, s2, s3, s4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s1, s2, s3, s4: Feature maps from stages 1-4\n",
    "        \"\"\"\n",
    "        # Merge from coarse to fine\n",
    "        fused_3 = self.f43(s3, s4)\n",
    "        fused_2 = self.f32(s2, fused_3)\n",
    "        fused_1 = self.f21(s1, fused_2)\n",
    "        \n",
    "        # Output: sigmoid for edge probability [0, 1]\n",
    "        output = torch.sigmoid(self.final(fused_1))\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"✓ Decoder defined (Bilinear upsampling + RCF fusion)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80504587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Complete XYW-Net defined\n"
     ]
    }
   ],
   "source": [
    "class XYW_Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete XYW-Net architecture.\n",
    "    \n",
    "    Encoder: S1 → S2 → S3 → S4\n",
    "    Decoder: Fuse back to S1 resolution\n",
    "    Output: Single-channel edge map\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(XYW_Net, self).__init__()\n",
    "        \n",
    "        # Encoder: Four stages, each detects edges at different scales\n",
    "        self.s1 = s1(in_channels=3, channel=30)\n",
    "        self.s2 = s2(in_channels=30, out_channels=60)\n",
    "        self.s3 = s3(in_channels=60, out_channels=120)\n",
    "        self.s4 = s4(in_channels=120, out_channels=120)\n",
    "        \n",
    "        # Decoder: Combine all scales\n",
    "        self.decode = decode_rcf()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through entire network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input image (B, 3, H, W)\n",
    "        \n",
    "        Returns:\n",
    "            Edge map (B, 1, H, W) with values in [0, 1]\n",
    "        \"\"\"\n",
    "        # Encode at multiple scales\n",
    "        s1_out = self.s1(x)\n",
    "        s2_out = self.s2(s1_out)\n",
    "        s3_out = self.s3(s2_out)\n",
    "        s4_out = self.s4(s3_out)\n",
    "        \n",
    "        # Decode: Fuse scales back to full resolution\n",
    "        edge_map = self.decode(s1_out, s2_out, s3_out, s4_out)\n",
    "        \n",
    "        return edge_map\n",
    "\n",
    "print(\"✓ Complete XYW-Net defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c35e2bf",
   "metadata": {},
   "source": [
    "## Section 9: Test with a Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c2321fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XYW-Net Architecture:\n",
      "============================================================\n",
      "Total parameters: 809,053\n",
      "============================================================\n",
      "\n",
      "Forward pass with 256×256 input:\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'deep_upsampled' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 18\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdummy_input\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\imed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\imed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[6], line 38\u001b[0m, in \u001b[0;36mXYW_Net.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m s4_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms4(s3_out)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Decode: Fuse scales back to full resolution\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m edge_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms1_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms2_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms3_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms4_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m edge_map\n",
      "File \u001b[1;32mc:\\Users\\imed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\imed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[5], line 118\u001b[0m, in \u001b[0;36mdecode_rcf.forward\u001b[1;34m(self, s1, s2, s3, s4)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;124;03m    s1, s2, s3, s4: Feature maps from stages 1-4\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Merge from coarse to fine\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m fused_3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf43\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms4\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m fused_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf32(s2, fused_3)\n\u001b[0;32m    120\u001b[0m fused_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf21(s1, fused_2)\n",
      "File \u001b[1;32mc:\\Users\\imed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\imed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[5], line 76\u001b[0m, in \u001b[0;36mRefine_block.forward\u001b[1;34m(self, x_shallow, x_deep)\u001b[0m\n\u001b[0;32m     68\u001b[0m deep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_deep(x_deep)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Upsample deep features using bilinear kernel\u001b[39;00m\n\u001b[0;32m     71\u001b[0m deep_upsampled \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mconv_transpose2d(\n\u001b[0;32m     72\u001b[0m     deep, \n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample_weights,\n\u001b[0;32m     74\u001b[0m     stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample_factor,\n\u001b[0;32m     75\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample_factor \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m---> 76\u001b[0m     output_padding\u001b[38;5;241m=\u001b[39m(shallow\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[43mdeep_upsampled\u001b[49m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(deep, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     77\u001b[0m                    shallow\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m-\u001b[39m deep_upsampled\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m3\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(deep, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     78\u001b[0m )\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Fuse: Add shallow detail to deep structure\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m shallow \u001b[38;5;241m+\u001b[39m deep_upsampled\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'deep_upsampled' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = XYW_Net().to(device)\n",
    "model.eval()\n",
    "\n",
    "# Print model structure\n",
    "print(\"XYW-Net Architecture:\")\n",
    "print(\"=\" * 60)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a dummy input\n",
    "dummy_input = torch.randn(1, 3, 256, 256).to(device)\n",
    "\n",
    "print(\"\\nForward pass with 256×256 input:\")\n",
    "print(\"-\" * 60)\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "\n",
    "print(f\"Input shape:  {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output range: [{output.min():.3f}, {output.max():.3f}]\")\n",
    "print(\"-\" * 60)\n",
    "print(\"✓ Model working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8608384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_test_image():\n",
    "    \"\"\"\n",
    "    Create a synthetic test image with edges at different scales.\n",
    "    \"\"\"\n",
    "    img = np.ones((256, 256, 3), dtype=np.uint8) * 200\n",
    "    \n",
    "    # Large rectangle (medium scale edge)\n",
    "    cv2.rectangle(img, (30, 30), (200, 200), (50, 50, 50), 2)\n",
    "    \n",
    "    # Small rectangle (fine edge)\n",
    "    cv2.rectangle(img, (80, 80), (120, 120), (100, 100, 100), 1)\n",
    "    \n",
    "    # Circle (curved boundary)\n",
    "    cv2.circle(img, (150, 80), 40, (100, 100, 100), 2)\n",
    "    \n",
    "    # Diagonal lines\n",
    "    cv2.line(img, (50, 100), (200, 150), (150, 150, 150), 1)\n",
    "    \n",
    "    # Add some texture (fine details)\n",
    "    for i in range(50, 100, 5):\n",
    "        for j in range(50, 100, 5):\n",
    "            cv2.circle(img, (i, j), 2, (80, 80, 80), -1)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Create test image\n",
    "test_image = create_synthetic_test_image()\n",
    "\n",
    "# Preprocess: Convert to tensor\n",
    "img_tensor = torch.from_numpy(test_image).float().permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "img_tensor = img_tensor.to(device)\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    edge_output = model(img_tensor)\n",
    "\n",
    "# Convert outputs to numpy\n",
    "test_image_rgb = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n",
    "edge_map = edge_output[0, 0].cpu().numpy()\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original\n",
    "axes[0].imshow(test_image_rgb)\n",
    "axes[0].set_title(\"Input Image\", fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Edge map (heatmap)\n",
    "axes[1].imshow(edge_map, cmap='hot')\n",
    "axes[1].set_title(\"Edge Probability Map\", fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "cbar = plt.colorbar(axes[1].images[0], ax=axes[1])\n",
    "cbar.set_label('Confidence')\n",
    "\n",
    "# Thresholded edges\n",
    "edge_binary = (edge_map > 0.5).astype(np.uint8) * 255\n",
    "axes[2].imshow(edge_binary, cmap='gray')\n",
    "axes[2].set_title(\"Binary Edges (threshold=0.5)\", fontsize=12, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle(\"XYW-Net Edge Detection on Synthetic Image\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Test inference successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abd4d4c",
   "metadata": {},
   "source": [
    "## Section 10: Visualize Internal Features\n",
    "\n",
    "Let's inspect what each pathway (X, Y, W) learns.\n",
    "\n",
    "### Extracting Individual Pathway Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6938e83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom forward hook to capture intermediate features\n",
    "feature_maps = {}\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        feature_maps[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Hook into the first stage pathways to see X, Y, W separately\n",
    "# (This requires modifying the S1 class temporarily)\n",
    "\n",
    "# For now, let's manually run through S1 to see individual pathway outputs\n",
    "class XYW_Net_Debug(nn.Module):\n",
    "    \"\"\"Modified version that returns intermediate features\"\"\"\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(XYW_Net_Debug, self).__init__()\n",
    "        self.s1 = pretrained_model.s1\n",
    "        self.s2 = pretrained_model.s2\n",
    "        self.s3 = pretrained_model.s3\n",
    "        self.s4 = pretrained_model.s4\n",
    "        self.decode = pretrained_model.decode\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # S1: Extract individual pathway outputs\n",
    "        temp = self.s1.relu(self.s1.conv1(x))\n",
    "        x_feat, y_feat, w_feat = self.s1.xyw_s(temp)\n",
    "        \n",
    "        # Return individual pathways for visualization\n",
    "        return {\n",
    "            'x': x_feat,\n",
    "            'y': y_feat,\n",
    "            'w': w_feat,\n",
    "            'input': x,\n",
    "            'init_features': temp\n",
    "        }\n",
    "\n",
    "# Create debug model\n",
    "debug_model = XYW_Net_Debug(model).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pathways = debug_model(img_tensor)\n",
    "\n",
    "# Extract and visualize\n",
    "x_pathway = pathways['x'][0].cpu().numpy()\n",
    "y_pathway = pathways['y'][0].cpu().numpy()\n",
    "w_pathway = pathways['w'][0].cpu().numpy()\n",
    "\n",
    "# Take mean across channels for visualization\n",
    "x_visual = np.mean(np.abs(x_pathway), axis=0)\n",
    "y_visual = np.mean(np.abs(y_pathway), axis=0)\n",
    "w_visual = np.mean(np.abs(w_pathway), axis=0)\n",
    "\n",
    "# Normalize for display\n",
    "x_visual = (x_visual - x_visual.min()) / (x_visual.max() - x_visual.min() + 1e-5)\n",
    "y_visual = (y_visual - y_visual.min()) / (y_visual.max() - y_visual.min() + 1e-5)\n",
    "w_visual = (w_visual - w_visual.min()) / (w_visual.max() - w_visual.min() + 1e-5)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Top row: Original and combined\n",
    "axes[0, 0].imshow(test_image_rgb)\n",
    "axes[0, 0].set_title(\"Input Image\", fontsize=11, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(edge_map, cmap='hot')\n",
    "axes[0, 1].set_title(\"Final Edge Map\", fontsize=11, fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Bottom row: Individual pathways\n",
    "axes[1, 0].imshow(x_visual, cmap='viridis')\n",
    "axes[1, 0].set_title(\"X Pathway\\n(Small-scale details)\", fontsize=11, fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(y_visual, cmap='plasma')\n",
    "axes[1, 1].set_title(\"Y Pathway\\n(Large-scale boundaries)\", fontsize=11, fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "axes[1, 2].imshow(w_visual, cmap='coolwarm')\n",
    "axes[1, 2].set_title(\"W Pathway\\n(Oriented structures)\", fontsize=11, fontweight='bold')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "# Remove the unused subplot\n",
    "fig.delaxes(axes[0, 2])\n",
    "\n",
    "plt.suptitle(\"XYW-Net: Individual Pathway Responses\", fontsize=14, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Pathway visualizations created!\")\n",
    "print(f\"\\nPathway output statistics:\")\n",
    "print(f\"  X pathway: mean={x_visual.mean():.3f}, max={x_visual.max():.3f}\")\n",
    "print(f\"  Y pathway: mean={y_visual.mean():.3f}, max={y_visual.max():.3f}\")\n",
    "print(f\"  W pathway: mean={w_visual.mean():.3f}, max={w_visual.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5360f5",
   "metadata": {},
   "source": [
    "## Section 11: Architecture Diagram\n",
    "\n",
    "Let's visualize the complete information flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55684feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Title\n",
    "fig.text(0.5, 0.98, 'XYW-Net Complete Architecture', \n",
    "         ha='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Create subplots\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.4, wspace=0.3)\n",
    "\n",
    "# ========== ENCODER ==========\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 3)\n",
    "ax1.axis('off')\n",
    "\n",
    "ax1.text(5, 2.7, 'ENCODER: Multi-Scale Feature Extraction', \n",
    "         ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# S1\n",
    "rect_s1 = FancyBboxPatch((0.2, 0.5), 1.8, 1.5, boxstyle=\"round,pad=0.05\", \n",
    "                         edgecolor='#1f77b4', facecolor='#D6E8F7', linewidth=2)\n",
    "ax1.add_patch(rect_s1)\n",
    "ax1.text(1.1, 1.8, 'S1', ha='center', fontsize=10, fontweight='bold')\n",
    "ax1.text(1.1, 1.4, '512×512\\n30ch', ha='center', fontsize=9)\n",
    "\n",
    "# S2\n",
    "rect_s2 = FancyBboxPatch((2.3, 0.5), 1.8, 1.5, boxstyle=\"round,pad=0.05\",\n",
    "                         edgecolor='#ff7f0e', facecolor='#FFE6CC', linewidth=2)\n",
    "ax1.add_patch(rect_s2)\n",
    "ax1.text(3.2, 1.8, 'S2', ha='center', fontsize=10, fontweight='bold')\n",
    "ax1.text(3.2, 1.4, '256×256\\n60ch', ha='center', fontsize=9)\n",
    "\n",
    "# S3\n",
    "rect_s3 = FancyBboxPatch((4.4, 0.5), 1.8, 1.5, boxstyle=\"round,pad=0.05\",\n",
    "                         edgecolor='#2ca02c', facecolor='#D6F5D6', linewidth=2)\n",
    "ax1.add_patch(rect_s3)\n",
    "ax1.text(5.3, 1.8, 'S3', ha='center', fontsize=10, fontweight='bold')\n",
    "ax1.text(5.3, 1.4, '128×128\\n120ch', ha='center', fontsize=9)\n",
    "\n",
    "# S4\n",
    "rect_s4 = FancyBboxPatch((6.5, 0.5), 1.8, 1.5, boxstyle=\"round,pad=0.05\",\n",
    "                         edgecolor='#d62728', facecolor='#F5D6D6', linewidth=2)\n",
    "ax1.add_patch(rect_s4)\n",
    "ax1.text(7.4, 1.8, 'S4', ha='center', fontsize=10, fontweight='bold')\n",
    "ax1.text(7.4, 1.4, '64×64\\n120ch', ha='center', fontsize=9)\n",
    "\n",
    "# Arrows between stages\n",
    "ax1.arrow(2.1, 1.25, 0.15, 0, head_width=0.15, head_length=0.05, fc='black', ec='black')\n",
    "ax1.arrow(3.3, 1.25, 0.15, 0, head_width=0.15, head_length=0.05, fc='black', ec='black')\n",
    "ax1.arrow(4.4, 1.25, 0.15, 0, head_width=0.15, head_length=0.05, fc='black', ec='black')\n",
    "\n",
    "# ========== XYW PATHWAYS ==========\n",
    "ax2 = fig.add_subplot(gs[1, :])\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 4)\n",
    "ax2.axis('off')\n",
    "\n",
    "ax2.text(5, 3.8, 'PATHWAYS: Each Stage Splits into 3 Streams', \n",
    "         ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# X pathway\n",
    "rect_x = FancyBboxPatch((0.5, 1.5), 2, 1.8, boxstyle=\"round,pad=0.05\",\n",
    "                        edgecolor='#1f77b4', facecolor='#E6F2FF', linewidth=2)\n",
    "ax2.add_patch(rect_x)\n",
    "ax2.text(1.5, 3, 'X Pathway', ha='center', fontsize=10, fontweight='bold', color='#1f77b4')\n",
    "ax2.text(1.5, 2.5, 'Small Scale', ha='center', fontsize=9)\n",
    "ax2.text(1.5, 2.1, 'Center (1×1) -', ha='center', fontsize=8)\n",
    "ax2.text(1.5, 1.8, 'Surround (3×3)', ha='center', fontsize=8)\n",
    "\n",
    "# Y pathway\n",
    "rect_y = FancyBboxPatch((3.5, 1.5), 2, 1.8, boxstyle=\"round,pad=0.05\",\n",
    "                        edgecolor='#ff7f0e', facecolor='#FFF4E6', linewidth=2)\n",
    "ax2.add_patch(rect_y)\n",
    "ax2.text(4.5, 3, 'Y Pathway', ha='center', fontsize=10, fontweight='bold', color='#ff7f0e')\n",
    "ax2.text(4.5, 2.5, 'Large Scale', ha='center', fontsize=9)\n",
    "ax2.text(4.5, 2.1, 'Center (1×1) -', ha='center', fontsize=8)\n",
    "ax2.text(4.5, 1.8, 'Surround (5×5, d=2)', ha='center', fontsize=8)\n",
    "\n",
    "# W pathway\n",
    "rect_w = FancyBboxPatch((6.5, 1.5), 2, 1.8, boxstyle=\"round,pad=0.05\",\n",
    "                        edgecolor='#2ca02c', facecolor='#E6FFE6', linewidth=2)\n",
    "ax2.add_patch(rect_w)\n",
    "ax2.text(7.5, 3, 'W Pathway', ha='center', fontsize=10, fontweight='bold', color='#2ca02c')\n",
    "ax2.text(7.5, 2.5, 'Directional', ha='center', fontsize=9)\n",
    "ax2.text(7.5, 2.1, 'H (1×3) +', ha='center', fontsize=8)\n",
    "ax2.text(7.5, 1.8, 'V (3×1)', ha='center', fontsize=8)\n",
    "\n",
    "# Merge arrows\n",
    "ax2.arrow(1.5, 1.4, 0.7, -0.6, head_width=0.1, head_length=0.1, fc='black', ec='black', alpha=0.5)\n",
    "ax2.arrow(4.5, 1.4, 0, -0.6, head_width=0.1, head_length=0.1, fc='black', ec='black', alpha=0.5)\n",
    "ax2.arrow(7.5, 1.4, -0.7, -0.6, head_width=0.1, head_length=0.1, fc='black', ec='black', alpha=0.5)\n",
    "\n",
    "# Merge box\n",
    "rect_merge = FancyBboxPatch((3.5, 0.1), 3, 0.7, boxstyle=\"round,pad=0.05\",\n",
    "                           edgecolor='black', facecolor='#FFFFE6', linewidth=2)\n",
    "ax2.add_patch(rect_merge)\n",
    "ax2.text(5, 0.45, 'Merge: X + Y + W', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# ========== DECODER ==========\n",
    "ax3 = fig.add_subplot(gs[2, :])\n",
    "ax3.set_xlim(0, 10)\n",
    "ax3.set_ylim(0, 3.5)\n",
    "ax3.axis('off')\n",
    "\n",
    "ax3.text(5, 3.2, 'DECODER: Hierarchical Feature Fusion', \n",
    "         ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Fusion blocks\n",
    "positions = [(1, 'S4\\n64×64'), (3, 'F43\\n→128×128'), (5, 'F32\\n→256×256'), (7, 'F21\\n→512×512'), (9, 'Output\\n1×512×512')]\n",
    "colors = ['#F5D6D6', '#FFE6CC', '#D6F5D6', '#D6E8F7', '#F5F5F5']\n",
    "\n",
    "for i, (pos, label) in enumerate(positions):\n",
    "    rect = FancyBboxPatch((pos-0.6, 1.5), 1.2, 1.2, boxstyle=\"round,pad=0.05\",\n",
    "                         edgecolor='#333333', facecolor=colors[i], linewidth=1.5)\n",
    "    ax3.add_patch(rect)\n",
    "    ax3.text(pos, 2.1, label, ha='center', fontsize=8, fontweight='bold')\n",
    "    \n",
    "    if i < len(positions) - 1:\n",
    "        ax3.arrow(pos + 0.65, 2.1, 0.65, 0, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "ax3.text(5, 0.7, 'Each Fij combines scales: Upsample(deeper) + shallow features', \n",
    "         ha='center', fontsize=9, style='italic')\n",
    "ax3.text(5, 0.3, 'Final 1×1 Conv → Single-channel edge map [0, 1]', \n",
    "         ha='center', fontsize=9, style='italic')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Architecture diagram complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d02e7a1",
   "metadata": {},
   "source": [
    "## Section 12: Summary and Key Takeaways\n",
    "\n",
    "### The Three Biological Principles\n",
    "\n",
    "| Principle | Implementation | Biological Inspiration |\n",
    "|-----------|-----------------|----------------------|\n",
    "| **Small-scale detection** | X pathway (1×1 vs 3×3) | ON/OFF retinal cells with small RF |\n",
    "| **Large-scale detection** | Y pathway (1×1 vs 5×5 dilated) | Magnocellular retinal pathway |\n",
    "| **Orientation selectivity** | W pathway (1×3, 3×1) | Simple cells in V1 cortex |\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "1. **Biological plausibility**: Mirrors actual brain structures\n",
    "2. **Multi-scale**: Captures edges at different zoom levels (fine texture → object boundaries)\n",
    "3. **Efficient**: Depthwise convolutions reduce parameters\n",
    "4. **Complementary**: X catches texture, Y catches structure, W catches orientation\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "All pathways are **difference-of-Gaussians (DoG)** in spirit:\n",
    "$$\\text{Response} = \\text{Surround}_{\\sigma_2} - \\text{Center}_{\\sigma_1}$$\n",
    "\n",
    "where $\\sigma_1 < \\sigma_2$ (or center is sharper than surround).\n",
    "\n",
    "### Training Strategy\n",
    "\n",
    "XYW-Net is typically trained with:\n",
    "- **Loss**: Binary cross-entropy on thresholded edges\n",
    "- **Dataset**: BSDS500, NYUD, or similar edge-annotated datasets\n",
    "- **Optimizer**: Adam with learning rate decay\n",
    "- **Augmentation**: Flips, rotations, scale variations\n",
    "\n",
    "### Performance\n",
    "\n",
    "When trained properly, XYW-Net achieves:\n",
    "- **BSDS500**: F-measure ≈ 0.80\n",
    "- **NYUD**: F-measure ≈ 0.74\n",
    "- Comparable or better than HED, RCF for many scenes\n",
    "\n",
    "---\n",
    "\n",
    "## Code is Ready for Local Use\n",
    "\n",
    "Save this notebook and run locally with:\n",
    "```python\n",
    "python -c \"jupyter notebook XYW_Net_Explained.ipynb\"\n",
    "```\n",
    "\n",
    "All imports are standard (torch, cv2, numpy, matplotlib) - no external dependencies needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
