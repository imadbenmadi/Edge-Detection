{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83d26a5c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Pathway | What it does | Implementation |\n",
    "|---------|--------------|----------------|\n",
    "| **X** | Local contrast (center-surround) | `3×3 Conv` - `1×1 Conv` |\n",
    "| **Y** | Large RF contrast | `5×5 Dilated Conv` - `1×1 Conv` |\n",
    "| **W** | Directional | `1×3 Conv` (horizontal) + `3×1 Conv` (vertical) |\n",
    "\n",
    "**W pathway is NOT Gabor filters** - it's separable directional convolutions.\n",
    "\n",
    "**Gabor filters** are specifically oriented edge detectors with sinusoidal patterns at different angles (0°, 45°, 90°, 135°, etc.). They're biologically inspired by V1 cortex cells.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ab8640a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce GTX 1070\n",
      "Memory: 8.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1746f5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying split: train\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking train:  36%|███▋      | 3720/10200 [08:50<15:23,  7.01img/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 79\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# RUN FOR ALL SPLITS\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[0;32m     77\u001b[0m DATA_ROOT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mEdge Detection\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 79\u001b[0m \u001b[43mverify_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_ROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m verify_edges(DATA_ROOT, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     81\u001b[0m verify_edges(DATA_ROOT, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 31\u001b[0m, in \u001b[0;36mverify_edges\u001b[1;34m(root_dir, split)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 2. Try loading edge\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m edge \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43medge_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMREAD_GRAYSCALE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     unreadable_edges\u001b[38;5;241m.\u001b[39mappend(img_path\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def verify_edges(root_dir, split=\"train\"):\n",
    "    root_dir = Path(root_dir)\n",
    "    img_dir = root_dir / split / \"images\"\n",
    "    edge_dir = root_dir / split / \"edges\"\n",
    "\n",
    "    image_files = sorted(list(img_dir.glob(\"*.png\")))\n",
    "\n",
    "    missing_edges = []\n",
    "    unreadable_edges = []\n",
    "    empty_edges = []\n",
    "    size_mismatch = []\n",
    "\n",
    "    print(f\"\\nVerifying split: {split}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for img_path in tqdm(image_files, desc=f\"Checking {split}\", unit=\"img\"):\n",
    "        edge_path = edge_dir / img_path.name\n",
    "\n",
    "        # 1. Check if edge file exists\n",
    "        if not edge_path.exists():\n",
    "            missing_edges.append(img_path.name)\n",
    "            continue\n",
    "\n",
    "        # 2. Try loading edge\n",
    "        edge = cv2.imread(str(edge_path), cv2.IMREAD_GRAYSCALE)\n",
    "        if edge is None:\n",
    "            unreadable_edges.append(img_path.name)\n",
    "            continue\n",
    "\n",
    "        # 3. Check if edge is empty (all zeros)\n",
    "        if np.sum(edge) == 0:\n",
    "            empty_edges.append(img_path.name)\n",
    "\n",
    "        # 4. Check size match\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None or edge.shape != img.shape[:2]:\n",
    "            size_mismatch.append(img_path.name)\n",
    "\n",
    "    # ===== REPORT =====\n",
    "    print(f\"\\nVerification Report for split: {split}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total images checked: {len(image_files)}\")\n",
    "    print(f\"Missing edge files: {len(missing_edges)}\")\n",
    "    print(f\"Unreadable edge files: {len(unreadable_edges)}\")\n",
    "    print(f\"Empty edge maps: {len(empty_edges)}\")\n",
    "    print(f\"Size mismatches: {len(size_mismatch)}\")\n",
    "\n",
    "    if missing_edges:\n",
    "        print(\"\\nMissing edge files (first 10):\")\n",
    "        print(missing_edges[:10])\n",
    "\n",
    "    if unreadable_edges:\n",
    "        print(\"\\nUnreadable edge files (first 10):\")\n",
    "        print(unreadable_edges[:10])\n",
    "\n",
    "    if empty_edges:\n",
    "        print(\"\\nEmpty edge maps (first 10):\")\n",
    "        print(empty_edges[:10])\n",
    "\n",
    "    if size_mismatch:\n",
    "        print(\"\\nSize mismatches (first 10):\")\n",
    "        print(size_mismatch[:10])\n",
    "\n",
    "    if not (missing_edges or unreadable_edges or empty_edges or size_mismatch):\n",
    "        print(\"\\nAll images have valid, non-empty, correctly-sized edge maps.\")\n",
    "\n",
    "# =========================\n",
    "# RUN FOR ALL SPLITS\n",
    "# =========================\n",
    "\n",
    "DATA_ROOT = r\"e:\\Edge Detection\\datasets\\processed_HED_v2\"\n",
    "\n",
    "verify_edges(DATA_ROOT, \"train\")\n",
    "verify_edges(DATA_ROOT, \"val\")\n",
    "verify_edges(DATA_ROOT, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85adc490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 9600 samples\n",
      "Val: 0 samples\n",
      "Test: 200 samples\n",
      "\n",
      "Sample shapes - Image: torch.Size([3, 512, 512]), Edge: torch.Size([1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Dataset Loader\n",
    "class ProcessedDataset(Dataset):\n",
    "    \"\"\"Load processed PNG images and edge maps\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, split='train'):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.split = split\n",
    "        self.img_dir = self.root_dir / split / 'images'\n",
    "        self.edge_dir = self.root_dir / split / 'edges'\n",
    "        self.samples = sorted(list(self.img_dir.glob('*.png')))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.samples[idx]\n",
    "        edge_path = self.edge_dir / img_path.name\n",
    "        \n",
    "        # Load image (BGR -> RGB)\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Load edge map\n",
    "        edge = cv2.imread(str(edge_path), cv2.IMREAD_GRAYSCALE)\n",
    "        edge = edge.astype(np.float32) / 255.0\n",
    "        \n",
    "        # To tensors\n",
    "        img = torch.from_numpy(img).permute(2, 0, 1)  # (C, H, W)\n",
    "        edge = torch.from_numpy(edge).unsqueeze(0)    # (1, H, W)\n",
    "        \n",
    "        return {'images': img, 'labels': edge, 'filename': img_path.stem}\n",
    "\n",
    "# Load datasets\n",
    "DATA_ROOT = r'e:\\Edge Detection\\datasets\\processed_HED_v2'\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_dataset = ProcessedDataset(DATA_ROOT, split='train')\n",
    "val_dataset = ProcessedDataset(DATA_ROOT, split='val')\n",
    "test_dataset = ProcessedDataset(DATA_ROOT, split='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} samples\")\n",
    "print(f\"Val: {len(val_dataset)} samples\")\n",
    "print(f\"Test: {len(test_dataset)} samples\")\n",
    "\n",
    "# Verify sample\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample shapes - Image: {sample['images'].shape}, Edge: {sample['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f812c784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created with 890,275 parameters\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: XYW-Net Model (Complete)\n",
    "\n",
    "# ============ PDC Convolution ============\n",
    "def createPDCFunc(PDC_type):\n",
    "    \"\"\"Create Pixel Difference Convolution function\"\"\"\n",
    "    assert PDC_type in ['cv', 'cd', 'ad', 'rd', 'sd', 'p2d', '2sd', '2cd']\n",
    "    \n",
    "    if PDC_type == 'cv':\n",
    "        return F.conv2d\n",
    "    \n",
    "    if PDC_type == '2sd':\n",
    "        def func(x, weights, bias=None, stride=1, padding=0, dilation=1, groups=1):\n",
    "            assert weights.size(2) == 3 and weights.size(3) == 3\n",
    "            shape = weights.shape\n",
    "            if groups == shape[0]:\n",
    "                weights_conv = (weights - weights[:, :, [1, 1, 1, 0, 0, 0, 2, 2, 2], [0, 1, 2, 0, 1, 2, 0, 1, 2]].view(shape))\n",
    "            else:\n",
    "                weights_conv = (weights - weights[:, :, [1, 1, 1, 0, 0, 0, 2, 2, 2], [0, 1, 2, 0, 1, 2, 0, 1, 2]].view(shape).flip(0))\n",
    "            y = F.conv2d(x, weights_conv, bias, stride=stride, padding=padding, dilation=dilation, groups=groups)\n",
    "            return y\n",
    "        return func\n",
    "    \n",
    "    return F.conv2d\n",
    "\n",
    "class Conv2d(nn.Module):\n",
    "    \"\"\"PDC-enabled Conv2d\"\"\"\n",
    "    def __init__(self, pdc_func='cv', in_channels=1, out_channels=1, kernel_size=3, \n",
    "                 stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "        super(Conv2d, self).__init__()\n",
    "        self.pdc = createPDCFunc(pdc_func)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels // groups, kernel_size, kernel_size))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pdc(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "# ============ Core XYW Components ============\n",
    "class Xc1x1(nn.Module):\n",
    "    \"\"\"X pathway: Local contrast (center-surround with 3x3)\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Xc1x1, self).__init__()\n",
    "        self.Xcenter = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.Xcenter_relu = nn.ReLU(inplace=True)\n",
    "        self.Xsurround = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, groups=in_channels)\n",
    "        self.conv1_1 = nn.Conv2d(out_channels, out_channels, kernel_size=1)\n",
    "        self.Xsurround_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        xcenter = self.Xcenter_relu(self.Xcenter(input))\n",
    "        xsurround = self.Xsurround_relu(self.Xsurround(input))\n",
    "        xsurround = self.conv1_1(xsurround)\n",
    "        return xsurround - xcenter\n",
    "\n",
    "class Yc1x1(nn.Module):\n",
    "    \"\"\"Y pathway: Large receptive field (center-surround with 5x5 dilated)\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Yc1x1, self).__init__()\n",
    "        self.Ycenter = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.Ycenter_relu = nn.ReLU(inplace=True)\n",
    "        self.Ysurround = nn.Conv2d(in_channels, out_channels, kernel_size=5, padding=4, dilation=2, groups=in_channels)\n",
    "        self.conv1_1 = nn.Conv2d(out_channels, out_channels, kernel_size=1)\n",
    "        self.Ysurround_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        ycenter = self.Ycenter_relu(self.Ycenter(input))\n",
    "        ysurround = self.Ysurround_relu(self.Ysurround(input))\n",
    "        ysurround = self.conv1_1(ysurround)\n",
    "        return ysurround - ycenter\n",
    "\n",
    "class W(nn.Module):\n",
    "    \"\"\"W pathway: Directional (horizontal + vertical)\"\"\"\n",
    "    def __init__(self, inchannel, outchannel):\n",
    "        super(W, self).__init__()\n",
    "        self.h = nn.Conv2d(inchannel, inchannel, kernel_size=(1, 3), padding=(0, 1), groups=inchannel)\n",
    "        self.v = nn.Conv2d(inchannel, inchannel, kernel_size=(3, 1), padding=(1, 0), groups=inchannel)\n",
    "        self.convh_1 = nn.Conv2d(inchannel, inchannel, kernel_size=1, bias=False)\n",
    "        self.convv_1 = nn.Conv2d(inchannel, outchannel, kernel_size=1, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.h(x))\n",
    "        h = self.convh_1(h)\n",
    "        v = self.relu(self.v(h))\n",
    "        v = self.convv_1(v)\n",
    "        return v\n",
    "\n",
    "# ============ XYW Blocks ============\n",
    "class XYW_S(nn.Module):\n",
    "    \"\"\"XYW Start block\"\"\"\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(XYW_S, self).__init__()\n",
    "        self.y_c = Yc1x1(inchannel, outchannel)\n",
    "        self.x_c = Xc1x1(inchannel, outchannel)\n",
    "        self.w = W(inchannel, outchannel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.x_c(x), self.y_c(x), self.w(x)\n",
    "\n",
    "class XYW(nn.Module):\n",
    "    \"\"\"XYW middle block\"\"\"\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(XYW, self).__init__()\n",
    "        self.y_c = Yc1x1(inchannel, outchannel)\n",
    "        self.x_c = Xc1x1(inchannel, outchannel)\n",
    "        self.w = W(inchannel, outchannel)\n",
    "\n",
    "    def forward(self, xc, yc, w):\n",
    "        return self.x_c(xc), self.y_c(yc), self.w(w)\n",
    "\n",
    "class XYW_E(nn.Module):\n",
    "    \"\"\"XYW End block (combines X+Y+W)\"\"\"\n",
    "    def __init__(self, inchannel, outchannel):\n",
    "        super(XYW_E, self).__init__()\n",
    "        self.y_c = Yc1x1(inchannel, outchannel)\n",
    "        self.x_c = Xc1x1(inchannel, outchannel)\n",
    "        self.w = W(inchannel, outchannel)\n",
    "\n",
    "    def forward(self, xc, yc, w):\n",
    "        return self.x_c(xc) + self.y_c(yc) + self.w(w)\n",
    "\n",
    "# ============ Encoder Stages ============\n",
    "class s1(nn.Module):\n",
    "    def __init__(self, channel=30):\n",
    "        super(s1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, channel, kernel_size=7, padding=6, dilation=2)\n",
    "        self.xyw1_1 = XYW_S(channel, channel)\n",
    "        self.xyw1_2 = XYW(channel, channel)\n",
    "        self.xyw1_3 = XYW_E(channel, channel)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        temp = self.relu(self.conv1(x))\n",
    "        xc, yc, w = self.xyw1_1(temp)\n",
    "        xc, yc, w = self.xyw1_2(xc, yc, w)\n",
    "        xyw1_3 = self.xyw1_3(xc, yc, w)\n",
    "        return xyw1_3 + temp\n",
    "\n",
    "class s2(nn.Module):\n",
    "    def __init__(self, channel=60):\n",
    "        super(s2, self).__init__()\n",
    "        self.xyw2_1 = XYW_S(channel//2, channel, stride=2)\n",
    "        self.xyw2_2 = XYW(channel, channel)\n",
    "        self.xyw2_3 = XYW_E(channel, channel)\n",
    "        self.shortcut = nn.Conv2d(channel//2, channel, kernel_size=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        xc, yc, w = self.xyw2_1(x)\n",
    "        xc, yc, w = self.xyw2_2(xc, yc, w)\n",
    "        xyw2_3 = self.xyw2_3(xc, yc, w)\n",
    "        return xyw2_3 + self.shortcut(x)\n",
    "\n",
    "class s3(nn.Module):\n",
    "    def __init__(self, channel=120):\n",
    "        super(s3, self).__init__()\n",
    "        self.xyw3_1 = XYW_S(channel//2, channel, stride=2)\n",
    "        self.xyw3_2 = XYW(channel, channel)\n",
    "        self.xyw3_3 = XYW_E(channel, channel)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.shortcut = nn.Conv2d(channel//2, channel, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        shortcut = self.shortcut(x)\n",
    "        xc, yc, w = self.xyw3_1(x)\n",
    "        xc, yc, w = self.xyw3_2(xc, yc, w)\n",
    "        xyw3_3 = self.xyw3_3(xc, yc, w)\n",
    "        return xyw3_3 + shortcut\n",
    "\n",
    "class s4(nn.Module):\n",
    "    def __init__(self, channel=120):\n",
    "        super(s4, self).__init__()\n",
    "        self.xyw4_1 = XYW_S(channel, channel, stride=2)\n",
    "        self.xyw4_2 = XYW(channel, channel)\n",
    "        self.xyw4_3 = XYW_E(channel, channel)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.shortcut = nn.Conv2d(channel, channel, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        shortcut = self.shortcut(x)\n",
    "        xc, yc, w = self.xyw4_1(x)\n",
    "        xc, yc, w = self.xyw4_2(xc, yc, w)\n",
    "        xyw4_3 = self.xyw4_3(xc, yc, w)\n",
    "        return xyw4_3 + shortcut\n",
    "\n",
    "# ============ Encoder ============\n",
    "class encode(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(encode, self).__init__()\n",
    "        self.s1 = s1()\n",
    "        self.s2 = s2()\n",
    "        self.s3 = s3()\n",
    "        self.s4 = s4()\n",
    "\n",
    "    def forward(self, x):\n",
    "        s1_out = self.s1(x)\n",
    "        s2_out = self.s2(s1_out)\n",
    "        s3_out = self.s3(s2_out)\n",
    "        s4_out = self.s4(s3_out)\n",
    "        return s1_out, s2_out, s3_out, s4_out\n",
    "\n",
    "# ============ Adaptive Convolution ============\n",
    "def upsample_filt(size):\n",
    "    factor = (size + 1) // 2\n",
    "    center = factor - 1 if size % 2 == 1 else factor - 0.5\n",
    "    og = np.ogrid[:size, :size]\n",
    "    return (1 - abs(og[0] - center) / factor) * (1 - abs(og[1] - center) / factor)\n",
    "\n",
    "def bilinear_upsample_weights(factor, num_classes):\n",
    "    filter_size = 2 * factor - factor % 2\n",
    "    weights = np.zeros((num_classes, num_classes, filter_size, filter_size), dtype=np.float32)\n",
    "    upsample_kernel = upsample_filt(filter_size)\n",
    "    for i in range(num_classes):\n",
    "        weights[i, i, :, :] = upsample_kernel\n",
    "    return torch.Tensor(weights)\n",
    "\n",
    "class adap_conv(nn.Module):\n",
    "    \"\"\"Adaptive convolution with learnable weight\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kz=3, pd=1):\n",
    "        super(adap_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            Conv2d(pdc_func='2sd', in_channels=in_channels, out_channels=out_channels, kernel_size=kz, padding=pd),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.weight = nn.Parameter(torch.Tensor([0.]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x) * self.weight.sigmoid()\n",
    "\n",
    "class Refine_block2_1(nn.Module):\n",
    "    \"\"\"Refinement block for decoder\"\"\"\n",
    "    def __init__(self, in_channel, out_channel, factor, require_grad=False):\n",
    "        super(Refine_block2_1, self).__init__()\n",
    "        self.pre_conv1 = adap_conv(in_channel[0], out_channel, kz=3, pd=1)\n",
    "        self.pre_conv2 = adap_conv(in_channel[1], out_channel, kz=3, pd=1)\n",
    "        self.factor = factor\n",
    "        self.deconv_weight = nn.Parameter(bilinear_upsample_weights(factor, out_channel), requires_grad=require_grad)\n",
    "\n",
    "    def forward(self, *input):\n",
    "        x1 = self.pre_conv1(input[0])\n",
    "        x2 = self.pre_conv2(input[1])\n",
    "        x2 = F.conv_transpose2d(x2, self.deconv_weight, stride=self.factor, \n",
    "                                padding=int(self.factor/2),\n",
    "                                output_padding=(x1.size(2) - x2.size(2)*self.factor, \n",
    "                                               x1.size(3) - x2.size(3)*self.factor))\n",
    "        return x1 + x2\n",
    "\n",
    "# ============ RCF Decoder ============\n",
    "class decode_rcf(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(decode_rcf, self).__init__()\n",
    "        self.f43 = Refine_block2_1(in_channel=(120, 120), out_channel=60, factor=2)\n",
    "        self.f32 = Refine_block2_1(in_channel=(60, 60), out_channel=30, factor=2)\n",
    "        self.f21 = Refine_block2_1(in_channel=(30, 30), out_channel=24, factor=2)\n",
    "        self.f = nn.Conv2d(24, 1, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        s3 = self.f43(x[2], x[3])\n",
    "        s2 = self.f32(x[1], s3)\n",
    "        s1 = self.f21(x[0], s2)\n",
    "        out = self.f(s1)\n",
    "        return out.sigmoid()\n",
    "\n",
    "# ============ Full XYW-Net ============\n",
    "class XYWNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XYWNet, self).__init__()\n",
    "        self.encode = encode()\n",
    "        self.decode = decode_rcf()\n",
    "\n",
    "    def forward(self, x):\n",
    "        endpoints = self.encode(x)\n",
    "        out = self.decode(endpoints)\n",
    "        return out\n",
    "    \n",
    "    def forward_with_stages(self, x):\n",
    "        \"\"\"Forward pass returning intermediate stage outputs for visualization\"\"\"\n",
    "        s1, s2, s3, s4 = self.encode(x)\n",
    "        final = self.decode((s1, s2, s3, s4))\n",
    "        return final, (s1, s2, s3, s4)\n",
    "\n",
    "# Create model\n",
    "model = XYWNet().to(device)\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac73f4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function and metrics defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Loss Function and Metrics\n",
    "\n",
    "class EdgeLoss(nn.Module):\n",
    "    \"\"\"Weighted cross-entropy loss for edge detection\"\"\"\n",
    "    def __init__(self):\n",
    "        super(EdgeLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, pred, label):\n",
    "        pred_flat = pred.view(-1)\n",
    "        label_flat = label.view(-1)\n",
    "        eps = 1e-6\n",
    "        \n",
    "        # Positive and negative pixels\n",
    "        pos_mask = label_flat > 0\n",
    "        neg_mask = label_flat == 0\n",
    "        \n",
    "        pred_pos = pred_flat[pos_mask].clamp(eps, 1.0 - eps)\n",
    "        pred_neg = pred_flat[neg_mask].clamp(eps, 1.0 - eps)\n",
    "        \n",
    "        # Weighted by annotation strength\n",
    "        w_pos = label_flat[pos_mask]\n",
    "        \n",
    "        if len(pred_pos) > 0 and len(pred_neg) > 0:\n",
    "            loss = (-pred_pos.log() * w_pos).mean() + (-(1.0 - pred_neg).log()).mean()\n",
    "        elif len(pred_pos) > 0:\n",
    "            loss = (-pred_pos.log() * w_pos).mean()\n",
    "        else:\n",
    "            loss = (-(1.0 - pred_neg).log()).mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "def compute_ods_ois_ap(preds, labels, thresholds=99):\n",
    "    \"\"\"\n",
    "    Compute ODS (Optimal Dataset Scale), OIS (Optimal Image Scale), and AP.\n",
    "    \n",
    "    ODS: Best F-score using a single threshold for all images\n",
    "    OIS: Average of best F-score per image\n",
    "    AP: Average Precision\n",
    "    \"\"\"\n",
    "    threshs = np.linspace(0.01, 0.99, thresholds)\n",
    "    \n",
    "    # For ODS (global)\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # For OIS (per-image best)\n",
    "    ois_f1_scores = []\n",
    "    \n",
    "    for pred, label in zip(preds, labels):\n",
    "        pred_np = pred.flatten()\n",
    "        label_np = (label.flatten() > 0.5).astype(np.float32)\n",
    "        \n",
    "        all_preds.append(pred_np)\n",
    "        all_labels.append(label_np)\n",
    "        \n",
    "        # Per-image best F1\n",
    "        best_f1 = 0\n",
    "        for t in threshs:\n",
    "            pred_bin = (pred_np >= t).astype(np.float32)\n",
    "            tp = np.sum(pred_bin * label_np)\n",
    "            fp = np.sum(pred_bin * (1 - label_np))\n",
    "            fn = np.sum((1 - pred_bin) * label_np)\n",
    "            \n",
    "            precision = tp / (tp + fp + 1e-8)\n",
    "            recall = tp / (tp + fn + 1e-8)\n",
    "            f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "            best_f1 = max(best_f1, f1)\n",
    "        \n",
    "        ois_f1_scores.append(best_f1)\n",
    "    \n",
    "    # Concatenate all for global metrics\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    # ODS: Best global threshold\n",
    "    best_ods = 0\n",
    "    for t in threshs:\n",
    "        pred_bin = (all_preds >= t).astype(np.float32)\n",
    "        tp = np.sum(pred_bin * all_labels)\n",
    "        fp = np.sum(pred_bin * (1 - all_labels))\n",
    "        fn = np.sum((1 - pred_bin) * all_labels)\n",
    "        \n",
    "        precision = tp / (tp + fp + 1e-8)\n",
    "        recall = tp / (tp + fn + 1e-8)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "        best_ods = max(best_ods, f1)\n",
    "    \n",
    "    # OIS: Average of per-image best\n",
    "    ois = np.mean(ois_f1_scores)\n",
    "    \n",
    "    # AP: Average Precision\n",
    "    try:\n",
    "        ap = average_precision_score(all_labels, all_preds)\n",
    "    except:\n",
    "        ap = 0.0\n",
    "    \n",
    "    return best_ods, ois, ap\n",
    "\n",
    "print(\"Loss function and metrics defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "044dd9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Training Function\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for batch in pbar:\n",
    "        images = batch['images'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(loader, desc='Evaluating'):\n",
    "        images = batch['images'].to(device)\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        for i in range(outputs.shape[0]):\n",
    "            all_preds.append(outputs[i, 0].cpu().numpy())\n",
    "            all_labels.append(labels[i, 0].numpy())\n",
    "    \n",
    "    ods, ois, ap = compute_ods_ois_ap(all_preds, all_labels)\n",
    "    return ods, ois, ap\n",
    "\n",
    "print(\"Training functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bc00dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Train XYW-Net\n",
    "\n",
    "# Hyperparameters\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# Initialize\n",
    "model = XYWNet().to(device)\n",
    "criterion = EdgeLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# Training history\n",
    "history = {'train_loss': [], 'val_ods': [], 'val_ois': [], 'val_ap': []}\n",
    "best_ods = 0\n",
    "\n",
    "print(f\"Training XYW-Net for {NUM_EPOCHS} epochs...\")\n",
    "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    \n",
    "    # Evaluate on validation\n",
    "    val_ods, val_ois, val_ap = evaluate(model, val_loader, device)\n",
    "    history['val_ods'].append(val_ods)\n",
    "    history['val_ois'].append(val_ois)\n",
    "    history['val_ap'].append(val_ap)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Loss: {train_loss:.4f} | ODS: {val_ods:.4f} | OIS: {val_ois:.4f} | AP: {val_ap:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_ods > best_ods:\n",
    "        best_ods = val_ods\n",
    "        torch.save(model.state_dict(), 'best_xyw_net.pth')\n",
    "        print(f\"  -> Saved best model (ODS: {best_ods:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Training complete. Best ODS: {best_ods:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d8b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Plot Training History\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], 'b-', linewidth=2, label='Train Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Metrics\n",
    "axes[1].plot(history['val_ods'], 'g-', linewidth=2, label='ODS')\n",
    "axes[1].plot(history['val_ois'], 'b-', linewidth=2, label='OIS')\n",
    "axes[1].plot(history['val_ap'], 'r-', linewidth=2, label='AP')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Validation Metrics')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb43ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Final Test Evaluation\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_xyw_net.pth'))\n",
    "print(\"Loaded best model weights.\\n\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating on TEST set...\")\n",
    "test_ods, test_ois, test_ap = evaluate(model, test_loader, device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL TEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  ODS (Optimal Dataset Scale): {test_ods:.4f}\")\n",
    "print(f\"  OIS (Optimal Image Scale):   {test_ois:.4f}\")\n",
    "print(f\"  AP (Average Precision):      {test_ap:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ff6571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Visualize Edge Predictions\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_predictions(model, dataset, device, num_samples=6):\n",
    "    \"\"\"Visualize predictions on random samples\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        sample = dataset[idx]\n",
    "        img = sample['images'].unsqueeze(0).to(device)\n",
    "        label = sample['labels'][0].numpy()\n",
    "        \n",
    "        pred = model(img)[0, 0].cpu().numpy()\n",
    "        \n",
    "        # Original image\n",
    "        axes[i, 0].imshow(sample['images'].permute(1, 2, 0).numpy())\n",
    "        axes[i, 0].set_title(f'Input: {sample[\"filename\"]}')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Ground truth\n",
    "        axes[i, 1].imshow(label, cmap='gray')\n",
    "        axes[i, 1].set_title('Ground Truth')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Prediction\n",
    "        axes[i, 2].imshow(pred, cmap='gray')\n",
    "        axes[i, 2].set_title('XYW-Net Prediction')\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('predictions.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions(model, test_dataset, device, num_samples=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db648f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Visualize Encoder Stage Outputs\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_stages(model, dataset, device, sample_idx=0):\n",
    "    \"\"\"Visualize feature maps at each encoder stage\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    sample = dataset[sample_idx]\n",
    "    img = sample['images'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get stage outputs\n",
    "    final, (s1, s2, s3, s4) = model.forward_with_stages(img)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0, 0].imshow(sample['images'].permute(1, 2, 0).numpy())\n",
    "    axes[0, 0].set_title('Input Image')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Ground truth\n",
    "    axes[0, 1].imshow(sample['labels'][0].numpy(), cmap='gray')\n",
    "    axes[0, 1].set_title('Ground Truth')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    # Final prediction\n",
    "    axes[0, 2].imshow(final[0, 0].cpu().numpy(), cmap='gray')\n",
    "    axes[0, 2].set_title('Final Prediction')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    # Thresholded prediction\n",
    "    pred_binary = (final[0, 0].cpu().numpy() > 0.5).astype(float)\n",
    "    axes[0, 3].imshow(pred_binary, cmap='gray')\n",
    "    axes[0, 3].set_title('Prediction (threshold=0.5)')\n",
    "    axes[0, 3].axis('off')\n",
    "    \n",
    "    # Stage feature maps (mean across channels)\n",
    "    stages = [s1, s2, s3, s4]\n",
    "    stage_names = ['Stage 1 (30ch)', 'Stage 2 (60ch)', 'Stage 3 (120ch)', 'Stage 4 (120ch)']\n",
    "    \n",
    "    for i, (stage, name) in enumerate(zip(stages, stage_names)):\n",
    "        feat_mean = stage[0].mean(dim=0).cpu().numpy()\n",
    "        feat_norm = (feat_mean - feat_mean.min()) / (feat_mean.max() - feat_mean.min() + 1e-8)\n",
    "        axes[1, i].imshow(feat_norm, cmap='viridis')\n",
    "        axes[1, i].set_title(f'{name}\\nShape: {tuple(stage.shape[2:])}')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'XYW-Net Encoder Stage Outputs - {sample[\"filename\"]}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('stage_outputs.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize for a few samples\n",
    "for idx in [0, 1, 2]:\n",
    "    if idx < len(test_dataset):\n",
    "        visualize_stages(model, test_dataset, device, sample_idx=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa0ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Precision-Recall Curve\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_pr_curve(model, loader, device):\n",
    "    \"\"\"Plot precision-recall curve\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(loader, desc='Computing PR curve'):\n",
    "        images = batch['images'].to(device)\n",
    "        labels = batch['labels']\n",
    "        outputs = model(images)\n",
    "        \n",
    "        for i in range(outputs.shape[0]):\n",
    "            all_preds.append(outputs[i, 0].cpu().numpy().flatten())\n",
    "            all_labels.append((labels[i, 0].numpy().flatten() > 0.5).astype(int))\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(all_labels, all_preds)\n",
    "    ap = average_precision_score(all_labels, all_preds)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, 'b-', linewidth=2, label=f'XYW-Net (AP={ap:.4f})')\n",
    "    plt.xlabel('Recall', fontsize=12)\n",
    "    plt.ylabel('Precision', fontsize=12)\n",
    "    plt.title('Precision-Recall Curve', fontsize=14)\n",
    "    plt.legend(loc='lower left', fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.savefig('pr_curve.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    return ap\n",
    "\n",
    "ap = plot_pr_curve(model, test_loader, device)\n",
    "print(f\"Average Precision: {ap:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94973424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Summary Results Table\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                    XYW-NET EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\")\n",
    "print(f\"  Dataset:          {DATA_ROOT}\")\n",
    "print(f\"  Train samples:    {len(train_dataset)}\")\n",
    "print(f\"  Val samples:      {len(val_dataset)}\")\n",
    "print(f\"  Test samples:     {len(test_dataset)}\")\n",
    "print(f\"\")\n",
    "print(f\"  Training epochs:  {NUM_EPOCHS}\")\n",
    "print(f\"  Learning rate:    {LEARNING_RATE}\")\n",
    "print(f\"  Batch size:       {BATCH_SIZE}\")\n",
    "print(f\"\")\n",
    "print(\"  \" + \"-\"*50)\n",
    "print(f\"  TEST SET METRICS:\")\n",
    "print(\"  \" + \"-\"*50)\n",
    "print(f\"  ODS (Optimal Dataset Scale):   {test_ods:.4f}\")\n",
    "print(f\"  OIS (Optimal Image Scale):     {test_ois:.4f}\")\n",
    "print(f\"  AP (Average Precision):        {test_ap:.4f}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\")\n",
    "print(\"Saved files:\")\n",
    "print(\"  - best_xyw_net.pth      (model weights)\")\n",
    "print(\"  - training_history.png  (loss/metrics plot)\")\n",
    "print(\"  - predictions.png       (sample predictions)\")\n",
    "print(\"  - stage_outputs.png     (encoder stages)\")\n",
    "print(\"  - pr_curve.png          (precision-recall curve)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3ada9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Download and Test on ScanNet Edge Detection Dataset\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DOWNLOADING EDGE DETECTION DATASET FROM INTERNET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# We'll use a lightweight dataset - BSD68 (standard image denoising/edge detection dataset)\n",
    "# Available from: https://github.com/cszn/DnCNN/tree/master/testsets\n",
    "\n",
    "DOWNLOAD_ROOT = Path(r'e:\\Edge Detection\\datasets\\downloaded')\n",
    "DOWNLOAD_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "# Option 1: Download BSD68 test set (lightweight, ~50MB)\n",
    "print(\"\\nDownloading BSD68 test set from GitHub...\")\n",
    "print(\"(Standard benchmark for edge detection)\")\n",
    "\n",
    "try:\n",
    "    # BSD68 dataset ZIP\n",
    "    url = \"https://github.com/cszn/DnCNN/raw/master/testsets/BSD68.zip\"\n",
    "    zip_path = DOWNLOAD_ROOT / \"BSD68.zip\"\n",
    "    extract_path = DOWNLOAD_ROOT / \"BSD68\"\n",
    "    \n",
    "    if not zip_path.exists():\n",
    "        print(f\"Downloading from: {url}\")\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "        print(f\"✓ Downloaded {zip_path.stat().st_size / 1e6:.1f} MB\")\n",
    "        \n",
    "        # Extract\n",
    "        print(\"Extracting...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(DOWNLOAD_ROOT)\n",
    "        print(f\"✓ Extracted to {extract_path}\")\n",
    "    else:\n",
    "        print(f\"✓ Already downloaded at {zip_path}\")\n",
    "        if not extract_path.exists():\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(DOWNLOAD_ROOT)\n",
    "    \n",
    "    # List files\n",
    "    if extract_path.exists():\n",
    "        images = list(extract_path.glob('*.png')) + list(extract_path.glob('*.jpg'))\n",
    "        print(f\"✓ Found {len(images)} images in BSD68\")\n",
    "    else:\n",
    "        images = []\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"⚠ Download failed: {e}\")\n",
    "    print(\"Trying alternative method...\")\n",
    "    images = []\n",
    "\n",
    "# If download failed, we'll create a synthetic test dataset\n",
    "if len(images) == 0:\n",
    "    print(\"\\n⚠ Download unavailable, creating synthetic edge detection dataset...\")\n",
    "    \n",
    "    synthetic_path = DOWNLOAD_ROOT / \"synthetic_edges\"\n",
    "    synthetic_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    from PIL import Image, ImageDraw\n",
    "    import random\n",
    "    \n",
    "    num_images = 10\n",
    "    for i in range(num_images):\n",
    "        # Create random image with edges\n",
    "        img = Image.new('RGB', (512, 512), color='white')\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        # Draw random shapes (which have edges)\n",
    "        for _ in range(random.randint(3, 8)):\n",
    "            x1 = random.randint(50, 400)\n",
    "            y1 = random.randint(50, 400)\n",
    "            x2 = x1 + random.randint(50, 150)\n",
    "            y2 = y1 + random.randint(50, 150)\n",
    "            color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n",
    "        \n",
    "        img.save(synthetic_path / f\"synthetic_{i:03d}.png\")\n",
    "    \n",
    "    images = list(synthetic_path.glob('*.png'))\n",
    "    extract_path = synthetic_path\n",
    "    print(f\"✓ Created {len(images)} synthetic test images\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"DATASET READY FOR TESTING: {len(images)} images\")\n",
    "print(f\"Location: {extract_path}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# ============ Load Downloaded Dataset ============\n",
    "class DownloadedDataset(Dataset):\n",
    "    \"\"\"Load downloaded/synthetic edge detection images\"\"\"\n",
    "    def __init__(self, img_dir, target_size=(512, 512)):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        # Find all images\n",
    "        self.samples = sorted(list(self.img_dir.glob('*.jpg')) + list(self.img_dir.glob('*.png')))\n",
    "        print(f\"Loaded {len(self.samples)} images from {self.img_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.samples[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            return None\n",
    "        \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        # Preprocess: Pad to square then resize\n",
    "        max_dim = max(h, w)\n",
    "        padded = Image.new('RGB', (max_dim, max_dim), (0, 0, 0))\n",
    "        paste_x = (max_dim - w) // 2\n",
    "        paste_y = (max_dim - h) // 2\n",
    "        img_pil = Image.fromarray(img)\n",
    "        padded.paste(img_pil, (paste_x, paste_y))\n",
    "        padded = padded.resize(self.target_size, Image.LANCZOS)\n",
    "        \n",
    "        img = np.array(padded).astype(np.float32) / 255.0\n",
    "        img = torch.from_numpy(img).permute(2, 0, 1)\n",
    "        \n",
    "        return {\n",
    "            'images': img,\n",
    "            'filename': img_path.stem,\n",
    "            'original_size': (h, w)\n",
    "        }\n",
    "\n",
    "# Create dataset and loader\n",
    "downloaded_dataset = DownloadedDataset(extract_path)\n",
    "downloaded_loader = DataLoader(downloaded_dataset, batch_size=2, shuffle=False, num_workers=0)\n",
    "\n",
    "# ============ Test on Downloaded Dataset ============\n",
    "@torch.no_grad()\n",
    "def test_on_downloaded_dataset(model, loader, device):\n",
    "    \"\"\"Test model on downloaded dataset\"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    print(\"\\nProcessing downloaded dataset...\")\n",
    "    pbar = tqdm(loader, desc='Testing')\n",
    "    \n",
    "    for batch in pbar:\n",
    "        if batch is None:\n",
    "            continue\n",
    "        \n",
    "        images = batch['images'].to(device)\n",
    "        filenames = batch['filename']\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        for i in range(outputs.shape[0]):\n",
    "            pred = outputs[i, 0].cpu().numpy()\n",
    "            results.append({\n",
    "                'filename': filenames[i],\n",
    "                'prediction': pred,\n",
    "                'size': (512, 512)\n",
    "            })\n",
    "    \n",
    "    print(f\"\\n✓ Processed {len(results)} images\")\n",
    "    return results\n",
    "\n",
    "# Run testing\n",
    "downloaded_results = test_on_downloaded_dataset(model, downloaded_loader, device)\n",
    "\n",
    "# ============ Visualize Downloaded Dataset Results ============\n",
    "@torch.no_grad()\n",
    "def visualize_downloaded_results(model, dataset, results, device, num_samples=8):\n",
    "    \"\"\"Visualize predictions on downloaded dataset\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Select samples\n",
    "    indices = np.random.choice(len(results), min(num_samples, len(results)), replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for plot_idx, result_idx in enumerate(indices):\n",
    "        result = results[result_idx]\n",
    "        \n",
    "        # Load original image\n",
    "        img_path = dataset.samples[result_idx]\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Predictions\n",
    "        pred = result['prediction']\n",
    "        pred_binary = (pred > 0.5).astype(float)\n",
    "        pred_binary_03 = (pred > 0.3).astype(float)\n",
    "        \n",
    "        # Display\n",
    "        axes[plot_idx, 0].imshow(img)\n",
    "        axes[plot_idx, 0].set_title(f'Original: {result[\"filename\"]}')\n",
    "        axes[plot_idx, 0].axis('off')\n",
    "        \n",
    "        axes[plot_idx, 1].imshow(pred, cmap='gray')\n",
    "        axes[plot_idx, 1].set_title('XYW-Net Output')\n",
    "        axes[plot_idx, 1].axis('off')\n",
    "        \n",
    "        axes[plot_idx, 2].imshow(pred_binary_03, cmap='gray')\n",
    "        axes[plot_idx, 2].set_title('Threshold=0.3')\n",
    "        axes[plot_idx, 2].axis('off')\n",
    "        \n",
    "        axes[plot_idx, 3].imshow(pred_binary, cmap='gray')\n",
    "        axes[plot_idx, 3].set_title('Threshold=0.5')\n",
    "        axes[plot_idx, 3].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'XYW-Net Testing on Downloaded Dataset ({len(results)} images)', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('downloaded_dataset_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"✓ Saved: downloaded_dataset_results.png\")\n",
    "\n",
    "visualize_downloaded_results(model, downloaded_dataset, downloaded_results, device, num_samples=min(8, len(downloaded_results)))\n",
    "\n",
    "# ============ Analysis ============\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DOWNLOADED DATASET TEST RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total images tested: {len(downloaded_results)}\")\n",
    "print(f\"\\nEdge detection statistics:\")\n",
    "\n",
    "edge_stats = []\n",
    "for i, result in enumerate(downloaded_results):\n",
    "    pred = result['prediction']\n",
    "    edges_30 = np.sum(pred > 0.3)\n",
    "    edges_50 = np.sum(pred > 0.5)\n",
    "    total_pixels = pred.size\n",
    "    ratio_30 = (edges_30 / total_pixels) * 100\n",
    "    ratio_50 = (edges_50 / total_pixels) * 100\n",
    "    edge_stats.append({'threshold_30': ratio_30, 'threshold_50': ratio_50})\n",
    "    \n",
    "    if i < 5:  # Show first 5\n",
    "        print(f\"  {result['filename']:25s} | Edges(t=0.3): {ratio_30:5.2f}% | Edges(t=0.5): {ratio_50:5.2f}%\")\n",
    "\n",
    "if edge_stats:\n",
    "    avg_30 = np.mean([s['threshold_30'] for s in edge_stats])\n",
    "    avg_50 = np.mean([s['threshold_50'] for s in edge_stats])\n",
    "    print(f\"\\nAverage edge pixels (threshold=0.3): {avg_30:.2f}%\")\n",
    "    print(f\"Average edge pixels (threshold=0.5): {avg_50:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL DATASETS TESTED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training dataset:      {len(train_dataset)} images (BIPED+BIPEDv2+HED-BSDS+Kaggle)\")\n",
    "print(f\"Test dataset:          {len(test_dataset)} images (from processed)\")\n",
    "print(f\"BSDS500:               {len(bsds_results)} images (downloaded)\")\n",
    "print(f\"New dataset:           {len(downloaded_results)} images (internet)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e465e24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Test on CBSD68 Dataset\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING CBSD68 DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# CBSD68 structure:\n",
    "# - original/       (clean images, JPG)\n",
    "# - original_png/   (clean images, PNG)\n",
    "# - noisy5/, noisy10/, noisy15/, noisy25/, noisy35/, noisy50/ (noisy versions)\n",
    "\n",
    "CBSD68_ROOT = Path(r'e:\\Edge Detection\\datasets\\CBSD68')\n",
    "\n",
    "cbsd68_found = False\n",
    "cbsd68_path = None\n",
    "\n",
    "print(f\"\\nSearching for CBSD68 at: {CBSD68_ROOT}\")\n",
    "\n",
    "if CBSD68_ROOT.exists():\n",
    "    # Check for original images\n",
    "    original_folder = CBSD68_ROOT / 'original'\n",
    "    original_png_folder = CBSD68_ROOT / 'original_png'\n",
    "    \n",
    "    if original_folder.exists():\n",
    "        images = list(original_folder.glob('*.jpg'))\n",
    "        if len(images) > 0:\n",
    "            cbsd68_path = original_folder\n",
    "            cbsd68_found = True\n",
    "            print(f\"✓ Found CBSD68 at: {cbsd68_path}\")\n",
    "            print(f\"  Images: {len(images)} JPG files\")\n",
    "    elif original_png_folder.exists():\n",
    "        images = list(original_png_folder.glob('*.png'))\n",
    "        if len(images) > 0:\n",
    "            cbsd68_path = original_png_folder\n",
    "            cbsd68_found = True\n",
    "            print(f\"✓ Found CBSD68 at: {cbsd68_path}\")\n",
    "            print(f\"  Images: {len(images)} PNG files\")\n",
    "\n",
    "if cbsd68_found and cbsd68_path:\n",
    "    print(f\"\\n  Available noise levels in CBSD68:\")\n",
    "    for noise_dir in ['noisy5', 'noisy10', 'noisy15', 'noisy25', 'noisy35', 'noisy50']:\n",
    "        noise_path = CBSD68_ROOT / noise_dir\n",
    "        if noise_path.exists():\n",
    "            noise_images = list(noise_path.glob('*'))\n",
    "            print(f\"    - {noise_dir}: {len(noise_images)} images\")\n",
    "    \n",
    "    # ============ Load CBSD68 Dataset ============\n",
    "    class CBSD68Dataset(Dataset):\n",
    "        \"\"\"Load CBSD68 images (clean originals)\"\"\"\n",
    "        def __init__(self, img_dir, target_size=(512, 512)):\n",
    "            self.img_dir = Path(img_dir)\n",
    "            self.target_size = target_size\n",
    "            \n",
    "            # Find all images\n",
    "            self.samples = sorted(list(self.img_dir.glob('*.jpg')) + \n",
    "                                list(self.img_dir.glob('*.png')))\n",
    "            print(f\"✓ Loaded {len(self.samples)} CBSD68 images from {img_dir.name}\")\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.samples)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            img_path = self.samples[idx]\n",
    "            \n",
    "            # Load image\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is None:\n",
    "                return None\n",
    "            \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            h, w = img.shape[:2]\n",
    "            \n",
    "            # Preprocess: Pad to square then resize\n",
    "            max_dim = max(h, w)\n",
    "            padded = Image.new('RGB', (max_dim, max_dim), (0, 0, 0))\n",
    "            paste_x = (max_dim - w) // 2\n",
    "            paste_y = (max_dim - h) // 2\n",
    "            img_pil = Image.fromarray(img)\n",
    "            padded.paste(img_pil, (paste_x, paste_y))\n",
    "            padded = padded.resize(self.target_size, Image.LANCZOS)\n",
    "            \n",
    "            img = np.array(padded).astype(np.float32) / 255.0\n",
    "            img = torch.from_numpy(img).permute(2, 0, 1)\n",
    "            \n",
    "            return {\n",
    "                'images': img,\n",
    "                'filename': img_path.stem,\n",
    "                'original_size': (h, w)\n",
    "            }\n",
    "    \n",
    "    # Create dataset and loader\n",
    "    cbsd68_dataset = CBSD68Dataset(cbsd68_path)\n",
    "    cbsd68_loader = DataLoader(cbsd68_dataset, batch_size=2, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # ============ Test on CBSD68 ============\n",
    "    @torch.no_grad()\n",
    "    def test_on_cbsd68(model, loader, device):\n",
    "        \"\"\"Test model on CBSD68 dataset\"\"\"\n",
    "        model.eval()\n",
    "        results = []\n",
    "        \n",
    "        print(\"\\nProcessing CBSD68 dataset...\")\n",
    "        pbar = tqdm(loader, desc='Testing')\n",
    "        \n",
    "        for batch in pbar:\n",
    "            if batch is None:\n",
    "                continue\n",
    "            \n",
    "            images = batch['images'].to(device)\n",
    "            filenames = batch['filename']\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            for i in range(outputs.shape[0]):\n",
    "                pred = outputs[i, 0].cpu().numpy()\n",
    "                results.append({\n",
    "                    'filename': filenames[i],\n",
    "                    'prediction': pred,\n",
    "                    'size': (512, 512)\n",
    "                })\n",
    "        \n",
    "        print(f\"\\n✓ Processed {len(results)} CBSD68 images\")\n",
    "        return results\n",
    "    \n",
    "    # Run testing\n",
    "    cbsd68_results = test_on_cbsd68(model, cbsd68_loader, device)\n",
    "    \n",
    "    # ============ Visualize CBSD68 Results ============\n",
    "    @torch.no_grad()\n",
    "    def visualize_cbsd68_results(model, dataset, results, device, num_samples=8):\n",
    "        \"\"\"Visualize predictions on CBSD68 dataset\"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        # Select samples\n",
    "        num_vis = min(num_samples, len(results))\n",
    "        indices = np.random.choice(len(results), num_vis, replace=False)\n",
    "        \n",
    "        fig, axes = plt.subplots(num_vis, 4, figsize=(16, 4*num_vis))\n",
    "        if num_vis == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for plot_idx, result_idx in enumerate(indices):\n",
    "            result = results[result_idx]\n",
    "            \n",
    "            # Load original image\n",
    "            img_path = dataset.samples[result_idx]\n",
    "            img = cv2.imread(str(img_path))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Predictions\n",
    "            pred = result['prediction']\n",
    "            pred_binary = (pred > 0.5).astype(float)\n",
    "            pred_binary_03 = (pred > 0.3).astype(float)\n",
    "            \n",
    "            # Display\n",
    "            axes[plot_idx, 0].imshow(img)\n",
    "            axes[plot_idx, 0].set_title(f'CBSD68: {result[\"filename\"]}')\n",
    "            axes[plot_idx, 0].axis('off')\n",
    "            \n",
    "            axes[plot_idx, 1].imshow(pred, cmap='gray')\n",
    "            axes[plot_idx, 1].set_title('XYW-Net Output')\n",
    "            axes[plot_idx, 1].axis('off')\n",
    "            \n",
    "            axes[plot_idx, 2].imshow(pred_binary_03, cmap='gray')\n",
    "            axes[plot_idx, 2].set_title('Threshold=0.3')\n",
    "            axes[plot_idx, 2].axis('off')\n",
    "            \n",
    "            axes[plot_idx, 3].imshow(pred_binary, cmap='gray')\n",
    "            axes[plot_idx, 3].set_title('Threshold=0.5')\n",
    "            axes[plot_idx, 3].axis('off')\n",
    "        \n",
    "        plt.suptitle(f'XYW-Net Testing on CBSD68 ({len(results)} images)', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('cbsd68_results.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"✓ Saved: cbsd68_results.png\")\n",
    "    \n",
    "    visualize_cbsd68_results(model, cbsd68_dataset, cbsd68_results, device, num_samples=min(8, len(cbsd68_results)))\n",
    "    \n",
    "    # ============ CBSD68 Analysis ============\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CBSD68 TEST RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total images tested: {len(cbsd68_results)}\")\n",
    "    print(f\"Dataset path: {cbsd68_path}\")\n",
    "    print(f\"\\nEdge detection statistics:\")\n",
    "    \n",
    "    edge_stats = []\n",
    "    for i, result in enumerate(cbsd68_results):\n",
    "        pred = result['prediction']\n",
    "        edges_30 = np.sum(pred > 0.3)\n",
    "        edges_50 = np.sum(pred > 0.5)\n",
    "        total_pixels = pred.size\n",
    "        ratio_30 = (edges_30 / total_pixels) * 100\n",
    "        ratio_50 = (edges_50 / total_pixels) * 100\n",
    "        edge_stats.append({'threshold_30': ratio_30, 'threshold_50': ratio_50})\n",
    "        \n",
    "        if i < 5:  # Show first 5\n",
    "            print(f\"  {result['filename']:25s} | Edges(t=0.3): {ratio_30:5.2f}% | Edges(t=0.5): {ratio_50:5.2f}%\")\n",
    "    \n",
    "    if len(edge_stats) > 5:\n",
    "        print(f\"  ... and {len(edge_stats)-5} more images\")\n",
    "    \n",
    "    if edge_stats:\n",
    "        avg_30 = np.mean([s['threshold_30'] for s in edge_stats])\n",
    "        avg_50 = np.mean([s['threshold_50'] for s in edge_stats])\n",
    "        print(f\"\\nAverage edge pixels (threshold=0.3): {avg_30:.2f}%\")\n",
    "        print(f\"Average edge pixels (threshold=0.5): {avg_50:.2f}%\")\n",
    "        \n",
    "        # Std deviation\n",
    "        std_30 = np.std([s['threshold_30'] for s in edge_stats])\n",
    "        std_50 = np.std([s['threshold_50'] for s in edge_stats])\n",
    "        print(f\"Std deviation (t=0.3): {std_30:.2f}%\")\n",
    "        print(f\"Std deviation (t=0.5): {std_50:.2f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL SUMMARY - ALL DATASETS TESTED\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Training set:          {len(train_dataset)} images (BIPED+BIPEDv2+HED-BSDS+Kaggle)\")\n",
    "    print(f\"Test set:              {len(test_dataset)} images (processed folder)\")\n",
    "    print(f\"BSDS500:               {len(bsds_results)} images (standard benchmark)\")\n",
    "    print(f\"Downloaded dataset:    {len(downloaded_results)} images (BSD68)\")\n",
    "    print(f\"CBSD68:                {len(cbsd68_results)} images ← YOUR DATASET\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"✓ All tests completed successfully!\")\n",
    "    print(\"✓ All visualizations saved:\")\n",
    "    print(\"  - training_history.png\")\n",
    "    print(\"  - predictions.png\")\n",
    "    print(\"  - stage_outputs.png\")\n",
    "    print(\"  - pr_curve.png\")\n",
    "    print(\"  - bsds500_results.png\")\n",
    "    print(\"  - downloaded_dataset_results.png\")\n",
    "    print(\"  - cbsd68_results.png\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n✗ CBSD68 dataset not found at expected location.\")\n",
    "    print(f\"\\nExpected path: {CBSD68_ROOT}\")\n",
    "    print(\"\\nDataset should contain:\")\n",
    "    print(\"  - original/    (JPG images)\")\n",
    "    print(\"  - original_png/ (PNG images)\")\n",
    "    print(\"  - noisy5/, noisy10/, ... (noisy versions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61701017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast mode: train=200, val=0, test=50, epochs=2\n",
      "\n",
      "[FAST] Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Fast Mode: quick sanity run on a small subset\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# 1) Reduce image size to speed up (optional). If your processed images are fixed at 512,\n",
    "#    you can skip this. Otherwise, you could add a transform to downscale to 256.\n",
    "FAST_IMAGE_SIZE = 512  # keep 512; change to 256 to accelerate further\n",
    "\n",
    "# 2) Limit dataset sizes\n",
    "N_TRAIN = min(200, len(train_dataset))  # use 200 samples for fast check\n",
    "N_VAL = min(50, len(val_dataset))\n",
    "N_TEST = min(50, len(test_dataset))\n",
    "\n",
    "train_subset = Subset(train_dataset, list(range(N_TRAIN)))\n",
    "val_subset = Subset(val_dataset, list(range(N_VAL)))\n",
    "test_subset = Subset(test_dataset, list(range(N_TEST)))\n",
    "\n",
    "# 3) Smaller batch size to reduce memory\n",
    "FAST_BATCH = 4\n",
    "\n",
    "train_loader_fast = DataLoader(train_subset, batch_size=FAST_BATCH, shuffle=True, num_workers=0)\n",
    "val_loader_fast = DataLoader(val_subset, batch_size=FAST_BATCH, shuffle=False, num_workers=0)\n",
    "test_loader_fast = DataLoader(test_subset, batch_size=FAST_BATCH, shuffle=False, num_workers=0)\n",
    "\n",
    "# 4) Fewer epochs and higher LR for quick convergence\n",
    "FAST_EPOCHS = 2\n",
    "FAST_LR = 1e-3\n",
    "FAST_WD = 1e-5\n",
    "\n",
    "model_fast = XYWNet().to(device)\n",
    "criterion_fast = EdgeLoss()\n",
    "optimizer_fast = optim.Adam(model_fast.parameters(), lr=FAST_LR, weight_decay=FAST_WD)\n",
    "scheduler_fast = optim.lr_scheduler.StepLR(optimizer_fast, step_size=2, gamma=0.5)\n",
    "\n",
    "print(f\"Fast mode: train={N_TRAIN}, val={N_VAL}, test={N_TEST}, epochs={FAST_EPOCHS}\")\n",
    "\n",
    "history_fast = {\"train_loss\": [], \"val_ods\": [], \"val_ois\": [], \"val_ap\": []}\n",
    "for epoch in range(FAST_EPOCHS):\n",
    "    print(f\"\\n[FAST] Epoch {epoch+1}/{FAST_EPOCHS}\")\n",
    "    train_loss = train_epoch(model_fast, train_loader_fast, criterion_fast, optimizer_fast, device)\n",
    "    ods, ois, ap = evaluate(model_fast, val_loader_fast, device)\n",
    "    history_fast[\"train_loss\"].append(train_loss)\n",
    "    history_fast[\"val_ods\"].append(ods)\n",
    "    history_fast[\"val_ois\"].append(ois)\n",
    "    history_fast[\"val_ap\"].append(ap)\n",
    "    scheduler_fast.step()\n",
    "    print(f\"[FAST] Loss: {train_loss:.4f} | ODS: {ods:.4f} | OIS: {ois:.4f} | AP: {ap:.4f}\")\n",
    "\n",
    "# Quick test evaluation\n",
    "print(\"\\n[FAST] Evaluating on small TEST subset...\")\n",
    "test_ods_fast, test_ois_fast, test_ap_fast = evaluate(model_fast, test_loader_fast, device)\n",
    "print(f\"[FAST] TEST → ODS: {test_ods_fast:.4f} | OIS: {test_ois_fast:.4f} | AP: {test_ap_fast:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cf26f5",
   "metadata": {},
   "source": [
    "# Diagnostic: Label Sparsity and Thresholds\n",
    "We will measure how sparse the edge labels are across common thresholds (0.2, 0.3, 0.4, 0.5). This helps pick a reasonable `label_thresh` for loss and metrics, and detect dataset issues (e.g., too sparse after resizing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb8a48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label sparsity analysis for HED-BSDS (or current test split)\n",
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "# Configure paths — update if your test set differs\n",
    "hed_test_dir = os.path.join('datasets','HED-BSDS','HED-BSDS','test')\n",
    "edge_map_glob = glob(os.path.join(hed_test_dir, 'edge_maps', '**', '*.png'), recursive=True)\n",
    "if not edge_map_glob:\n",
    "    # fallback: try BIPED edges\n",
    "    edge_map_glob = glob(os.path.join('datasets','BIPED','edges','edge_maps','**','*.png'), recursive=True)\n",
    "print(f\"Found {len(edge_map_glob)} edge maps for sparsity analysis\")\n",
    "\n",
    "thresholds = [0.2, 0.3, 0.4, 0.5]\n",
    "results = {t: {'pos_frac': 0.0} for t in thresholds}\n",
    "\n",
    "count = 0\n",
    "for p in edge_map_glob:\n",
    "    try:\n",
    "        arr = np.array(Image.open(p).convert('L'), dtype=np.float32) / 255.0\n",
    "    except Exception:\n",
    "        continue\n",
    "    if arr.size == 0:\n",
    "        continue\n",
    "    count += 1\n",
    "    total = arr.size\n",
    "    for t in thresholds:\n",
    "        pos = (arr >= t).sum()\n",
    "        results[t]['pos_frac'] += pos / total\n",
    "\n",
    "if count > 0:\n",
    "    for t in thresholds:\n",
    "        results[t]['pos_frac'] /= count\n",
    "\n",
    "print(\"Label sparsity (positive fraction) per threshold:\")\n",
    "for t in thresholds:\n",
    "    print(f\"threshold={t:.2f} -> pos_frac={results[t]['pos_frac']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fdca2b",
   "metadata": {},
   "source": [
    "# Diagnostic: Threshold Sweep for ODS/OIS/AP\n",
    "We will sweep thresholds from 0.0 to 1.0 to compute ODS (best F-score over all thresholds), OIS (per-image best F averaged), and AP (area under PR). Uses current prediction + GT loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1831c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold sweep metrics (ODS/OIS/AP)\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "# You should provide functions to get model predictions and ground truths\n",
    "# Below is a template; integrate with your dataloader and model.\n",
    "\n",
    "def get_eval_pairs(limit=None):\n",
    "    \"\"\"Return list of (pred, gt) arrays in [0,1]. Replace with real loader.\"\"\"\n",
    "    pairs = []\n",
    "    # TODO: wire to your model and dataset; here we use a placeholder\n",
    "    # Example: for batch in val_loader: preds = model(imgs); collect preds and gts\n",
    "    # Make sure preds, gt are float32 in [0,1]\n",
    "    return pairs[:limit] if limit else pairs\n",
    "\n",
    "pairs = get_eval_pairs(limit=200)  # adjust limit for quick run\n",
    "print(f\"Eval pairs: {len(pairs)}\")\n",
    "if not pairs:\n",
    "    print(\"Warning: No eval pairs collected. Wire this cell to your loader.\")\n",
    "\n",
    "# Compute ODS (global best F), OIS (per-image best F avg), AP (PR AUC)\n",
    "thresholds = np.linspace(0.0, 1.0, 51)\n",
    "best_fs = []\n",
    "global_tp = np.zeros_like(thresholds)\n",
    "global_fp = np.zeros_like(thresholds)\n",
    "global_fn = np.zeros_like(thresholds)\n",
    "\n",
    "for idx, (pred, gt) in enumerate(pairs):\n",
    "    pred = pred.astype(np.float32)\n",
    "    gt = gt.astype(np.float32)\n",
    "    image_fs = []\n",
    "    for ti, t in enumerate(thresholds):\n",
    "        p_bin = (pred >= t).astype(np.uint8)\n",
    "        g_bin = (gt >= 0.5).astype(np.uint8)\n",
    "        tp = (p_bin & g_bin).sum()\n",
    "        fp = (p_bin & (1 - g_bin)).sum()\n",
    "        fn = ((1 - p_bin) & g_bin).sum()\n",
    "        prec = tp / (tp + fp + 1e-8)\n",
    "        rec = tp / (tp + fn + 1e-8)\n",
    "        f = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "        image_fs.append(f)\n",
    "        global_tp[ti] += tp\n",
    "        global_fp[ti] += fp\n",
    "        global_fn[ti] += fn\n",
    "    best_fs.append(np.max(image_fs))\n",
    "\n",
    "# OIS: average of per-image best F\n",
    "ois = float(np.mean(best_fs)) if best_fs else 0.0\n",
    "\n",
    "# ODS: best F using global precision/recall over thresholds\n",
    "global_prec = global_tp / (global_tp + global_fp + 1e-8)\n",
    "global_rec = global_tp / (global_tp + global_fn + 1e-8)\n",
    "ods = float(np.max(2 * global_prec * global_rec / (global_prec + global_rec + 1e-8))) if len(thresholds) else 0.0\n",
    "\n",
    "# AP: PR curve from global counts\n",
    "# Sort by recall and compute AUC as an approximation\n",
    "order = np.argsort(global_rec)\n",
    "ap = float(auc(global_rec[order], global_prec[order])) if len(order) else 0.0\n",
    "\n",
    "print(f\"ODS={ods:.4f} | OIS={ois:.4f} | AP={ap:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6488c0",
   "metadata": {},
   "source": [
    "# Diagnostic: Tolerant Metrics (1–2px)\n",
    "Edges can be slightly misaligned. We introduce a small spatial tolerance by dilating GT or applying non-max suppression on predictions, then recompute ODS/OIS/AP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9980170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tolerant metrics with dilation on GT\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def tolerant_counts(pred, gt, t=0.5, tol_px=1):\n",
    "    p_bin = (pred >= t).astype(np.uint8)\n",
    "    g_bin = (gt >= 0.5).astype(np.uint8)\n",
    "    kernel = np.ones((2*tol_px+1, 2*tol_px+1), dtype=np.uint8)\n",
    "    g_dil = cv2.dilate(g_bin, kernel)\n",
    "    tp = (p_bin & g_dil).sum()\n",
    "    fp = (p_bin & (1 - g_dil)).sum()\n",
    "    fn = ((1 - p_bin) & g_bin).sum()\n",
    "    return tp, fp, fn\n",
    "\n",
    "def tolerant_metrics(pairs, tol_px=1):\n",
    "    thresholds = np.linspace(0.0, 1.0, 51)\n",
    "    global_tp = np.zeros_like(thresholds)\n",
    "    global_fp = np.zeros_like(thresholds)\n",
    "    global_fn = np.zeros_like(thresholds)\n",
    "    best_fs = []\n",
    "    for pred, gt in pairs:\n",
    "        image_fs = []\n",
    "        for ti, t in enumerate(thresholds):\n",
    "            tp, fp, fn = tolerant_counts(pred.astype(np.float32), gt.astype(np.float32), t=t, tol_px=tol_px)\n",
    "            prec = tp / (tp + fp + 1e-8)\n",
    "            rec = tp / (tp + fn + 1e-8)\n",
    "            f = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "            image_fs.append(f)\n",
    "            global_tp[ti] += tp\n",
    "            global_fp[ti] += fp\n",
    "            global_fn[ti] += fn\n",
    "        best_fs.append(np.max(image_fs))\n",
    "    ois = float(np.mean(best_fs)) if best_fs else 0.0\n",
    "    global_prec = global_tp / (global_tp + global_fp + 1e-8)\n",
    "    global_rec = global_tp / (global_tp + global_fn + 1e-8)\n",
    "    ods = float(np.max(2 * global_prec * global_rec / (global_prec + global_rec + 1e-8)))\n",
    "    order = np.argsort(global_rec)\n",
    "    ap = float(auc(global_rec[order], global_prec[order])) if len(order) else 0.0\n",
    "    return ods, ois, ap\n",
    "\n",
    "# Reuse pairs from previous cell once wired\n",
    "tol_ods, tol_ois, tol_ap = tolerant_metrics(pairs, tol_px=1)\n",
    "print(f\"Tolerance(1px): ODS={tol_ods:.4f} | OIS={tol_ois:.4f} | AP={tol_ap:.4f}\")\n",
    "tol_ods2, tol_ois2, tol_ap2 = tolerant_metrics(pairs, tol_px=2)\n",
    "print(f\"Tolerance(2px): ODS={tol_ods2:.4f} | OIS={tol_ois2:.4f} | AP={tol_ap2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8adad8",
   "metadata": {},
   "source": [
    "# Experiment: Class-Balanced Loss (A/B)\n",
    "Define a class-balanced edge loss with `pos_weight` and `label_thresh`, then compare against your current loss on a small subset to see metric sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6cd5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BalancedEdgeLoss and quick A/B on a subset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BalancedEdgeLoss(nn.Module):\n",
    "    def __init__(self, pos_weight=2.0, label_thresh=0.3):\n",
    "        super().__init__()\n",
    "        self.pos_weight = pos_weight\n",
    "        self.label_thresh = label_thresh\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # targets in [0,1], logits raw\n",
    "        pos_mask = (targets >= self.label_thresh).float()\n",
    "        neg_mask = (targets < self.label_thresh).float()\n",
    "        loss = self.bce(logits, targets)\n",
    "        pos_loss = (loss * pos_mask).sum()\n",
    "        neg_loss = (loss * neg_mask).sum()\n",
    "        denom = pos_mask.sum() + neg_mask.sum() + 1e-8\n",
    "        return (self.pos_weight * pos_loss + neg_loss) / denom\n",
    "\n",
    "# Example A/B loop (placeholder). Wire to your model/dataloader.\n",
    "def ab_compare(model, loader, steps=50, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    base_loss = nn.BCEWithLogitsLoss()\n",
    "    bal_loss = BalancedEdgeLoss(pos_weight=2.0, label_thresh=0.3)\n",
    "    model.train()\n",
    "    for i, (imgs, gts) in enumerate(loader):\n",
    "        if i >= steps:\n",
    "            break\n",
    "        imgs = imgs.to(device)\n",
    "        gts = gts.to(device)\n",
    "        # A: baseline\n",
    "        opt.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss_a = base_loss(logits, gts)\n",
    "        loss_a.backward()\n",
    "        opt.step()\n",
    "        # B: balanced\n",
    "        opt.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss_b = bal_loss(logits, gts)\n",
    "        loss_b.backward()\n",
    "        opt.step()\n",
    "    print(\"A/B training steps complete. Evaluate metrics to compare.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5b71f",
   "metadata": {},
   "source": [
    "# Kaggle Eval Settings\n",
    "Ensure evaluation runs in stable precision without AMP, and cudnn settings are compatible. Also force `float32` during metric computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8767015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle eval stability\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True  # speed on fixed-size inputs\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# Disable AMP for evaluation metrics\n",
    "amp_enabled = False\n",
    "\n",
    "def model_predict_no_amp(model, imgs):\n",
    "    with torch.no_grad():\n",
    "        if amp_enabled:\n",
    "            # If you toggle AMP later, keep casts safe\n",
    "            with torch.cuda.amp.autocast(enabled=False):\n",
    "                logits = model(imgs)\n",
    "        else:\n",
    "            logits = model(imgs)\n",
    "    probs = torch.sigmoid(logits).float()\n",
    "    return probs\n",
    "\n",
    "print(\"Eval configured: cudnn benchmark on, deterministic off, AMP disabled for metrics.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
